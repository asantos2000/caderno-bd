[["index.html", "Caderno de exercícos Mestrado de computação aplicada Big Data Analytics 2022 Organização", " Caderno de exercícos Mestrado de computação aplicada Anderson dos Santos 2022-12-13 Big Data Analytics 2022 Autor: Anderson dos Santos Prof: Olga Satomi Yoshida Organização Analise de ações (Gráficos + Discritivos) Lista de regressões (Aula 6) - EXERCICIOS.pdf Lab 3 (Aula 8 - Classificação) LABORATÓRIO R.pdf Trabalho de conclusão "],["descriptive.html", "Cap. 1 Estatística descritiva 1.1 Exercício 1.2 Pacotes 1.3 Conjunto de dados 1.4 Informações dos atributos 1.5 Análise descritiva de algumas ações 1.6 Avaliando NUBR33", " Cap. 1 Estatística descritiva Utilizamos métodos de Estatística Descritiva para organizar, resumir e descrever os aspectos importantes de um conjunto de características observadas ou comparar tais características entre dois ou mais conjuntos 1.1 Exercício Descrever o conjunto de dados da bolsa de valores brasileira (B3) 1.2 Pacotes Pacotes necessários para executar este capítulo: library(readxl) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.4.0 ✔ purrr 0.3.5 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(readxl) library(tibble) library(knitr) library(kableExtra) ## ## Attaching package: &#39;kableExtra&#39; ## ## The following object is masked from &#39;package:dplyr&#39;: ## ## group_rows library(shiny) library(dplyr) 1.3 Conjunto de dados Este conjunto de dados de ações da bovespa foi criado pela professora Dra.Olga Satomi Yoshida para aula de Big Data no IPT. 1.4 Informações dos atributos Enconta no forma de planilha excel - 03 B2 DADOS Time Series Preços Açoes Diario.xlsx, contém 10 séries com as ações: ELET3; ITUB4; ITSA4; PETR4; BBDC4; VALE3; BBAS3; LREN3; todos para o período de 01/04/2010 à 19/09/2022 e NUBR33 período 12/09/2021 a 19/09/2022; MGLU3 perído 05/02/2011 a 19/09/2022. Os atributos para cada série são a data e valor no fechamento do pregão. series = read_excel(&quot;dados/03 B1 ESTATISTICAS DESC E REGRESSAO.xlsx&quot;, sheet = 2, n_max = 1) ## New names: ## • `44898` -&gt; `44898...2` ## • `44898` -&gt; `44898...4` ## • `44898` -&gt; `44898...6` ## • `44898` -&gt; `44898...8` ## • `44898` -&gt; `44898...10` ## • `44898` -&gt; `44898...12` ## • `44898` -&gt; `44898...14` ## • `44898` -&gt; `44898...16` ## • `44898` -&gt; `44898...18` ## • `44898` -&gt; `44898...20` nomes = names(series)[seq(1, 20, 2)] series = read_excel(&quot;dados/03 B1 ESTATISTICAS DESC E REGRESSAO.xlsx&quot;, sheet = 2, skip = 1) ## New names: ## • `Date` -&gt; `Date...1` ## • `Close` -&gt; `Close...2` ## • `Date` -&gt; `Date...3` ## • `Close` -&gt; `Close...4` ## • `Date` -&gt; `Date...5` ## • `Close` -&gt; `Close...6` ## • `Date` -&gt; `Date...7` ## • `Close` -&gt; `Close...8` ## • `Date` -&gt; `Date...9` ## • `Close` -&gt; `Close...10` ## • `Date` -&gt; `Date...11` ## • `Close` -&gt; `Close...12` ## • `Date` -&gt; `Date...13` ## • `Close` -&gt; `Close...14` ## • `Date` -&gt; `Date...15` ## • `Close` -&gt; `Close...16` ## • `Date` -&gt; `Date...17` ## • `Close` -&gt; `Close...18` ## • `Date` -&gt; `Date...19` ## • `Close` -&gt; `Close...20` names(series)[seq(1, 20, 2)] = paste0(&quot;DATA_&quot;, nomes) names(series)[seq(2, 20, 2)] = nomes ## Transforma o tipo das colunas de Datetime para Date. for (i in seq(1, 20, 2)) { series[[i]] = as.Date(series[[i]]) } kable(head(series, 10)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) DATA_ELET3 ELET3 DATA_ITUB4 ITUB4 DATA_ITSA4 ITSA4 DATA_PETR4 PETR4 DATA_NUBR33 NUBR33 DATA_MGLU3 MGLU3 DATA_BBDC4 BBDC4 DATA_VALE3 VALE3 DATA_BBAS3 BBAS3 DATA_LREN3 LREN3 2010-01-04 37.37 2010-01-04 20.10 2010-01-04 8.05 2010-01-04 37.32 2021-12-09 10.04 2011-05-02 0.51 2010-01-04 12.16 2010-01-04 51.49 2010-01-04 29.90 2010-01-04 6.05 2010-01-05 37.07 2010-01-05 20.23 2010-01-05 8.02 2010-01-05 37.00 2021-12-10 11.50 2011-05-03 0.51 2010-01-05 12.10 2010-01-05 51.97 2010-01-05 29.60 2010-01-05 5.83 2010-01-06 36.59 2010-01-06 20.05 2010-01-06 7.92 2010-01-06 37.50 2021-12-13 10.65 2011-05-04 0.52 2010-01-06 12.00 2010-01-06 53.07 2010-01-06 29.64 2010-01-06 5.72 2010-01-07 37.44 2010-01-07 19.84 2010-01-07 7.88 2010-01-07 37.15 2021-12-14 9.25 2011-05-05 0.51 2010-01-07 11.97 2010-01-07 53.29 2010-01-07 29.65 2010-01-07 5.66 2010-01-08 38.07 2010-01-08 19.54 2010-01-08 7.82 2010-01-08 36.95 2021-12-15 9.54 2011-05-06 0.51 2010-01-08 11.95 2010-01-08 53.81 2010-01-08 29.82 2010-01-08 5.69 2010-01-11 37.48 2010-01-11 19.37 2010-01-11 7.79 2010-01-11 36.83 2021-12-16 9.49 2011-05-10 0.50 2010-01-11 11.96 2010-01-11 53.65 2010-01-11 30.05 2010-01-11 5.80 2010-01-12 37.13 2010-01-12 19.19 2010-01-12 7.79 2010-01-12 36.36 2021-12-17 8.95 2011-05-11 0.51 2010-01-12 12.00 2010-01-12 53.50 2010-01-12 29.80 2010-01-12 5.94 2010-01-13 37.49 2010-01-13 19.25 2010-01-13 7.77 2010-01-13 36.30 2021-12-20 8.50 2011-05-12 0.51 2010-01-13 12.06 2010-01-13 54.16 2010-01-13 30.18 2010-01-13 6.00 2010-01-14 36.89 2010-01-14 18.99 2010-01-14 7.66 2010-01-14 35.67 2021-12-21 8.70 2011-05-13 0.51 2010-01-14 11.81 2010-01-14 54.15 2010-01-14 29.75 2010-01-14 5.87 2010-01-15 35.59 2010-01-15 18.67 2010-01-15 7.47 2010-01-15 35.75 2021-12-22 8.54 2011-05-17 0.50 2010-01-15 11.64 2010-01-15 53.45 2010-01-15 29.25 2010-01-15 5.66 1.5 Análise descritiva de algumas ações plot(x=series$DATA_ELET3, y= series$ELET3, type = &quot;l&quot;, xlab = &quot;Tempo (dias)&quot;, ylab = &quot;Preço fechamento (R$)&quot;, main = &quot;Performance de algumas ações ao longo do tempo&quot;, ylim = c(0, 60) ) lines(x=series$DATA_ITUB4, y= series$ITUB4, col = 2) lines(x=series$DATA_PETR4, y= series$PETR4, col = 3) lines(x=series$DATA_NUBR33, y= series$NUBR33, col = 4) legend(&quot;top&quot;, legend = c(&quot;ELET3&quot;, &quot;ITUB4&quot;, &quot;PETR4&quot;, &quot;NUBR33&quot;), fill = c(1,2,3,4)) &gt; NUBR33 tem uma série menor que as demais ações. Parâmetros stock_symbols = c(&quot;ELET3&quot;,&quot;ITUB4&quot;,&quot;ITSA4&quot;,&quot;PETR4&quot;,&quot;NUBR33&quot;,&quot;MGLU3&quot;,&quot;BBDC4&quot;,&quot;VALE3&quot;,&quot;BBAS3&quot;,&quot;LREN3&quot;) stock_symbol = 5 # 1 a 10 dataInicial = NULL # as.Date(&quot;2022-01-01&quot;) dataFinal = NULL # as.Date(&quot;2022-12-31&quot;) 1.6 Avaliando NUBR33 symbol_series = series %&gt;% dplyr::select(matches(stock_symbols[stock_symbol]) ) names(symbol_series)[1] = &quot;stock_date&quot; names(symbol_series)[2] = &quot;stock_value&quot; if (is.null(dataInicial)) { dataInicial = min(symbol_series$stock_date, na.rm = TRUE) } if (is.null(dataFinal)) { dataFinal = max(symbol_series$stock_date, na.rm = TRUE) } subConjunto = filter(symbol_series, stock_date &gt;= dataInicial &amp; stock_date &lt;= dataFinal) Data = subConjunto$stock_date Ser = subConjunto$stock_value getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } cont = count(subConjunto) menor = min(Ser) maior = max(Ser) p = quantile(Ser, c(.025, .10, .25, .50, .75, .90, .975)) media = mean(Ser, na.rm = T) desvio = sd(Ser, na.rm = T) mediana = median(Ser, na.rm = T) moda = getmode(Ser) ep = desvio / sqrt(cont) periodo = dataFinal - dataInicial Medidas de posição: Contagem de resultados 195 Menor valor: 2.72 Percentil: 2.9885, 3.352, 3.6, 4.76, 6.585, 8.6, 9.3285 Maior valor: 11.5 Medidas de tendência central: Moda: 3.6 Mediana: 4.76 Média dos resultados: 5.3922564 Medidas de dispersão dos dados individuais em relação a média: desvio padrão dos resultados em relação a média: 2.0007468 coeficiente de variação = d.p. / média: 0.3710407 c.v. % = (d.p. / média ) x 100 %: 37.1040734 Medidas de disperção da média: e.p. = d.p. da média = d.p. / Raiz(n): 0.1432765 c.v.e.p. = e.p. / média: 0.0265708 c.v.e.p.% = (e.p. / média) x 100 %: 2.6570779 Para NUBR33 o preço médio da ação, no período de 2021-12-09 a 2022-09-19 (período de 284 dias) é de 5.3922564 e o desvio padrão de 2.0007468. plot(subConjunto, type = &quot;l&quot;, xlab = &quot;Tempo (dias)&quot;, ylab = &quot;Preço fechamento (R$)&quot;, main=stock_symbols[stock_symbol]) caixa = boxplot(Ser, add = T) limSup = caixa$stats[5,1] Q3 = caixa$stats[4,1] Q2 = caixa$stats[3,1] Q1 = caixa$stats[2,1] limInf = caixa$stats[1,1] abline(h = Q2, col = &quot;black&quot;) abline(h = Q3, col = &quot;red&quot;, lwd = 2) abline(h = Q1, col = &quot;red&quot;, lwd = 2) abline(h = limSup, col = &quot;green&quot;, lwd = 2) abline(h = limInf, col = &quot;green&quot;, lwd = 2) legend(&quot;top&quot;, legend = c(&quot;Sup&quot;, &quot;Q3&quot;, &quot;Q2&quot;, &quot;Q1&quot;, &quot;Inf&quot;), fill = c(&quot;green&quot;,&quot;red&quot;,&quot;black&quot;,&quot;red&quot;,&quot;green&quot;)) Usando boxplot para obter limites e percentíls. "],["regression.html", "Cap. 2 Regressão 2.1 Conjunto de dados 2.2 Preço das casas 2.3 Análise conjunto energia 2.4 Análise Conjunto vendas vs fontes de publidades 2.5 Análise conjunto ST vs demais variáveisCREDIT SCORE X RENDA E OUTRAS V 2.6 Consumo alimentar médio", " Cap. 2 Regressão A regressão em geral tem como objetivo: Medir a influência de uma ou mais variáveis explicativas (x) sobre a variável resposta (y); Predição de uma variável resposta (y) a partir de uma ou mais variáveis explicativas (x). 2.1 Conjunto de dados Este conjunto de dados foram criados pela professora Dra.Olga Satomi Yoshida para aula de Big Data no IPT. Preço das casas: Data_HousePrice_Area.xlsx Consumo de energia: Data_ConsumoEnergia.xlsx SALES_X_YOUTUBE: DadosAula06.xlsx CREDIT SCORE X RENDA E OUTRAS V: DadosAula06.xlsx 2.2 Preço das casas Análise descritiva e regressão linear sobre o conjunto de dados Data_HousePrice_Area.xlsx. 2.2.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) 2.2.2 Conjunto de dados dadosCen01 = read_excel(&quot;../dados/Data_HousePrice_Area.xlsx&quot;, sheet = 1) dadosCen02 = read_excel(&quot;../dados/Data_HousePrice_Area.xlsx&quot;, sheet = 2) Dados do cenário 01 Square Feet House Price 1400 245 1600 312 1700 279 1875 308 1100 199 1550 219 2350 405 2450 324 1425 319 1700 255 Dados do cenário 02 Square Feet House Price 1400 245 1800 312 1700 279 1875 308 1200 199 1480 219 2350 405 2100 324 2000 319 1700 255 Dispersão dos valores para os dois cenários: Comparando os dois gráficos, podemos observar: O primeiro conjunto é mais esparso O segundo cenário os dados estão agrupados de forma linear 2.2.3 Descrevendo os dados: 2.2.3.1 Cenário 1 House Price summary(dadosCen01$`House Price`) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 199.0 247.5 293.5 286.5 317.2 405.0 Square Feet summary(dadosCen01$`Square Feet`) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1100 1456 1650 1715 1831 2450 Distribuição dos valores hist(dadosCen01$`House Price`) 2.2.3.2 Cenário 2 House Price summary(dadosCen02$`House Price`) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 199.0 247.5 293.5 286.5 317.2 405.0 Square Feet summary(dadosCen02$`Square Feet`) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1200 1535 1750 1760 1969 2350 Distribuição dos valores hist(dadosCen01$`House Price`) 2.2.4 Ajustes de modelos lineares simples Vamos agora ajustar um modelo de regressão para ambos os cenários. 2.2.4.1 Cenário 01 modelCen01 &lt;- lm(dadosCen01$`House Price` ~ dadosCen01$`Square Feet`) modelCen01 ## ## Call: ## lm(formula = dadosCen01$`House Price` ~ dadosCen01$`Square Feet`) ## ## Coefficients: ## (Intercept) dadosCen01$`Square Feet` ## 98.2483 0.1098 \\(y = 98.2483296 + 0.1097677 x\\) resumoMod01 = summary(modelCen01) resumoMod01 ## ## Call: ## lm(formula = dadosCen01$`House Price` ~ dadosCen01$`Square Feet`) ## ## Residuals: ## Min 1Q Median 3Q Max ## -49.388 -27.388 -6.388 29.577 64.333 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 98.24833 58.03348 1.693 0.1289 ## dadosCen01$`Square Feet` 0.10977 0.03297 3.329 0.0104 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 41.33 on 8 degrees of freedom ## Multiple R-squared: 0.5808, Adjusted R-squared: 0.5284 ## F-statistic: 11.08 on 1 and 8 DF, p-value: 0.01039 \\(R^2 = 0.58\\) Vamos analisar os resíduos: plot(modelCen01$residuals ~ dadosCen01$`House Price`) plot(modelCen01, pch = 16, col = &quot;blue&quot;) plot(dadosCen01$`House Price` ~ dadosCen01$`Square Feet`) abline(modelCen01) #Add a regression line Como as observações são mais esparças, um modelo linear simples não se ajusta muito bem. 2.2.4.2 Cenário 02 y &lt;- dadosCen02$`House Price` x &lt;- dadosCen02$`Square Feet` modelCen02 &lt;- lm(y ~ x) modelCen02 ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## -9.6451 0.1682 \\(y = -9.6450892 + 0.1682165 x\\) plot(modelCen02, pch = 16, col = &quot;blue&quot;) plot(dadosCen01$`House Price` ~ dadosCen01$`Square Feet`) abline(modelCen01) #Add a regression line resumoMod02 = summary(modelCen02) resumoMod02 ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.323 -16.654 2.458 15.838 19.336 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.64509 30.46626 -0.317 0.76 ## x 0.16822 0.01702 9.886 9.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.56 on 8 degrees of freedom ## Multiple R-squared: 0.9243, Adjusted R-squared: 0.9149 ## F-statistic: 97.73 on 1 and 8 DF, p-value: 9.246e-06 \\(R^2 = 0.92\\) Vamos analisar os resíduos: plot(modelCen02$residuals ~ dadosCen02$`House Price`) Neste cenário as observações estão mais agrupadas próximas a uma reta, sendo assim o modelo linear simples descreveu melhor as observações. 2.2.5 Predição new &lt;- data.frame(x = 1300) p &lt;- predict(lm(y ~ x), new) Usando o modelo um, uma casa de \\(1300ft^2\\) custaria \\(209.04\\) 2.3 Análise conjunto energia Análise descritiva e regressão linear sobre o conjunto de dados Data_ConsumoEnergia.xlsx. 2.3.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) 2.3.2 Conjunto de dados Ajuste na maquina Consumo de energia 11.15 21.6 15.70 4.0 18.90 1.8 19.40 1.0 21.40 1.0 21.70 0.8 25.30 3.8 26.40 7.4 26.70 4.3 29.10 36.2 Um gráfico dos dados: Ajustando os modelos: modelEner01 = lm(`Consumo de energia`~`Ajuste na maquina`, data = dados02) modelEner02 = lm(`Consumo de energia`~`Ajuste na maquina` + I(`Ajuste na maquina`^2), data = dados02) modelEner03 = lm(`Consumo de energia`~`Ajuste na maquina` + I(`Ajuste na maquina`^2) + I(`Ajuste na maquina`^3), data = dados02) modelEner04 = lm(`Consumo de energia`~`Ajuste na maquina` + I(`Ajuste na maquina`^2) + I(`Ajuste na maquina`^3)+ I(`Ajuste na maquina`^4), data = dados02) Gráfico do ajuste dos modelos: x = seq(0,40, 0.1) y1 = modelEner01$coefficients[1] + modelEner01$coefficients[2]*x y2 = modelEner02$coefficients[1] + modelEner02$coefficients[2]*x + modelEner02$coefficients[3]*x^2 y3 = modelEner03$coefficients[1] + modelEner03$coefficients[2]*x + modelEner03$coefficients[3]*x^2 + modelEner03$coefficients[4]*x^3 y4 = modelEner04$coefficients[1] + modelEner04$coefficients[2]*x + modelEner04$coefficients[3]*x^2 + modelEner04$coefficients[4]*x^3 + modelEner04$coefficients[5]*x^4 par(mfrow = c(1,1)) plot(`Consumo de energia`~`Ajuste na maquina`, data = dados02, col = 1, pch = 16, xlab = &quot;Ajuste na máquina&quot;, ylab = &quot;Consumo de energia&quot;, main = &quot;Consumo de energia x Ajuste na máquina&quot;, ylim = c(-10, 40)) lines(y1~x, col = 2, lty = 1, lwd = 2) lines(y2~x, col = 3, lty = 2, lwd = 2) lines(y3~x, col = 4, lty = 3, lwd = 2) lines(y4~x, col = 5, lty = 4, lwd = 2) legend(&quot;top&quot;, legend = c(&quot;Linear&quot;, &quot;Grau 2&quot;, &quot;Grau 3&quot;, &quot;Grau 4&quot;), fill = c(2,3,4,5)) O modelo polinomial de grau 2 representou bem as observações sem o risco de perda de generalização. Polinômios de graus mais altos correm o risco de não generalizarem o fenômeno, embora se saiam muito bem com as observações do treinamento. 2.4 Análise Conjunto vendas vs fontes de publidades Análise descritiva e regressão linear sobre o conjunto de dados SALES_X_YOUTUBE em DadosAula06.xlsx. 2.4.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) 2.4.2 Conjunto de dados dados03 = read_excel(path = &quot;../dados/04_LABORATORIO REGRESSAO COM DADOS 03_DADOS.xlsx&quot;, sheet = 3) dados03 = dados03[,2:5] # tail(dados03, 3) # dados03_t = pivot_longer(dados03, c(2:5)) # names(dados03_t) = c(&quot;Indice&quot;, &quot;Grupo&quot;, &quot;Valor&quot;) kable(dados03) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Vendas em relação aos anúncios no youtube. ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Ajustando um modelo linear simples model = lm(sales ~ sqrtYou, data = dados03_mod) summary(model) ## ## Call: ## lm(formula = sales ~ sqrtYou, data = dados03_mod) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.916 -2.344 -0.131 2.326 9.316 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.20688 0.80092 4.004 8.8e-05 *** ## sqrtYou 1.09042 0.06029 18.085 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.854 on 198 degrees of freedom ## Multiple R-squared: 0.6229, Adjusted R-squared: 0.621 ## F-statistic: 327.1 on 1 and 198 DF, p-value: &lt; 2.2e-16 O gráfico mostra as observações em relação ao modelo. plot(sales ~ sqrtYou, data = dados03_mod) abline(model) Analisando os resíduos. plot(model) Ajustando o modelo a mais variáveis (multiclass). modelMult = lm(sales ~ youtube + facebook + newspaper, data = dados03) summary(modelMult) ## ## Call: ## lm(formula = sales ~ youtube + facebook + newspaper, data = dados03) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.5932 -1.0690 0.2902 1.4272 3.3951 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.526667 0.374290 9.422 &lt;2e-16 *** ## youtube 0.045765 0.001395 32.809 &lt;2e-16 *** ## facebook 0.188530 0.008611 21.893 &lt;2e-16 *** ## newspaper -0.001037 0.005871 -0.177 0.86 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.023 on 196 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 ## F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 plot(modelMult) Newspaper tem pouca influência no modelo, sendo o youtube a que mais influência nas vendas. modelMult2 = lm(sales ~ youtube + facebook, data = dados03) summary(modelMult2) ## ## Call: ## lm(formula = sales ~ youtube + facebook, data = dados03) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.5572 -1.0502 0.2906 1.4049 3.3994 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.50532 0.35339 9.919 &lt;2e-16 *** ## youtube 0.04575 0.00139 32.909 &lt;2e-16 *** ## facebook 0.18799 0.00804 23.382 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.018 on 197 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8962 ## F-statistic: 859.6 on 2 and 197 DF, p-value: &lt; 2.2e-16 plot(modelMult2) 2.5 Análise conjunto ST vs demais variáveisCREDIT SCORE X RENDA E OUTRAS V Análise descritiva e regressão linear sobre o conjunto de dados CREDIT SCORE X RENDA E OUTRAS V em DadosAula06.xlsx. 2.5.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) 2.5.2 Conjunto de dados Analise se o cliente pode receber o crédito de acordo com a análise. As variáveis são: ST - Situação (0 - Passou na análise, 1 - Nâo passou na análise) - Y R - Renda - X ND - Num Dependentes - X VE - Vinculo Empregaticio - X dados04 = read_excel(path = &quot;../dados/04_LABORATORIO REGRESSAO COM DADOS 03_DADOS.xlsx&quot;, sheet = 4) dados04 = dados04[,18:21] dados04$ST = factor(dados04$ST) dados04$VE = factor(dados04$VE) kable(dados04) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Situação explicada pela renda. plot(dados04$R ~ dados04$ST) O modelo é \\[ \\log{\\left(\\frac{P(y_i=1)}{1-P(y_i=1)}\\right)} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon_i \\] \\[ \\frac{P(y=1)}{1-P(y=1)} = e^{(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3)} \\] O ajuste é modelo04 = glm(dados04$ST ~ dados04$R + dados04$ND + dados04$VE, family = binomial(link=&#39;logit&#39;)) valoresPredito = predict.glm(modelo04, type = &quot;response&quot;) summary(modelo04) ## ## Call: ## glm(formula = dados04$ST ~ dados04$R + dados04$ND + dados04$VE, ## family = binomial(link = &quot;logit&quot;)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6591 -0.2633 -0.0531 0.4187 2.0147 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.1117 1.5725 0.707 0.479578 ## dados04$R -1.7872 0.4606 -3.880 0.000105 *** ## dados04$ND 0.9031 0.3857 2.341 0.019212 * ## dados04$VE1 2.9113 0.8506 3.423 0.000620 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 126.450 on 91 degrees of freedom ## Residual deviance: 51.382 on 88 degrees of freedom ## AIC: 59.382 ## ## Number of Fisher Scoring iterations: 6 Os valores preditos são: ## Warning in confusionMatrix.default(valoresPredito_cl, dados04$ST): Levels are ## not in the same order for reference and data. Refactoring data to match. ## Warning in confusionMatrix.default(valoresPredito_cl, dados04$ST): Levels are ## not in the same order for reference and data. Refactoring data to match. ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 46 5 ## 1 5 36 ## ## Accuracy : 0.8913 ## 95% CI : (0.8092, 0.9466) ## No Information Rate : 0.5543 ## P-Value [Acc &gt; NIR] : 2.554e-12 ## ## Kappa : 0.78 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.9020 ## Specificity : 0.8780 ## Pos Pred Value : 0.9020 ## Neg Pred Value : 0.8780 ## Prevalence : 0.5543 ## Detection Rate : 0.5000 ## Detection Prevalence : 0.5543 ## Balanced Accuracy : 0.8900 ## ## &#39;Positive&#39; Class : 0 ## Matriz de confusão draw_confusion_matrix(cm) A acurácia do modelo é de 89% e a sensibilidade é alta, em torno de 90%. Nos dados treinados o acerto de “passou na análise” foi de 100% (46/46). Já a especificidade é de 88% havendo confusão com 5/46 observações. O mesmo ocorreu para “não passou na análise”, onde 36/46 observações estão corretas e 5/46 não. 2.6 Consumo alimentar médio Análise descritiva e regressão linear. 2.6.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) library(factoextra) 2.6.2 Conjunto de dados Considere os dados a seguir do consumo alimentar médio de diferentes tipos de alimentos para famílias classificadas de acordo com o número de filhos (2, 3, 4 ou 5) e principal área de trabalho (MA: Setor de Trabalho Manual, EM: Empregados do Setor Público ou CA: Cargos Administrativos). Fonte: LABORATORIO-R.pdf dados = tibble(AreaTrabalho = as.factor(rep(c(&quot;MA&quot;, &quot;EM&quot;, &quot;CA&quot;), 4)), Filhos = as.factor(rep(2:5, each = 3)), Paes = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515), Vegetais = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097), Frutas = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887), Carnes = c(1437,1527,1948,1507,1501,2345,1620,1856,2366,1848,2056,2630), Aves = c(526, 567, 927, 544, NA, 1148,638, 762, 1149,759, 893, 1167), Leite = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561), Alcoolicos = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)) kable(dados) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 2.6.3 Regressão #dummy &lt;- dummyVars(&quot; ~ .&quot;, data=dados) #dadosS &lt;- data.frame(predict(dummy, newdata = dados)) dadosS = subset(dados, select=c(&quot;Aves&quot;, &quot;Filhos&quot;, &quot;Paes&quot;, &quot;Vegetais&quot;, &quot;Frutas&quot;, &quot;Carnes&quot;, &quot;Leite&quot;, &quot;Alcoolicos&quot;)) #modelo = lm(dadosS$Aves ~ dadosS$AreaTrabalho.CA + dadosS$AreaTrabalho.EM + dadosS$AreaTrabalho.MA + dadosS$Filhos.2 + dadosS$Filhos.3 + dadosS$Filhos.4 + dadosS$Filhos.5 + dadosS$Paes + dadosS$Vegetais + dadosS$Frutas + dadosS$Carnes + dadosS$Leite + dadosS$Alcoolicos) modelo = lm(Aves ~ Filhos + Paes + Vegetais + Frutas + Carnes + Leite + Alcoolicos, data = dadosS) valoresPredito = predict.lm(modelo, type = &quot;response&quot;) summary(modelo) ## ## Call: ## lm(formula = Aves ~ Filhos + Paes + Vegetais + Frutas + Carnes + ## Leite + Alcoolicos, data = dadosS) ## ## Residuals: ## 1 2 3 4 6 7 8 9 10 11 12 ## 11.874 -9.816 -2.058 -1.250 1.250 3.533 -5.322 1.789 -8.056 9.567 -1.510 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -746.3706 632.2722 -1.180 0.447 ## Filhos3 -75.6290 123.1415 -0.614 0.649 ## Filhos4 -105.4167 262.6281 -0.401 0.757 ## Filhos5 -192.0193 421.1901 -0.456 0.728 ## Paes 0.1803 0.3299 0.547 0.681 ## Vegetais 0.2438 0.2282 1.068 0.479 ## Frutas -0.7431 0.9391 -0.791 0.574 ## Carnes 0.9335 0.5221 1.788 0.325 ## Leite -0.1543 1.1509 -0.134 0.915 ## Alcoolicos 0.1313 0.2243 0.586 0.663 ## ## Residual standard error: 21.15 on 1 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.9993, Adjusted R-squared: 0.9928 ## F-statistic: 153.7 on 9 and 1 DF, p-value: 0.06252 confint(modelo) ## 2.5 % 97.5 % ## (Intercept) -8780.151071 7287.409914 ## Filhos3 -1640.289906 1489.031864 ## Filhos4 -3442.422544 3231.589190 ## Filhos5 -5543.747107 5159.708466 ## Paes -4.010944 4.371643 ## Vegetais -2.655840 3.143461 ## Frutas -12.674990 11.188748 ## Carnes -5.701068 7.567978 ## Leite -14.778470 14.469823 ## Alcoolicos -2.718471 2.981129 rse = sigma(modelo)/mean(dadosS$Aves, na.rm = TRUE) rse ## [1] 0.02562551 plot(dadosS, pch = 16, col = &quot;blue&quot;) #Plot the results plot(modelo$residuals, pch = 16, col = &quot;red&quot;) Verificando modelo library(performance) check_model(modelo) 2.6.4 Predição predict(modelo, interval = &quot;prediction&quot;) ## Warning in predict.lm(modelo, interval = &quot;prediction&quot;): predictions on current data refer to _future_ responses ## fit lwr upr ## 1 514.1259 165.2536 862.9982 ## 2 576.8163 217.7647 935.8679 ## 3 929.0578 549.8593 1308.2563 ## 4 545.2501 165.4832 925.0169 ## 6 1146.7499 766.9831 1526.5168 ## 7 634.4669 257.0284 1011.9055 ## 8 767.3224 393.2881 1141.3567 ## 9 1147.2107 767.7924 1526.6290 ## 10 767.0563 401.0010 1133.1117 ## 11 883.4332 523.2959 1243.5705 ## 12 1168.5105 788.8965 1548.1245 p = predict.lm(modelo, newdata = data.frame(Filhos = as.factor(3), Paes = 386, Vegetais = 608, Frutas = 396, Carnes = 1501, Leite = 319, Alcoolicos = 363)) p ## 1 ## 501.1353 O valor da Ave na linha 5 é \\(501.135316\\) Ajustando o conjunto de dados: O valor de Aves é \\(NA\\): dados[5, ] ## # A tibble: 1 × 9 ## AreaTrabalho Filhos Paes Vegetais Frutas Carnes Aves Leite Alcoolicos ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EM 3 386 608 396 1501 NA 319 363 Vamos ajustar o conjunto de dados com o valor predito para aves: dados[5, ][&#39;Aves&#39;] = p Conjunto de dados ajustado: dados[5, ] ## # A tibble: 1 × 9 ## AreaTrabalho Filhos Paes Vegetais Frutas Carnes Aves Leite Alcoolicos ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EM 3 386 608 396 1501 501. 319 363 2.6.5 Agrupamento dadosS = subset(dados, select=c(&quot;Paes&quot;, &quot;Vegetais&quot;, &quot;Frutas&quot;, &quot;Carnes&quot;, &quot;Aves&quot;, &quot;Leite&quot;, &quot;Alcoolicos&quot;)) d &lt;- dist(dadosS, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) groups &lt;- cutree(grup, k=3) plot(grup, cex = 0.6) rect.hclust(grup , k = 3, border = 2:6) abline(h = 3, col = &#39;red&#39;) kable(sort(groups)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) x 1 1 1 1 1 2 2 2 2 3 3 3 Pelo dendograma podemos dividir os dados em 3 clusters. km1 = kmeans(dadosS, 3) p1 = fviz_cluster(km1, data=dadosS, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw()) p1 "],["classification.html", "Cap. 3 Classificação 3.1 Conjunto de dados 3.2 Inibina B como marcador 3.3 Ultrassom para medir deslocamento do disco 3.4 Classificação de tipos faciais 3.5 Análise do conjunto de dados USArrests", " Cap. 3 Classificação A classificação estatística é uma ampla abordagem de aprendizado supervisionado que treina um programa para categorizar informações novas e não rotuladas com base em sua relevância para dados rotulados conhecidos. A análise de cluster visa segmentar objetos em grupos com membros semelhantes e, portanto, ajuda a descobrir a distribuição de propriedades e correlações em grandes conjuntos de dados. Os exercícios deste capítulo estão relacionados a esses dois tópicos. 3.1 Conjunto de dados Este conjunto de dados foram criados pela professora Dra.Olga Satomi Yoshida para aula de Big Data no IPT. Inibina: inibina.xlsx avaliação do deslocamento do disco da articulação temporomandibular: disco.xls Tipos faciais: tipofacial.xls Consumo alimentar médio: LABORATORIO-R.pdf (ex 5.) 3.2 Inibina B como marcador Avaliar a inibina B como marcador da reserva ovariana de pacientes submetidas à fertilização in vitro 3.2.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) library(rpart) library(rpart.plot) library(caret) library(MASS) library(httr) library(readxl) library(tibble) library(e1071) library(neuralnet) library(factoextra) library(ggpubr) 3.2.2 Conjunto de dados httr::GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/inibina.xls&quot;, httr::write_disk(&quot;../dados/inibina.xls&quot;, overwrite = TRUE)) ## Response [https://www.ime.usp.br/~jmsinger/MorettinSinger/inibina.xls] ## Date: 2022-12-14 00:18 ## Status: 200 ## Content-Type: application/vnd.ms-excel ## Size: 8.7 kB ## &lt;ON DISK&gt; G:\\onedrive\\obsidian\\adsantos\\Mestrado\\BD\\trabalhos\\caderno-bd\\dados\\inibina.xls inibina &lt;- read_excel(&quot;../dados/inibina.xls&quot;) Adicionando coluna com a diferença entre pré e pós e ajustando a variável resposta para categórica: inibina$difinib = inibina$inibpos - inibina$inibpre inibina$resposta = as.factor(inibina$resposta) Distribuição das respostas do conjunto de dados: kable(inibina) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ident resposta inibpre inibpos difinib 1 positiva 54.03 65.93 11.90 2 positiva 159.13 281.09 121.96 3 positiva 98.34 305.37 207.03 4 positiva 85.30 434.41 349.11 5 positiva 127.93 229.30 101.37 6 positiva 143.60 353.82 210.22 7 positiva 110.58 254.07 143.49 8 positiva 47.52 199.29 151.77 9 positiva 122.62 327.87 205.25 10 positiva 165.95 339.46 173.51 11 positiva 145.28 377.26 231.98 12 positiva 186.38 1055.19 868.81 13 positiva 149.45 353.89 204.44 14 positiva 33.29 100.09 66.80 15 positiva 181.57 358.45 176.88 16 positiva 58.43 168.14 109.71 17 positiva 128.16 228.48 100.32 18 positiva 152.92 312.34 159.42 19 positiva 148.75 406.11 257.36 20 negativa 81.00 201.40 120.40 21 negativa 24.74 45.17 20.43 22 negativa 3.02 6.03 3.01 23 negativa 4.27 17.80 13.53 24 negativa 99.30 127.93 28.63 25 negativa 108.29 129.39 21.10 26 negativa 7.36 21.27 13.91 27 negativa 161.28 319.65 158.37 28 negativa 184.46 311.44 126.98 29 negativa 23.13 45.64 22.51 30 negativa 111.18 192.22 81.04 31 negativa 105.82 130.61 24.79 32 negativa 3.98 6.46 2.48 plot(inibina$difinib ~ inibina$resposta, ylim = c(0, 400)) summary(inibina) ## ident resposta inibpre inibpos ## Min. : 1.00 negativa:13 Min. : 3.02 Min. : 6.03 ## 1st Qu.: 8.75 positiva:19 1st Qu.: 52.40 1st Qu.: 120.97 ## Median :16.50 Median :109.44 Median : 228.89 ## Mean :16.50 Mean :100.53 Mean : 240.80 ## 3rd Qu.:24.25 3rd Qu.:148.93 3rd Qu.: 330.77 ## Max. :32.00 Max. :186.38 Max. :1055.19 ## difinib ## Min. : 2.48 ## 1st Qu.: 24.22 ## Median :121.18 ## Mean :140.27 ## 3rd Qu.:183.77 ## Max. :868.81 O desvio padrão da diferençca da inibina pós e pré é \\(159.2217295\\). Distribuição: x = 1:32 plot(inibina$difinib ~ x, col = inibina$resposta) 3.2.3 Generalized Linear Models Treinando e avalaindo o modelo: modLogist01 = glm(resposta ~ difinib, family = binomial, data = inibina) summary(modLogist01) ## ## Call: ## glm(formula = resposta ~ difinib, family = binomial, data = inibina) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9770 -0.5594 0.1890 0.5589 2.0631 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.310455 0.947438 -2.439 0.01474 * ## difinib 0.025965 0.008561 3.033 0.00242 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 24.758 on 30 degrees of freedom ## AIC: 28.758 ## ## Number of Fisher Scoring iterations: 6 predito = predict.glm(modLogist01, type = &quot;response&quot;) classPred = ifelse(predito&gt;0.5, &quot;positiva&quot;, &quot;negativa&quot;) classPred = as.factor(classPred) cm = confusionMatrix(classPred, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 10 2 ## positiva 3 17 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6721 ## ## Mcnemar&#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.8947 ## Specificity : 0.7692 ## Pos Pred Value : 0.8500 ## Neg Pred Value : 0.8333 ## Prevalence : 0.5938 ## Detection Rate : 0.5312 ## Detection Prevalence : 0.6250 ## Balanced Accuracy : 0.8320 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modLogist01&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) Validação cruzada leave-one-out trControl &lt;- trainControl(method = &quot;LOOCV&quot;) modLogist02 &lt;- train(resposta ~ difinib, method = &quot;glm&quot;, data = inibina, family = binomial, trControl = trControl, metric = &quot;Accuracy&quot;) summary(modLogist02) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9770 -0.5594 0.1890 0.5589 2.0631 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.310455 0.947438 -2.439 0.01474 * ## difinib 0.025965 0.008561 3.033 0.00242 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 24.758 on 30 degrees of freedom ## AIC: 28.758 ## ## Number of Fisher Scoring iterations: 6 predito = predict(modLogist02, newdata = inibina) classPred = as.factor(predito) cm = confusionMatrix(classPred, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 10 2 ## positiva 3 17 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6721 ## ## Mcnemar&#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.8947 ## Specificity : 0.7692 ## Pos Pred Value : 0.8500 ## Neg Pred Value : 0.8333 ## Prevalence : 0.5938 ## Detection Rate : 0.5312 ## Detection Prevalence : 0.6250 ## Balanced Accuracy : 0.8320 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modLogist02-LOOCV&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) Treinar o modelo com o método LOOCV, neste conjunto de dados, não mudou o resultado. 3.2.4 Linear Discriminant Analysis - Fisher Treinando e avalaindo o modelo: modFisher01 = lda(resposta ~ difinib, data = inibina, prior = c(0.5, 0.5)) predito = predict(modFisher01) classPred = predito$class cm = confusionMatrix(classPred, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 11 6 ## positiva 2 13 ## ## Accuracy : 0.75 ## 95% CI : (0.566, 0.8854) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.04978 ## ## Kappa : 0.5058 ## ## Mcnemar&#39;s Test P-Value : 0.28884 ## ## Sensitivity : 0.6842 ## Specificity : 0.8462 ## Pos Pred Value : 0.8667 ## Neg Pred Value : 0.6471 ## Prevalence : 0.5938 ## Detection Rate : 0.4062 ## Detection Prevalence : 0.4688 ## Balanced Accuracy : 0.7652 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modFisher01-prior 0.5 / 0.5&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.5 Bayes Treinando e avalaindo o modelo: inibina$resposta ## [1] positiva positiva positiva positiva positiva positiva positiva positiva ## [9] positiva positiva positiva positiva positiva positiva positiva positiva ## [17] positiva positiva positiva negativa negativa negativa negativa negativa ## [25] negativa negativa negativa negativa negativa negativa negativa negativa ## Levels: negativa positiva modBayes01 = lda(resposta ~ difinib, data = inibina, prior = c(0.65, 0.35)) predito = predict(modBayes01) classPred = predito$class cm = confusionMatrix(classPred, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 13 13 ## positiva 0 6 ## ## Accuracy : 0.5938 ## 95% CI : (0.4064, 0.763) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.5755484 ## ## Kappa : 0.2727 ## ## Mcnemar&#39;s Test P-Value : 0.0008741 ## ## Sensitivity : 0.3158 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 0.5000 ## Prevalence : 0.5938 ## Detection Rate : 0.1875 ## Detection Prevalence : 0.1875 ## Balanced Accuracy : 0.6579 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modBayes01-prior 0.65 / 0.35&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) table(classPred) ## classPred ## negativa positiva ## 26 6 print(inibina, n = 32) ## # A tibble: 32 × 5 ## ident resposta inibpre inibpos difinib ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 positiva 54.0 65.9 11.9 ## 2 2 positiva 159. 281. 122. ## 3 3 positiva 98.3 305. 207. ## 4 4 positiva 85.3 434. 349. ## 5 5 positiva 128. 229. 101. ## 6 6 positiva 144. 354. 210. ## 7 7 positiva 111. 254. 143. ## 8 8 positiva 47.5 199. 152. ## 9 9 positiva 123. 328. 205. ## 10 10 positiva 166. 339. 174. ## 11 11 positiva 145. 377. 232. ## 12 12 positiva 186. 1055. 869. ## 13 13 positiva 149. 354. 204. ## 14 14 positiva 33.3 100. 66.8 ## 15 15 positiva 182. 358. 177. ## 16 16 positiva 58.4 168. 110. ## 17 17 positiva 128. 228. 100. ## 18 18 positiva 153. 312. 159. ## 19 19 positiva 149. 406. 257. ## 20 20 negativa 81 201. 120. ## 21 21 negativa 24.7 45.2 20.4 ## 22 22 negativa 3.02 6.03 3.01 ## 23 23 negativa 4.27 17.8 13.5 ## 24 24 negativa 99.3 128. 28.6 ## 25 25 negativa 108. 129. 21.1 ## 26 26 negativa 7.36 21.3 13.9 ## 27 27 negativa 161. 320. 158. ## 28 28 negativa 184. 311. 127. ## 29 29 negativa 23.1 45.6 22.5 ## 30 30 negativa 111. 192. 81.0 ## 31 31 negativa 106. 131. 24.8 ## 32 32 negativa 3.98 6.46 2.48 3.2.5.1 Naive Bayes Treinando e avalaindo o modelo: modNaiveBayes01 = naiveBayes(resposta ~ difinib, data = inibina) predito = predict(modNaiveBayes01, inibina) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 11 5 ## positiva 2 14 ## ## Accuracy : 0.7812 ## 95% CI : (0.6003, 0.9072) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.02102 ## ## Kappa : 0.5625 ## ## Mcnemar&#39;s Test P-Value : 0.44969 ## ## Sensitivity : 0.7368 ## Specificity : 0.8462 ## Pos Pred Value : 0.8750 ## Neg Pred Value : 0.6875 ## Prevalence : 0.5938 ## Detection Rate : 0.4375 ## Detection Prevalence : 0.5000 ## Balanced Accuracy : 0.7915 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modNaiveBayes01&quot;, &quot;naiveBayes&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.6 Decison tree Treinando e avalaindo o modelo: modArvDec01 = rpart(resposta ~ difinib, data = inibina) prp(modArvDec01, faclen=0, extra=1, roundint=F, digits=5) predito = predict(modArvDec01, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 9 1 ## positiva 4 18 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6639 ## ## Mcnemar&#39;s Test P-Value : 0.371093 ## ## Sensitivity : 0.9474 ## Specificity : 0.6923 ## Pos Pred Value : 0.8182 ## Neg Pred Value : 0.9000 ## Prevalence : 0.5938 ## Detection Rate : 0.5625 ## Detection Prevalence : 0.6875 ## Balanced Accuracy : 0.8198 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modArvDec01&quot;, &quot;rpart&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.7 SVM Treinando e avalaindo o modelo: modSVM01 = svm(resposta ~ difinib, data = inibina, kernel = &quot;linear&quot;) predito = predict(modSVM01, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 10 2 ## positiva 3 17 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6721 ## ## Mcnemar&#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.8947 ## Specificity : 0.7692 ## Pos Pred Value : 0.8500 ## Neg Pred Value : 0.8333 ## Prevalence : 0.5938 ## Detection Rate : 0.5312 ## Detection Prevalence : 0.6250 ## Balanced Accuracy : 0.8320 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modSVM01&quot;, &quot;svm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.8 Neural network Treinando e avalaindo o modelo: modRedNeural01 = neuralnet(resposta ~ difinib, data = inibina, hidden = c(2,4,3)) plot(modRedNeural01) ypred = neuralnet::compute(modRedNeural01, inibina) yhat = ypred$net.result yhat = round(yhat) yhat=data.frame(&quot;yhat&quot;=ifelse(max.col(yhat[ ,1:2])==1, &quot;negativa&quot;, &quot;positiva&quot;)) cm = confusionMatrix(as.factor(yhat$yhat), inibina$resposta) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 11 1 ## positiva 2 18 ## ## Accuracy : 0.9062 ## 95% CI : (0.7498, 0.9802) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.000105 ## ## Kappa : 0.8033 ## ## Mcnemar&#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.8462 ## Specificity : 0.9474 ## Pos Pred Value : 0.9167 ## Neg Pred Value : 0.9000 ## Prevalence : 0.4062 ## Detection Rate : 0.3438 ## Detection Prevalence : 0.3750 ## Balanced Accuracy : 0.8968 ## ## &#39;Positive&#39; Class : negativa ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modRedNeural01&quot;, &quot;neuralnet&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.9 KNN Treinando e avalaindo o modelo: Para \\(k = 3\\): modKnn3_01 = knn3(resposta ~ difinib, data = inibina, k = 3) predito = predict(modKnn3_01, inibina, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=3&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) Para \\(k = 5\\): modKnn5_01 = knn3(resposta ~ difinib, data = inibina, k = 5) predito = predict(modKnn5_01, inibina, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 9 1 ## positiva 4 18 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6639 ## ## Mcnemar&#39;s Test P-Value : 0.371093 ## ## Sensitivity : 0.9474 ## Specificity : 0.6923 ## Pos Pred Value : 0.8182 ## Neg Pred Value : 0.9000 ## Prevalence : 0.5938 ## Detection Rate : 0.5625 ## Detection Prevalence : 0.6875 ## Balanced Accuracy : 0.8198 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn5_01-k=5&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.10 Comparando os modelos Model Algorithm Accuracy Sensitivity Specificity modLogist01 glm 0.84375 0.894736842105263 0.769230769230769 modLogist02-LOOCV glm 0.84375 0.894736842105263 0.769230769230769 modFisher01-prior 0.5 / 0.5 lda 0.75 0.684210526315789 0.846153846153846 modBayes01-prior 0.65 / 0.35 lda 0.59375 0.315789473684211 1 modNaiveBayes01 naiveBayes 0.78125 0.736842105263158 0.846153846153846 modArvDec01 rpart 0.84375 0.947368421052632 0.692307692307692 modSVM01 svm 0.84375 0.894736842105263 0.769230769230769 modRedNeural01 neuralnet 0.90625 0.846153846153846 0.947368421052632 modKnn3_01-k=3 knn3 0.875 0.894736842105263 0.846153846153846 modKnn5_01-k=5 knn3 0.84375 0.947368421052632 0.692307692307692 Os modelos modLogist01, modFisher01 com \\(prior = 0.5, 0.5\\), modArvDec01 e modKnn3_01 com \\(k=3\\) tiveram performance muito parecidas no conjunto de dados. Uma possível escolha seria o modKnn3_01 que combinado obteve melhor acurácia, sensibilidade e especificidade. 3.2.11 Agrupamento inibinaS = inibina[, 3:5] d &lt;- dist(inibinaS, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) groups &lt;- cutree(grup, k=3) plot(grup, cex = 0.6) rect.hclust(grup , k = 3, border = 2:6) abline(h = 3, col = &#39;red&#39;) table(groups, inibina$resposta) ## ## groups negativa positiva ## 1 11 7 ## 2 2 11 ## 3 0 1 Como indicado no dendograma, podemos dividir em 3 grupos: km1 = kmeans(inibinaS, 3) p1 = fviz_cluster(km1, data=inibinaS, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;, &quot;#D7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw()) p1 groups = km1$cluster table(groups, inibina$resposta) ## ## groups negativa positiva ## 1 3 15 ## 2 10 3 ## 3 0 1 3.3 Ultrassom para medir deslocamento do disco Os dados disponíveis aqui foram extraídos de um estudo realizado no Hospital Universitário da Universidade de São Paulo com o objetivo de avaliar se algumas medidas obtidas ultrassonograficamente poderiam ser utilizadas como substitutas de medidas obtidas por métodos de ressonância magnética, considerada como padrão ouro para avaliação do deslocamento do disco da articulação temporomandibular (referido simplesmente como disco). 3.3.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) library(rpart) library(rpart.plot) library(caret) library(MASS) library(httr) library(readxl) library(tibble) library(e1071) library(neuralnet) library(factoextra) library(ggpubr) 3.3.2 Conjunto de dados #GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/disco.xls&quot;, write_disk(tf &lt;- tempfile(fileext = &quot;.xls&quot;))) httr::GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/disco.xls&quot;, httr::write_disk(&quot;../dados/disco.xls&quot;, overwrite = TRUE)) ## Response [https://www.ime.usp.br/~jmsinger/MorettinSinger/disco.xls] ## Date: 2022-12-14 00:18 ## Status: 200 ## Content-Type: application/vnd.ms-excel ## Size: 11.3 kB ## &lt;ON DISK&gt; G:\\onedrive\\obsidian\\adsantos\\Mestrado\\BD\\trabalhos\\caderno-bd\\dados\\disco.xls disco &lt;- read_excel(&quot;../dados/disco.xls&quot;) Número de observações 104. 3.3.2.1 Categorizando a variável de deslocamento. disco$sumdistancia = disco$distanciaA + disco$distanciaF disco$deslocamento = as.factor(disco$deslocamento) plot(disco$sumdistancia ~ disco$deslocamento) kable(disco) %&gt;% kable_styling(latex_options = &quot;striped&quot;) deslocamento distanciaA distanciaF sumdistancia 0 2.2 1.4 3.6 0 2.4 1.2 3.6 0 2.6 2.0 4.6 1 3.5 1.8 5.3 0 1.3 1.0 2.3 1 2.8 1.1 3.9 0 1.5 1.2 2.7 0 2.6 1.1 3.7 0 1.2 0.6 1.8 0 1.7 1.5 3.2 0 1.3 1.2 2.5 0 1.2 1.0 2.2 1 4.0 2.5 6.5 0 1.2 1.0 2.2 1 3.1 1.7 4.8 1 2.6 0.6 3.2 0 1.8 0.8 2.6 0 1.2 1.0 2.2 0 1.9 1.0 2.9 0 1.2 0.9 2.1 1 1.7 0.9 2.6 0 1.2 0.8 2.0 1 3.9 3.2 7.1 0 1.7 1.1 2.8 0 1.4 1.0 2.4 0 1.6 1.3 2.9 0 1.3 0.5 1.8 0 1.7 0.7 2.4 1 2.6 1.8 4.4 0 1.5 1.5 3.0 0 1.8 1.4 3.2 0 1.2 0.9 2.1 0 1.9 1.0 2.9 0 2.3 1.0 3.3 0 1.6 1.0 2.6 0 1.0 0.6 1.6 0 1.6 1.3 2.9 1 4.3 2.3 6.6 0 2.1 1.0 3.1 0 1.6 0.9 2.5 0 2.3 1.2 3.5 0 2.4 1.3 3.7 0 2.0 1.1 3.1 0 1.8 1.2 3.0 0 1.4 1.9 3.3 0 1.5 1.3 2.8 0 2.2 1.2 3.4 0 1.6 2.0 3.6 0 1.5 1.1 2.6 0 1.2 0.7 1.9 0 1.5 0.8 2.3 0 1.8 1.1 2.9 0 0.9 0.8 1.7 0 1.1 0.9 2.0 0 1.4 1.1 2.5 0 1.6 0.8 2.4 0 2.1 1.3 3.4 0 1.8 0.9 2.7 0 2.4 0.9 3.3 0 2.0 2.3 4.3 0 2.0 2.3 4.3 0 2.4 2.9 5.3 1 2.7 2.4 5.1 1 1.9 2.7 4.6 1 2.4 1.3 3.7 1 2.1 0.8 2.9 0 0.8 1.3 2.1 1 0.8 2.0 2.8 0 0.5 0.6 1.1 0 1.5 0.7 2.2 1 2.9 1.6 4.5 0 1.4 1.2 2.6 1 3.2 0.5 3.7 0 1.2 1.2 2.4 1 2.1 1.6 3.7 1 1.4 1.5 2.9 0 1.5 1.4 2.9 0 1.6 1.5 3.1 1 4.9 1.2 6.1 0 1.1 1.1 2.2 1 2.0 1.3 3.3 0 1.5 2.2 3.7 0 1.7 1.0 2.7 0 1.9 1.4 3.3 1 2.5 3.1 5.6 0 1.4 1.5 2.9 1 2.5 1.8 4.3 1 2.3 1.6 3.9 0 1.2 0.4 1.6 0 1.0 1.1 2.1 1 2.9 2.4 5.3 1 2.5 3.3 5.8 0 1.4 1.1 2.5 0 1.5 1.3 2.8 0 0.8 2.0 2.8 0 2.0 2.1 4.1 1 3.1 2.2 5.3 1 3.1 2.1 5.2 0 1.7 1.2 2.9 0 1.6 0.5 2.1 0 1.4 1.1 2.5 0 1.6 1.0 2.6 1 2.3 1.6 3.9 1 2.2 1.8 4.0 summary(disco) ## deslocamento distanciaA distanciaF sumdistancia ## 0:75 Min. :0.500 Min. :0.400 Min. :1.100 ## 1:29 1st Qu.:1.400 1st Qu.:1.000 1st Qu.:2.500 ## Median :1.700 Median :1.200 Median :2.900 ## Mean :1.907 Mean :1.362 Mean :3.268 ## 3rd Qu.:2.300 3rd Qu.:1.600 3rd Qu.:3.700 ## Max. :4.900 Max. :3.300 Max. :7.100 O desvio padrão da soma das distâncias é \\(1.1839759\\). 3.3.3 Generalized Linear Models Treinando o modelo: modLogist01 = glm(deslocamento ~ sumdistancia, family = binomial, data = disco) summary(modLogist01) ## ## Call: ## glm(formula = deslocamento ~ sumdistancia, family = binomial, ## data = disco) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2291 -0.4957 -0.3182 0.1560 2.3070 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.3902 1.3740 -5.379 7.51e-08 *** ## sumdistancia 1.8467 0.3799 4.861 1.17e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.107 on 103 degrees of freedom ## Residual deviance: 72.567 on 102 degrees of freedom ## AIC: 76.567 ## ## Number of Fisher Scoring iterations: 5 Avaliando o modelo: predito = predict.glm(modLogist01, type = &quot;response&quot;) classPred = ifelse(predito&gt;0.5, &quot;0&quot;, &quot;1&quot;) classPred = as.factor(classPred) cm = confusionMatrix(classPred, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 5 16 ## 1 70 13 ## ## Accuracy : 0.1731 ## 95% CI : (0.1059, 0.2597) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : -0.3088 ## ## Mcnemar&#39;s Test P-Value : 1.096e-08 ## ## Sensitivity : 0.06667 ## Specificity : 0.44828 ## Pos Pred Value : 0.23810 ## Neg Pred Value : 0.15663 ## Prevalence : 0.72115 ## Detection Rate : 0.04808 ## Detection Prevalence : 0.20192 ## Balanced Accuracy : 0.25747 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modLogist01&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) Validação cruzada leave-one-out trControl &lt;- trainControl(method = &quot;LOOCV&quot;) modLogist02 &lt;- train(deslocamento ~ sumdistancia, method = &quot;glm&quot;, data = disco, family = binomial, trControl = trControl, metric = &quot;Accuracy&quot;) summary(modLogist02) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2291 -0.4957 -0.3182 0.1560 2.3070 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.3902 1.3740 -5.379 7.51e-08 *** ## sumdistancia 1.8467 0.3799 4.861 1.17e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.107 on 103 degrees of freedom ## Residual deviance: 72.567 on 102 degrees of freedom ## AIC: 76.567 ## ## Number of Fisher Scoring iterations: 5 predito = predict(modLogist02, newdata = disco) classPred = as.factor(predito) cm = confusionMatrix(classPred, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 70 13 ## 1 5 16 ## ## Accuracy : 0.8269 ## 95% CI : (0.7403, 0.8941) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.00857 ## ## Kappa : 0.5299 ## ## Mcnemar&#39;s Test P-Value : 0.09896 ## ## Sensitivity : 0.9333 ## Specificity : 0.5517 ## Pos Pred Value : 0.8434 ## Neg Pred Value : 0.7619 ## Prevalence : 0.7212 ## Detection Rate : 0.6731 ## Detection Prevalence : 0.7981 ## Balanced Accuracy : 0.7425 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modLogist02-LOOCV&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) kable(model_eval) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Model Algorithm Accuracy Sensitivity Specificity modLogist01 glm 0.173076923076923 0.0666666666666667 0.448275862068966 modLogist02-LOOCV glm 0.826923076923077 0.933333333333333 0.551724137931034 Treinar o modelo com o método LOOCV melhorou consideravelmente todas as métricas do modelo. 3.3.4 Linear Discriminant Analysis - Fisher Treinando e avalaindo o modelo: modFisher01 = lda(deslocamento ~ sumdistancia, data = disco, prior = c(0.5, 0.5)) predito = predict(modFisher01) classPred = predito$class cm = confusionMatrix(classPred, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 67 6 ## 1 8 23 ## ## Accuracy : 0.8654 ## 95% CI : (0.7845, 0.9244) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.0003676 ## ## Kappa : 0.6722 ## ## Mcnemar&#39;s Test P-Value : 0.7892680 ## ## Sensitivity : 0.8933 ## Specificity : 0.7931 ## Pos Pred Value : 0.9178 ## Neg Pred Value : 0.7419 ## Prevalence : 0.7212 ## Detection Rate : 0.6442 ## Detection Prevalence : 0.7019 ## Balanced Accuracy : 0.8432 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modFisher01-prior 0.5 / 0.5&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.5 Bayes Treinando e avalaindo o modelo: modBayes01 = lda(deslocamento ~ sumdistancia, data = disco, prior = c(0.65, 0.35)) predito = predict(modBayes01) classPred = predito$class cm = confusionMatrix(classPred, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 70 12 ## 1 5 17 ## ## Accuracy : 0.8365 ## 95% CI : (0.7512, 0.9018) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.004313 ## ## Kappa : 0.5611 ## ## Mcnemar&#39;s Test P-Value : 0.145610 ## ## Sensitivity : 0.9333 ## Specificity : 0.5862 ## Pos Pred Value : 0.8537 ## Neg Pred Value : 0.7727 ## Prevalence : 0.7212 ## Detection Rate : 0.6731 ## Detection Prevalence : 0.7885 ## Balanced Accuracy : 0.7598 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modBayes01-prior 0.65 / 0.35&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) table(classPred) ## classPred ## 0 1 ## 82 22 print(disco, n = 32) ## # A tibble: 104 × 4 ## deslocamento distanciaA distanciaF sumdistancia ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 2.2 1.4 3.6 ## 2 0 2.4 1.2 3.6 ## 3 0 2.6 2 4.6 ## 4 1 3.5 1.8 5.3 ## 5 0 1.3 1 2.3 ## 6 1 2.8 1.1 3.9 ## 7 0 1.5 1.2 2.7 ## 8 0 2.6 1.1 3.7 ## 9 0 1.2 0.6 1.8 ## 10 0 1.7 1.5 3.2 ## 11 0 1.3 1.2 2.5 ## 12 0 1.2 1 2.2 ## 13 1 4 2.5 6.5 ## 14 0 1.2 1 2.2 ## 15 1 3.1 1.7 4.8 ## 16 1 2.6 0.6 3.2 ## 17 0 1.8 0.8 2.6 ## 18 0 1.2 1 2.2 ## 19 0 1.9 1 2.9 ## 20 0 1.2 0.9 2.1 ## 21 1 1.7 0.9 2.6 ## 22 0 1.2 0.8 2 ## 23 1 3.9 3.2 7.1 ## 24 0 1.7 1.1 2.8 ## 25 0 1.4 1 2.4 ## 26 0 1.6 1.3 2.9 ## 27 0 1.3 0.5 1.8 ## 28 0 1.7 0.7 2.4 ## 29 1 2.6 1.8 4.4 ## 30 0 1.5 1.5 3 ## 31 0 1.8 1.4 3.2 ## 32 0 1.2 0.9 2.1 ## # … with 72 more rows 3.3.6 Naive Bayes Treinando e avalaindo o modelo: modNaiveBayes01 = naiveBayes(deslocamento ~ sumdistancia, data = disco) predito = predict(modNaiveBayes01, disco) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 70 13 ## 1 5 16 ## ## Accuracy : 0.8269 ## 95% CI : (0.7403, 0.8941) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.00857 ## ## Kappa : 0.5299 ## ## Mcnemar&#39;s Test P-Value : 0.09896 ## ## Sensitivity : 0.9333 ## Specificity : 0.5517 ## Pos Pred Value : 0.8434 ## Neg Pred Value : 0.7619 ## Prevalence : 0.7212 ## Detection Rate : 0.6731 ## Detection Prevalence : 0.7981 ## Balanced Accuracy : 0.7425 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modNaiveBayes01&quot;, &quot;naiveBayes&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.7 Decison tree Treinando e avalaindo o modelo: modArvDec01 = rpart(deslocamento ~ sumdistancia, data = disco) prp(modArvDec01, faclen=0, #use full names for factor labels extra=1, #display number of observations for each terminal node roundint=F, #don&#39;t round to integers in output digits=5) predito = predict(modArvDec01, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 67 6 ## 1 8 23 ## ## Accuracy : 0.8654 ## 95% CI : (0.7845, 0.9244) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.0003676 ## ## Kappa : 0.6722 ## ## Mcnemar&#39;s Test P-Value : 0.7892680 ## ## Sensitivity : 0.8933 ## Specificity : 0.7931 ## Pos Pred Value : 0.9178 ## Neg Pred Value : 0.7419 ## Prevalence : 0.7212 ## Detection Rate : 0.6442 ## Detection Prevalence : 0.7019 ## Balanced Accuracy : 0.8432 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modArvDec01&quot;, &quot;rpart&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) x = 1:nrow(disco) plot(disco$sumdistancia ~ x, col = disco$deslocamento) 3.3.8 SVM Treinando e avalaindo o modelo: modSVM01 = svm(deslocamento ~ sumdistancia, data = disco, kernel = &quot;linear&quot;) predito = predict(modSVM01, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modSVM01&quot;, &quot;svm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.9 Neural Network Treinando e avalaindo o modelo: modRedNeural01 = neuralnet(deslocamento ~ sumdistancia, data=disco, hidden = c(2,4,3)) plot(modRedNeural01) ypred = neuralnet::compute(modRedNeural01, disco) yhat = ypred$net.result yhat = round(yhat) yhat=data.frame(&quot;yhat&quot;=ifelse(max.col(yhat[ ,1:2])==1, &quot;0&quot;, &quot;1&quot;)) cm = confusionMatrix(as.factor(yhat$yhat), disco$deslocamento) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 70 7 ## 1 5 22 ## ## Accuracy : 0.8846 ## 95% CI : (0.8071, 0.9389) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 4.877e-05 ## ## Kappa : 0.7069 ## ## Mcnemar&#39;s Test P-Value : 0.7728 ## ## Sensitivity : 0.9333 ## Specificity : 0.7586 ## Pos Pred Value : 0.9091 ## Neg Pred Value : 0.8148 ## Prevalence : 0.7212 ## Detection Rate : 0.6731 ## Detection Prevalence : 0.7404 ## Balanced Accuracy : 0.8460 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modRedNeural01&quot;, &quot;neuralnet&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.10 KNN Treinando e avalaindo o modelo: Para \\(k = 3\\): modKnn3_01 = knn3(deslocamento ~ sumdistancia, data=disco, k=3) predito = predict(modKnn3_01, disco, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=3&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) Para \\(k = 5\\): modKnn5_01 = knn3(deslocamento ~ sumdistancia, data=disco, k=5) predito = predict(modKnn5_01, disco, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 68 9 ## 1 7 20 ## ## Accuracy : 0.8462 ## 95% CI : (0.7622, 0.9094) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.002035 ## ## Kappa : 0.6092 ## ## Mcnemar&#39;s Test P-Value : 0.802587 ## ## Sensitivity : 0.9067 ## Specificity : 0.6897 ## Pos Pred Value : 0.8831 ## Neg Pred Value : 0.7407 ## Prevalence : 0.7212 ## Detection Rate : 0.6538 ## Detection Prevalence : 0.7404 ## Balanced Accuracy : 0.7982 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn5_01-k=5&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.11 Comparando os modelos Model Algorithm Accuracy Sensitivity Specificity modLogist01 glm 0.173076923076923 0.0666666666666667 0.448275862068966 modLogist02-LOOCV glm 0.826923076923077 0.933333333333333 0.551724137931034 modFisher01-prior 0.5 / 0.5 lda 0.865384615384615 0.893333333333333 0.793103448275862 modBayes01-prior 0.65 / 0.35 lda 0.836538461538462 0.933333333333333 0.586206896551724 modNaiveBayes01 naiveBayes 0.826923076923077 0.933333333333333 0.551724137931034 modArvDec01 rpart 0.865384615384615 0.893333333333333 0.793103448275862 modSVM01 svm 0.836538461538462 0.946666666666667 0.551724137931034 modRedNeural01 neuralnet 0.884615384615385 0.933333333333333 0.758620689655172 modKnn3_01-k=3 knn3 0.884615384615385 0.96 0.689655172413793 modKnn5_01-k=5 knn3 0.846153846153846 0.906666666666667 0.689655172413793 Todos os modelos tiveram performance semelhante, com destaque para modFisher01 com \\(prior 0.5, 0.5\\). 3.3.12 Agrupamento discoS = disco[, 2:3] d &lt;- dist(discoS, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) groups &lt;- cutree(grup, k=3) plot(grup, cex = 0.6) rect.hclust(grup , k = 3, border = 2:6) abline(h = 3, col = &#39;red&#39;) kable(sort(groups)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) x 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 Pelo dendograma temos 3 grupos: km1 = kmeans(discoS, 3) p1 = fviz_cluster(km1, data=discoS, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw()) p1 groups = km1$cluster table(groups, disco$deslocamento) ## ## groups 0 1 ## 1 55 3 ## 2 1 12 ## 3 19 14 3.4 Classificação de tipos faciais Os dados do arquivo tipofacial (disponível aqui) foram extraídos de um estudo odontológico realizado pelo Dr. Flávio Cotrim Vellini. Um dos objetivos era utilizar medidas entre diferentes pontos do crânio para caracterizar indivíduos com diferentes tipos faciais, a saber, braquicéfalos, mesocéfalos e dolicocéfalos (grupos). O conjunto de dados contém observações de 11 variáveis em 101 pacientes. Para efeitos didáticos, considere apenas a altura facial (altfac) e a profundidade facial (proffac) como variáveis preditoras. 3.4.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) library(rpart) library(rpart.plot) library(caret) library(MASS) library(httr) library(readxl) library(tibble) library(e1071) library(neuralnet) library(factoextra) library(ggpubr) library(nnet) library(modelr) 3.4.2 Conjunto de dados httr::GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/tipofacial.xls&quot;, httr::write_disk(&quot;../dados/tipofacial.xls&quot;, overwrite = TRUE)) ## Response [https://www.ime.usp.br/~jmsinger/MorettinSinger/tipofacial.xls] ## Date: 2022-12-14 00:18 ## Status: 200 ## Content-Type: application/vnd.ms-excel ## Size: 20.5 kB ## &lt;ON DISK&gt; G:\\onedrive\\obsidian\\adsantos\\Mestrado\\BD\\trabalhos\\caderno-bd\\dados\\tipofacial.xls tipofacial &lt;- read_excel(&quot;../dados/tipofacial.xls&quot;) Número de observações 101. 3.4.2.1 Categorizando a variável de grupo. # Considerar (altfac) e a profundidade facial (proffac) como variáveis preditoras. tipofacial$sumfac = tipofacial$altfac + tipofacial$proffac tipofacial$grupo = as.factor(tipofacial$grupo) plot(tipofacial$sumfac ~ tipofacial$grupo) kable(tipofacial) %&gt;% kable_styling(latex_options = &quot;striped&quot;) paciente sexo grupo idade nsba ns sba altfac proffac eixofac planmand arcomand vert sumfac 10 M braq 5.58 132.0 58.0 36.0 1.2 0.8 0.4 0.4 2.5 1.06 2.0 10 M braq 11.42 134.0 63.0 42.5 1.2 0.4 1.0 1.0 3.6 1.44 1.6 27 F braq 16.17 121.5 77.5 48.0 2.6 0.2 0.3 0.9 3.4 1.48 2.8 39 F braq 4.92 129.5 64.0 34.5 3.1 -1.0 1.9 1.3 1.6 1.38 2.1 39 F braq 10.92 129.5 70.0 36.5 3.1 0.6 1.2 2.2 2.3 1.88 3.7 39 F braq 12.92 128.0 68.5 41.5 3.3 -0.6 1.1 1.2 2.1 1.42 2.7 55 F braq 16.75 130.0 71.0 42.0 2.4 0.3 1.1 1.2 3.5 1.70 2.7 76 F braq 16.00 125.0 72.0 46.5 1.9 0.5 1.4 0.6 3.5 1.58 2.4 77 F braq 17.08 129.5 70.0 44.0 2.1 -0.1 2.2 0.8 0.7 1.14 2.0 133 M braq 14.83 130.0 80.0 52.0 2.8 0.2 0.4 1.1 1.8 1.26 3.0 145 F braq 10.67 118.5 67.0 43.5 1.2 0.7 1.7 1.5 0.9 1.20 1.9 148 F braq 11.33 129.0 68.0 41.5 2.9 -0.7 2.2 1.3 2.2 1.58 2.2 165 F braq 15.00 137.0 74.0 43.0 2.1 -0.3 1.3 1.2 2.5 1.36 1.8 176 M braq 5.67 134.0 65.0 39.0 2.5 0.2 2.1 2.2 3.1 2.02 2.7 176 M braq 13.00 130.0 72.0 48.0 2.2 0.2 2.9 2.4 4.0 2.34 2.4 208 M braq 13.75 130.0 67.5 43.0 0.9 1.5 -0.4 1.2 2.3 1.10 2.4 246 F braq 11.75 129.0 67.0 43.0 0.8 1.1 1.5 0.3 1.4 1.02 1.9 27 F braq 4.67 126.0 63.0 35.0 1.9 0.2 0.3 0.1 1.6 0.82 2.1 39 F braq 3.67 131.0 61.0 32.0 3.0 -1.4 1.8 0.7 0.8 0.98 1.6 45 M braq 22.67 133.0 77.0 46.0 1.7 -0.9 -0.6 0.3 2.7 0.64 0.8 49 F braq 16.58 135.5 71.0 42.0 1.0 0.5 0.1 0.2 2.7 0.90 1.5 52 M braq 12.58 126.5 69.0 41.5 1.4 0.0 0.4 0.5 2.5 0.96 1.4 63 F braq 6.17 130.5 63.0 37.0 1.9 -0.5 0.8 0.5 1.3 0.80 1.4 63 F braq 15.67 132.0 72.5 43.0 1.9 0.0 2.0 0.4 0.3 0.92 1.9 68 M braq 13.83 128.0 72.0 43.0 1.2 -0.6 1.0 1.1 2.3 1.00 0.6 76 F braq 5.33 124.0 61.0 39.5 1.3 -0.3 -0.1 0.6 1.6 0.62 1.0 86 F braq 5.25 126.5 61.5 42.5 1.1 0.0 -0.1 0.2 2.7 0.78 1.1 109 M braq 14.92 131.0 78.5 50.0 1.8 -0.4 1.1 0.8 1.3 0.92 1.4 145 F braq 3.67 129.0 60.0 37.5 1.0 0.0 1.1 0.1 0.4 0.52 1.0 148 F braq 4.25 127.0 61.5 34.0 1.9 0.2 1.6 0.0 1.4 1.02 2.1 155 F braq 5.25 127.0 65.0 39.5 1.2 -0.5 1.2 0.2 1.4 0.70 0.7 155 F braq 11.33 128.0 71.5 45.0 2.3 -0.3 0.7 0.4 2.2 0.80 2.0 194 M braq 5.67 123.0 67.5 39.5 1.7 -0.8 1.0 -0.5 2.3 0.74 0.9 40 M dolico 6.42 121.0 65.0 40.5 -0.5 -2.7 -1.5 -1.4 1.5 -0.92 -3.2 44 M dolico 4.50 124.5 64.0 43.0 -1.9 -1.8 -1.5 -2.8 0.0 -1.60 -3.7 44 M dolico 11.92 124.0 69.0 51.5 -0.3 -1.3 -1.9 -1.7 0.6 -0.92 -1.6 46 M dolico 5.08 123.0 65.0 44.0 -0.5 -1.6 -2.3 -1.3 0.8 -0.98 -2.1 46 M dolico 12.50 126.0 72.5 54.0 -0.3 -0.4 -1.9 -0.8 -0.3 -0.74 -0.7 53 F dolico 10.00 129.5 67.0 44.5 -0.3 -0.4 -0.8 -1.4 0.0 -0.58 -0.7 62 F dolico 5.58 128.0 60.0 36.0 -0.6 -1.8 -1.0 -2.5 -0.5 -1.28 -2.4 65 M dolico 5.58 127.5 66.0 39.0 0.0 -0.8 -0.6 -1.3 -1.9 -0.92 -0.8 65 M dolico 11.00 129.0 72.0 43.5 0.6 -0.8 -0.4 -1.2 -1.2 -0.60 -0.2 85 M dolico 5.58 127.5 65.0 37.0 -0.1 -1.6 -1.2 -2.2 -0.5 -1.12 -1.7 85 M dolico 10.92 127.0 68.0 40.5 -0.9 -1.5 -1.2 -2.4 -1.4 -1.48 -2.4 92 M dolico 5.25 128.0 60.0 39.0 -1.3 -1.1 -1.7 -2.4 -0.4 -1.38 -2.4 92 M dolico 12.50 125.5 68.0 46.0 -0.7 -1.2 -0.8 -1.9 -0.2 -0.96 -1.9 105 M dolico 4.33 124.0 58.0 34.0 -2.3 -0.4 -1.1 -2.0 -0.9 -1.34 -2.7 106 F dolico 5.50 130.0 62.0 36.0 -1.3 -1.0 -1.0 -1.6 0.2 -0.94 -2.3 107 F dolico 5.75 128.0 67.0 38.5 -0.1 -1.7 -0.5 -1.9 0.0 -0.84 -1.8 107 F dolico 10.75 123.0 72.5 44.0 -2.0 -1.1 -1.2 -2.3 -0.6 -1.44 -3.1 113 M dolico 10.42 117.0 61.5 41.0 -1.3 1.0 -2.4 -0.9 0.2 -0.68 -0.3 114 M dolico 4.92 132.0 65.0 39.5 -0.8 -1.9 -1.7 -0.3 1.8 -0.58 -2.7 115 M dolico 5.58 128.5 65.5 45.0 -0.5 -1.8 -0.9 -2.0 -0.9 -1.22 -2.3 128 M dolico 5.58 128.0 65.5 40.0 -0.5 -1.0 -0.5 -1.1 -0.1 -0.64 -1.5 131 M dolico 6.92 119.5 75.5 42.0 -0.6 -1.0 0.2 -1.8 -1.7 -0.98 -1.6 131 M dolico 12.92 121.0 81.0 44.5 0.0 -0.7 0.6 -1.5 -2.3 -0.78 -0.7 182 M dolico 4.25 123.0 64.0 35.0 -0.6 0.1 -0.3 -1.4 -1.2 -0.68 -0.5 182 M dolico 10.25 122.0 69.5 44.0 -0.7 -0.9 -0.7 -2.3 0.2 -0.88 -1.6 183 M dolico 4.25 132.0 62.0 36.0 -1.3 -2.1 -0.7 -1.6 -1.0 -1.34 -3.4 183 M dolico 11.33 124.0 66.5 43.0 -0.1 -1.8 -0.3 -2.2 -1.8 -1.24 -1.9 197 F dolico 3.67 126.0 64.0 38.0 -1.0 -1.6 -1.9 -2.4 -0.7 -1.52 -2.6 197 F dolico 5.67 128.5 66.0 41.0 -1.3 -2.7 -2.0 -2.9 -0.7 -1.92 -4.0 203 F dolico 5.25 125.0 59.0 39.5 -1.5 -1.8 -3.2 -2.6 -0.2 -1.86 -3.3 203 F dolico 12.33 130.5 63.0 46.5 -0.8 0.2 -3.5 -1.2 0.3 -1.00 -0.6 40 M meso 14.58 125.0 74.0 48.5 0.6 -1.1 -1.1 0.1 2.8 0.26 -0.5 45 M meso 6.33 134.5 66.0 35.5 1.1 -1.3 -1.0 -1.1 0.9 -0.28 -0.2 45 M meso 12.75 136.5 76.0 46.5 2.2 -0.9 -0.7 -0.4 1.0 0.24 1.3 49 F meso 5.08 131.5 66.0 36.0 0.7 -1.2 0.0 -1.0 0.6 -0.18 -0.5 52 M meso 5.00 126.0 61.5 36.5 0.0 -1.6 -0.2 1.8 -0.3 -0.06 -1.6 53 F meso 6.00 126.5 65.0 39.0 0.0 -0.9 -0.6 -1.6 1.2 -0.38 -0.9 55 F meso 5.25 126.5 63.0 36.5 0.6 -0.7 -0.3 -0.2 0.9 0.06 -0.1 61 F meso 5.00 129.0 59.0 35.0 0.2 -0.4 0.8 0.5 0.5 0.32 -0.2 61 F meso 12.42 129.5 67.5 41.5 -0.1 -0.8 -0.1 -0.3 0.6 -0.14 -0.9 62 F meso 17.00 121.5 66.0 43.0 0.8 0.0 0.4 -0.2 1.4 0.48 0.8 68 M meso 4.42 127.0 63.0 36.0 0.1 -0.9 0.1 1.0 0.9 0.24 -0.8 77 F meso 5.58 129.0 62.0 39.5 0.7 -1.2 0.7 1.0 -0.9 -0.12 -0.5 86 F meso 14.58 126.5 71.0 50.5 0.1 -0.7 -0.5 -0.3 2.6 0.24 -0.6 102 F meso 3.83 129.0 61.0 35.0 0.4 -0.8 -1.0 -1.1 0.8 -0.34 -0.4 102 F meso 11.58 134.0 69.5 45.0 0.5 -0.2 -1.2 -0.7 1.5 -0.02 0.3 103 M meso 5.75 122.0 65.5 41.0 0.3 -0.4 -0.4 -0.9 -0.2 -0.32 -0.1 103 M meso 14.33 123.0 72.0 49.5 0.9 -0.6 0.3 -1.0 0.4 0.00 0.3 105 M meso 15.83 126.0 68.0 44.0 0.1 1.1 0.4 -1.3 1.0 0.26 1.2 106 F meso 11.25 131.0 65.5 42.5 -0.4 -0.2 -1.0 -1.2 0.6 -0.44 -0.6 109 M meso 4.83 127.0 71.0 41.5 1.9 -0.6 0.2 -0.2 0.3 0.32 1.3 110 M meso 4.75 127.5 67.0 40.5 1.2 -1.9 -0.3 -0.8 0.1 -0.34 -0.7 110 M meso 12.75 125.0 74.0 48.0 1.7 -1.1 -0.1 -0.1 1.1 0.30 0.6 113 M meso 4.42 120.0 57.0 39.0 0.2 1.0 -1.4 -0.5 0.6 -0.02 1.2 114 M meso 13.00 130.0 69.0 46.5 -0.5 -1.2 -1.7 -0.2 1.5 -0.42 -1.7 115 M meso 14.58 126.0 72.0 49.5 0.8 0.1 -0.6 -1.1 0.1 -0.14 0.9 127 M meso 5.33 135.5 58.5 36.5 0.7 -0.8 -0.3 -1.1 0.7 -0.16 -0.1 127 M meso 11.33 136.0 64.0 39.5 1.7 -1.1 0.0 -1.7 1.3 0.04 0.6 128 M meso 10.50 125.0 70.5 43.0 0.0 0.0 -0.3 -0.4 0.2 -0.10 0.0 133 M meso 5.83 127.0 71.0 40.0 1.8 -1.6 0.3 0.0 1.4 0.38 0.2 165 F meso 4.58 133.0 64.0 38.5 1.7 -0.8 0.8 -0.3 0.0 0.28 0.9 194 M meso 10.58 119.5 73.0 46.0 1.3 -0.6 0.7 -0.5 0.4 0.26 0.7 197 F meso 10.75 130.5 71.0 41.5 0.6 -0.7 -0.9 -0.8 1.0 -0.16 -0.1 197 F meso 12.75 127.0 71.5 49.0 0.8 -1.2 -1.5 -1.2 1.0 -0.42 -0.4 208 M meso 7.75 129.5 62.5 38.5 0.4 0.6 -1.0 -0.2 1.4 0.24 1.0 214 F meso 6.33 127.0 65.5 36.0 0.4 -1.3 1.4 -1.1 -0.3 -0.18 -0.9 214 F meso 11.50 128.5 69.5 41.5 0.3 0.1 0.8 -0.7 -0.1 0.08 0.4 246 F meso 5.67 131.0 59.0 37.5 0.1 0.1 1.2 -0.7 1.7 0.48 0.2 kable(summary(tipofacial)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) paciente sexo grupo idade nsba ns sba altfac proffac eixofac planmand arcomand vert sumfac Min. : 10.0 Length:101 braq :33 Min. : 3.670 Min. :117.0 Min. :57.00 Min. :32.0 Min. :-2.3000 Min. :-2.7000 Min. :-3.5000 Min. :-2.9000 Min. :-2.3000 Min. :-1.92000 Min. :-4.00000 1st Qu.: 61.0 Class :character dolico:31 1st Qu.: 5.250 1st Qu.:125.0 1st Qu.:63.00 1st Qu.:38.5 1st Qu.:-0.3000 1st Qu.:-1.2000 1st Qu.:-1.0000 1st Qu.:-1.3000 1st Qu.: 0.0000 1st Qu.:-0.68000 1st Qu.:-0.90000 Median :105.0 Mode :character meso :37 Median :10.000 Median :128.0 Median :66.50 Median :41.5 Median : 0.6000 Median :-0.7000 Median :-0.2000 Median :-0.5000 Median : 0.8000 Median : 0.00000 Median :-0.10000 Mean :108.4 NA NA Mean : 9.204 Mean :127.7 Mean :67.03 Mean :41.5 Mean : 0.6238 Mean :-0.6119 Mean :-0.1129 Mean :-0.4693 Mean : 0.8386 Mean : 0.04931 Mean : 0.01188 3rd Qu.:148.0 NA NA 3rd Qu.:12.580 3rd Qu.:130.0 3rd Qu.:71.00 3rd Qu.:44.0 3rd Qu.: 1.7000 3rd Qu.: 0.0000 3rd Qu.: 0.8000 3rd Qu.: 0.4000 3rd Qu.: 1.6000 3rd Qu.: 0.80000 3rd Qu.: 1.40000 Max. :246.0 NA NA Max. :22.670 Max. :137.0 Max. :81.00 Max. :54.0 Max. : 3.3000 Max. : 1.5000 Max. : 2.9000 Max. : 2.4000 Max. : 4.0000 Max. : 2.34000 Max. : 3.70000 Distribuição: x = 1:nrow(tipofacial) plot(tipofacial$sumfac ~ x, col = tipofacial$grupo) 3.4.3 Separando o conjunto de dados para treinamento Separando os dados de treinamento e testes utilizando kfold: folds = 5 cv &lt;- crossv_kfold(tipofacial, k = folds) cv ## # A tibble: 5 × 3 ## train test .id ## &lt;named list&gt; &lt;named list&gt; &lt;chr&gt; ## 1 &lt;resample [80 x 14]&gt; &lt;resample [21 x 14]&gt; 1 ## 2 &lt;resample [81 x 14]&gt; &lt;resample [20 x 14]&gt; 2 ## 3 &lt;resample [81 x 14]&gt; &lt;resample [20 x 14]&gt; 3 ## 4 &lt;resample [81 x 14]&gt; &lt;resample [20 x 14]&gt; 4 ## 5 &lt;resample [81 x 14]&gt; &lt;resample [20 x 14]&gt; 5 Separando dados de treinamento (70%) e testes (30%). alpha=0.7 d = sort(sample(nrow(tipofacial), nrow(tipofacial)*alpha)) train = tipofacial[d,] test = tipofacial[-d,] 3.4.4 Generalized Linear Models Treinando os modelos: glm.fit = map(cv$train, ~multinom(grupo ~ sumfac, data= .)) ## # weights: 9 (4 variable) ## initial value 87.888983 ## iter 10 value 28.997786 ## final value 28.982608 ## converged ## # weights: 9 (4 variable) ## initial value 88.987595 ## iter 10 value 30.767060 ## final value 30.644565 ## converged ## # weights: 9 (4 variable) ## initial value 88.987595 ## iter 10 value 28.608975 ## final value 28.590407 ## converged ## # weights: 9 (4 variable) ## initial value 88.987595 ## iter 10 value 30.946120 ## final value 30.473343 ## converged ## # weights: 9 (4 variable) ## initial value 88.987595 ## iter 10 value 32.122918 ## final value 32.013001 ## converged Predição com os modelos: preditos &lt;- map2_df(glm.fit, cv$test, get_pred, .id = &quot;Run&quot;) for (run in 1:folds) { pred &lt;- preditos %&gt;% filter(Run == run) cm = confusionMatrix(pred$pred, pred$grupo) cat(&quot;\\n&quot;, &quot;--- Run &quot;, run, &quot;---&quot;, &quot;\\n&quot;) print(cm) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(paste(&quot;glm.fit&quot;, run, sep=&quot;-&quot;), &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) run &lt;- run + 1 } ## ## --- Run 1 --- ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 7 0 0 ## dolico 0 3 0 ## meso 1 3 7 ## ## Overall Statistics ## ## Accuracy : 0.8095 ## 95% CI : (0.5809, 0.9455) ## No Information Rate : 0.381 ## P-Value [Acc &gt; NIR] : 7.582e-05 ## ## Kappa : 0.7103 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8750 0.5000 1.0000 ## Specificity 1.0000 1.0000 0.7143 ## Pos Pred Value 1.0000 1.0000 0.6364 ## Neg Pred Value 0.9286 0.8333 1.0000 ## Prevalence 0.3810 0.2857 0.3333 ## Detection Rate 0.3333 0.1429 0.3333 ## Detection Prevalence 0.3333 0.1429 0.5238 ## Balanced Accuracy 0.9375 0.7500 0.8571 ## ## --- Run 2 --- ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 5 0 4 ## dolico 0 4 1 ## meso 0 0 6 ## ## Overall Statistics ## ## Accuracy : 0.75 ## 95% CI : (0.509, 0.9134) ## No Information Rate : 0.55 ## P-Value [Acc &gt; NIR] : 0.05533 ## ## Kappa : 0.6283 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 1.0000 1.0000 0.5455 ## Specificity 0.7333 0.9375 1.0000 ## Pos Pred Value 0.5556 0.8000 1.0000 ## Neg Pred Value 1.0000 1.0000 0.6429 ## Prevalence 0.2500 0.2000 0.5500 ## Detection Rate 0.2500 0.2000 0.3000 ## Detection Prevalence 0.4500 0.2500 0.3000 ## Balanced Accuracy 0.8667 0.9688 0.7727 ## ## --- Run 3 --- ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 6 0 0 ## dolico 0 3 2 ## meso 1 3 5 ## ## Overall Statistics ## ## Accuracy : 0.7 ## 95% CI : (0.4572, 0.8811) ## No Information Rate : 0.35 ## P-Value [Acc &gt; NIR] : 0.001521 ## ## Kappa : 0.5472 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8571 0.5000 0.7143 ## Specificity 1.0000 0.8571 0.6923 ## Pos Pred Value 1.0000 0.6000 0.5556 ## Neg Pred Value 0.9286 0.8000 0.8182 ## Prevalence 0.3500 0.3000 0.3500 ## Detection Rate 0.3000 0.1500 0.2500 ## Detection Prevalence 0.3000 0.2500 0.4500 ## Balanced Accuracy 0.9286 0.6786 0.7033 ## ## --- Run 4 --- ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 3 0 2 ## dolico 0 8 0 ## meso 0 2 5 ## ## Overall Statistics ## ## Accuracy : 0.8 ## 95% CI : (0.5634, 0.9427) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : 0.005909 ## ## Kappa : 0.6875 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 1.0000 0.8000 0.7143 ## Specificity 0.8824 1.0000 0.8462 ## Pos Pred Value 0.6000 1.0000 0.7143 ## Neg Pred Value 1.0000 0.8333 0.8462 ## Prevalence 0.1500 0.5000 0.3500 ## Detection Rate 0.1500 0.4000 0.2500 ## Detection Prevalence 0.2500 0.4000 0.3500 ## Balanced Accuracy 0.9412 0.9000 0.7802 ## ## --- Run 5 --- ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 7 0 0 ## dolico 0 5 1 ## meso 3 0 4 ## ## Overall Statistics ## ## Accuracy : 0.8 ## 95% CI : (0.5634, 0.9427) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : 0.005909 ## ## Kappa : 0.6981 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.7000 1.0000 0.8000 ## Specificity 1.0000 0.9333 0.8000 ## Pos Pred Value 1.0000 0.8333 0.5714 ## Neg Pred Value 0.7692 1.0000 0.9231 ## Prevalence 0.5000 0.2500 0.2500 ## Detection Rate 0.3500 0.2500 0.2000 ## Detection Prevalence 0.3500 0.3000 0.3500 ## Balanced Accuracy 0.8500 0.9667 0.8000 Treinando o modelo: glm.fit = multinom(grupo ~ sumfac, data=train) ## # weights: 9 (4 variable) ## initial value 76.902860 ## iter 10 value 25.113276 ## final value 24.783162 ## converged summary(glm.fit) ## Call: ## multinom(formula = grupo ~ sumfac, data = train) ## ## Coefficients: ## (Intercept) sumfac ## dolico 1.116711 -6.284307 ## meso 3.500467 -4.001364 ## ## Std. Errors: ## (Intercept) sumfac ## dolico 1.471414 1.527252 ## meso 1.300203 1.338159 ## ## Residual Deviance: 49.56632 ## AIC: 57.56632 predito = predict(glm.fit, newdata=test) cm = confusionMatrix(predito, test$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 6 0 4 ## dolico 0 9 1 ## meso 1 3 7 ## ## Overall Statistics ## ## Accuracy : 0.7097 ## 95% CI : (0.5196, 0.8578) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 0.0002756 ## ## Kappa : 0.5634 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8571 0.7500 0.5833 ## Specificity 0.8333 0.9474 0.7895 ## Pos Pred Value 0.6000 0.9000 0.6364 ## Neg Pred Value 0.9524 0.8571 0.7500 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.1935 0.2903 0.2258 ## Detection Prevalence 0.3226 0.3226 0.3548 ## Balanced Accuracy 0.8452 0.8487 0.6864 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;glm.fit-split&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) kable(model_eval) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Model Algorithm Accuracy Sensitivity_C1 Sensitivity_C2 Sensitivity_C3 Specificity_C1 Specificity_C2 Specificity_C3 glm.fit-1 glm 0.80952380952381 0.875 0.5 1 1 1 0.714285714285714 glm.fit-2 glm 0.75 1 1 0.545454545454545 0.733333333333333 0.9375 1 glm.fit-3 glm 0.7 0.857142857142857 0.5 0.714285714285714 1 0.857142857142857 0.692307692307692 glm.fit-4 glm 0.8 1 0.8 0.714285714285714 0.882352941176471 1 0.846153846153846 glm.fit-5 glm 0.8 0.7 1 0.8 1 0.933333333333333 0.8 glm.fit-split glm 0.709677419354839 0.857142857142857 0.75 0.583333333333333 0.833333333333333 0.947368421052632 0.789473684210526 Treinar o modelo com um conjunto de dados diferente do de teste mostrou que o modelo tem uma capacidade razoável de generalização. 3.4.5 Linear Discriminant Analysis - Fisher Treinando o modelo: modFisher01 = lda(grupo ~ sumfac, data = tipofacial) predito = predict(modFisher01) classPred = predito$class cm = confusionMatrix(classPred, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 29 0 5 ## dolico 0 23 2 ## meso 4 8 30 ## ## Overall Statistics ## ## Accuracy : 0.8119 ## 95% CI : (0.7219, 0.8828) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7157 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8788 0.7419 0.8108 ## Specificity 0.9265 0.9714 0.8125 ## Pos Pred Value 0.8529 0.9200 0.7143 ## Neg Pred Value 0.9403 0.8947 0.8814 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2871 0.2277 0.2970 ## Detection Prevalence 0.3366 0.2475 0.4158 ## Balanced Accuracy 0.9026 0.8567 0.8117 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modFisher01&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando com conjunto de dados de treinamento e testes: modFisher01 = lda(grupo ~ sumfac, data=train) predito_test = as.data.frame(predict(modFisher01, test)) predito_test = predito_test[c(5, 1)] cm = confusionMatrix(predito_test$class, test$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 6 0 4 ## dolico 0 9 1 ## meso 1 3 7 ## ## Overall Statistics ## ## Accuracy : 0.7097 ## 95% CI : (0.5196, 0.8578) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 0.0002756 ## ## Kappa : 0.5634 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8571 0.7500 0.5833 ## Specificity 0.8333 0.9474 0.7895 ## Pos Pred Value 0.6000 0.9000 0.6364 ## Neg Pred Value 0.9524 0.8571 0.7500 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.1935 0.2903 0.2258 ## Detection Prevalence 0.3226 0.3226 0.3548 ## Balanced Accuracy 0.8452 0.8487 0.6864 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modFisher01-split&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.6 Bayes Treinando o modelo: modBayes01 = lda(grupo ~ sumfac, data=tipofacial, prior=c(0.25, 0.50, 0.25)) predito = predict(modBayes01) classPred = predito$class cm = confusionMatrix(classPred, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 29 0 5 ## dolico 0 24 6 ## meso 4 7 26 ## ## Overall Statistics ## ## Accuracy : 0.7822 ## 95% CI : (0.689, 0.8582) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.6723 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8788 0.7742 0.7027 ## Specificity 0.9265 0.9143 0.8281 ## Pos Pred Value 0.8529 0.8000 0.7027 ## Neg Pred Value 0.9403 0.9014 0.8281 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2871 0.2376 0.2574 ## Detection Prevalence 0.3366 0.2970 0.3663 ## Balanced Accuracy 0.9026 0.8442 0.7654 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modBayes01-prior 0.25 / 0.50 / 0.25&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando com conjunto de dados de treinamento e testes: modBayes01 = lda(grupo ~ sumfac, data=train, prior=c(0.25, 0.50, 0.25)) predito_test = as.data.frame(predict(modBayes01, test)) predito_test = predito_test[c(5, 1)] cm = confusionMatrix(predito_test$class, test$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 6 0 3 ## dolico 0 9 1 ## meso 1 3 8 ## ## Overall Statistics ## ## Accuracy : 0.7419 ## 95% CI : (0.5539, 0.8814) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 6.52e-05 ## ## Kappa : 0.6088 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8571 0.7500 0.6667 ## Specificity 0.8750 0.9474 0.7895 ## Pos Pred Value 0.6667 0.9000 0.6667 ## Neg Pred Value 0.9545 0.8571 0.7895 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.1935 0.2903 0.2581 ## Detection Prevalence 0.2903 0.3226 0.3871 ## Balanced Accuracy 0.8661 0.8487 0.7281 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modBayes01-prior-0.25/0.50/0.25-split&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.7 Naive Bayes Treinando o modelo: modNaiveBayes01 = naiveBayes(grupo ~ sumfac, data=tipofacial) predito = predict(modNaiveBayes01, tipofacial) cm = confusionMatrix(predito, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 29 0 5 ## dolico 0 23 2 ## meso 4 8 30 ## ## Overall Statistics ## ## Accuracy : 0.8119 ## 95% CI : (0.7219, 0.8828) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7157 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8788 0.7419 0.8108 ## Specificity 0.9265 0.9714 0.8125 ## Pos Pred Value 0.8529 0.9200 0.7143 ## Neg Pred Value 0.9403 0.8947 0.8814 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2871 0.2277 0.2970 ## Detection Prevalence 0.3366 0.2475 0.4158 ## Balanced Accuracy 0.9026 0.8567 0.8117 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modNaiveBayes01&quot;, &quot;naiveBayes&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando com conjunto de dados de treinamento e testes: modNaiveBayes01 = naiveBayes(grupo ~ sumfac, data=train) predito_test = predict(modNaiveBayes01, test) cm = confusionMatrix(predito_test, test$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 6 0 4 ## dolico 0 9 1 ## meso 1 3 7 ## ## Overall Statistics ## ## Accuracy : 0.7097 ## 95% CI : (0.5196, 0.8578) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 0.0002756 ## ## Kappa : 0.5634 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8571 0.7500 0.5833 ## Specificity 0.8333 0.9474 0.7895 ## Pos Pred Value 0.6000 0.9000 0.6364 ## Neg Pred Value 0.9524 0.8571 0.7500 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.1935 0.2903 0.2258 ## Detection Prevalence 0.3226 0.3226 0.3548 ## Balanced Accuracy 0.8452 0.8487 0.6864 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modNaiveBayes01-split&quot;, &quot;naiveBayes&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.8 Decison tree Treinando o modelo: modArvDec01 = rpart(grupo ~ sumfac, data = tipofacial) prp(modArvDec01, faclen=0, #use full names for factor labels extra=1, #display number of observations for each terminal node roundint=F, #don&#39;t round to integers in output digits=5) predito = predict(modArvDec01, type = &quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 26 0 0 ## dolico 0 23 2 ## meso 7 8 35 ## ## Overall Statistics ## ## Accuracy : 0.8317 ## 95% CI : (0.7442, 0.8988) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7444 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.7879 0.7419 0.9459 ## Specificity 1.0000 0.9714 0.7656 ## Pos Pred Value 1.0000 0.9200 0.7000 ## Neg Pred Value 0.9067 0.8947 0.9608 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2574 0.2277 0.3465 ## Detection Prevalence 0.2574 0.2475 0.4950 ## Balanced Accuracy 0.8939 0.8567 0.8558 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modArvDec01&quot;, &quot;rpart&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando com conjunto de dados de treinamento e testes: modArvDec01 = rpart(grupo ~ sumfac, data=train) prp(modArvDec01, faclen=0, #use full names for factor labels extra=1, #display number of observations for each terminal node roundint=F, #don&#39;t round to integers in output digits=5) predito_test = predict(modArvDec01, test, type = &quot;class&quot;) cm = confusionMatrix(predito_test, test$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 7 0 6 ## dolico 0 9 1 ## meso 0 3 5 ## ## Overall Statistics ## ## Accuracy : 0.6774 ## 95% CI : (0.4863, 0.8332) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 0.001009 ## ## Kappa : 0.526 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 1.0000 0.7500 0.4167 ## Specificity 0.7500 0.9474 0.8421 ## Pos Pred Value 0.5385 0.9000 0.6250 ## Neg Pred Value 1.0000 0.8571 0.6957 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.2258 0.2903 0.1613 ## Detection Prevalence 0.4194 0.3226 0.2581 ## Balanced Accuracy 0.8750 0.8487 0.6294 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modArvDec01-split&quot;, &quot;rpart&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.9 SVM Treinando o modelo: modSVM01 = svm(grupo ~ sumfac, data=tipofacial, kernel = &quot;linear&quot;) predito = predict(modSVM01, type = &quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modSVM01&quot;, &quot;svm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando com conjunto de dados de treinamento e testes: modSVM01 = svm(grupo ~ sumfac, data=train, kernel = &quot;linear&quot;) predito_test = predict(modSVM01, test, type = &quot;class&quot;) cm = confusionMatrix(predito_test, test$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 6 0 4 ## dolico 0 9 1 ## meso 1 3 7 ## ## Overall Statistics ## ## Accuracy : 0.7097 ## 95% CI : (0.5196, 0.8578) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 0.0002756 ## ## Kappa : 0.5634 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8571 0.7500 0.5833 ## Specificity 0.8333 0.9474 0.7895 ## Pos Pred Value 0.6000 0.9000 0.6364 ## Neg Pred Value 0.9524 0.8571 0.7500 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.1935 0.2903 0.2258 ## Detection Prevalence 0.3226 0.3226 0.3548 ## Balanced Accuracy 0.8452 0.8487 0.6864 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modSVM01-split&quot;, &quot;svm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.10 Neural networks Treinando o modelo: modRedNeural01 = neuralnet(grupo ~ sumfac, data = tipofacial, hidden = c(2,4,3)) plot(modRedNeural01) ypred = neuralnet::compute(modRedNeural01, tipofacial) yhat = ypred$net.result yhat = round(yhat) yhat=data.frame(&quot;yhat&quot;= dplyr::case_when(yhat[ ,1:1]==1 ~ &quot;braq&quot;, yhat[ ,2:2]==1 ~ &quot;dolico&quot;, TRUE ~ &quot;meso&quot;)) cm = confusionMatrix(as.factor(yhat$yhat), tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 29 0 5 ## dolico 0 23 2 ## meso 4 8 30 ## ## Overall Statistics ## ## Accuracy : 0.8119 ## 95% CI : (0.7219, 0.8828) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7157 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8788 0.7419 0.8108 ## Specificity 0.9265 0.9714 0.8125 ## Pos Pred Value 0.8529 0.9200 0.7143 ## Neg Pred Value 0.9403 0.8947 0.8814 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2871 0.2277 0.2970 ## Detection Prevalence 0.3366 0.2475 0.4158 ## Balanced Accuracy 0.9026 0.8567 0.8117 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modRedNeural01&quot;, &quot;neuralnet&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando com conjunto de dados de treinamento e testes: modRedNeural01 = neuralnet(grupo ~ sumfac, data=train, hidden = c(2,4,3)) ypred = neuralnet::compute(modRedNeural01, test) yhat = ypred$net.result yhat = round(yhat) yhat=data.frame(&quot;yhat&quot;= dplyr::case_when(yhat[ ,1:1]==1 ~ &quot;braq&quot;, yhat[ ,2:2]==1 ~ &quot;dolico&quot;, TRUE ~ &quot;meso&quot;)) cm = confusionMatrix(as.factor(yhat$yhat), test$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 7 0 4 ## dolico 0 9 1 ## meso 0 3 7 ## ## Overall Statistics ## ## Accuracy : 0.7419 ## 95% CI : (0.5539, 0.8814) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 6.52e-05 ## ## Kappa : 0.6149 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 1.0000 0.7500 0.5833 ## Specificity 0.8333 0.9474 0.8421 ## Pos Pred Value 0.6364 0.9000 0.7000 ## Neg Pred Value 1.0000 0.8571 0.7619 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.2258 0.2903 0.2258 ## Detection Prevalence 0.3548 0.3226 0.3226 ## Balanced Accuracy 0.9167 0.8487 0.7127 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modRedNeural01-split&quot;, &quot;neuralnet&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.11 KNN Treinando o modelo com \\(k = 3\\): modKnn3_01 = knn3(grupo ~ sumfac, data=tipofacial, k=3) predito = predict(modKnn3_01, tipofacial, type = &quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo, positive=&quot;0&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=3&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando com conjunto de dados de treinamento e testes: modKnn3_01 = knn3(grupo ~ sumfac, data=train, k=3) predito_test = predict(modKnn3_01, test, type = &quot;class&quot;) cm = confusionMatrix(predito_test, test$grupo, positive=&quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 6 0 4 ## dolico 0 9 1 ## meso 1 3 7 ## ## Overall Statistics ## ## Accuracy : 0.7097 ## 95% CI : (0.5196, 0.8578) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 0.0002756 ## ## Kappa : 0.5634 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8571 0.7500 0.5833 ## Specificity 0.8333 0.9474 0.7895 ## Pos Pred Value 0.6000 0.9000 0.6364 ## Neg Pred Value 0.9524 0.8571 0.7500 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.1935 0.2903 0.2258 ## Detection Prevalence 0.3226 0.3226 0.3548 ## Balanced Accuracy 0.8452 0.8487 0.6864 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=3-split&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando o modelo com \\(k = 5\\): modKnn5_01 = knn3(grupo ~ sumfac, data=tipofacial, k=5) predito = predict(modKnn5_01, tipofacial, type=&quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo, positive=&quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 30 0 2 ## dolico 0 27 4 ## meso 3 4 31 ## ## Overall Statistics ## ## Accuracy : 0.8713 ## 95% CI : (0.79, 0.9296) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.8063 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.9091 0.8710 0.8378 ## Specificity 0.9706 0.9429 0.8906 ## Pos Pred Value 0.9375 0.8710 0.8158 ## Neg Pred Value 0.9565 0.9429 0.9048 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2970 0.2673 0.3069 ## Detection Prevalence 0.3168 0.3069 0.3762 ## Balanced Accuracy 0.9398 0.9069 0.8642 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=5&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Treinando com conjunto de dados de treinamento e testes: modKnn5_01 = knn3(grupo ~ sumfac, data=train, k=5) predito_test = predict(modKnn5_01, test, type = &quot;class&quot;) cm = confusionMatrix(predito_test, test$grupo, positive=&quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 7 0 5 ## dolico 0 9 1 ## meso 0 3 6 ## ## Overall Statistics ## ## Accuracy : 0.7097 ## 95% CI : (0.5196, 0.8578) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 0.0002756 ## ## Kappa : 0.5701 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 1.0000 0.7500 0.5000 ## Specificity 0.7917 0.9474 0.8421 ## Pos Pred Value 0.5833 0.9000 0.6667 ## Neg Pred Value 1.0000 0.8571 0.7273 ## Prevalence 0.2258 0.3871 0.3871 ## Detection Rate 0.2258 0.2903 0.1935 ## Detection Prevalence 0.3871 0.3226 0.2903 ## Balanced Accuracy 0.8958 0.8487 0.6711 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn5_01-k=5-split&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.12 Comparando os modelos Model Algorithm Accuracy Sensitivity_C1 Sensitivity_C2 Sensitivity_C3 Specificity_C1 Specificity_C2 Specificity_C3 glm.fit-1 glm 0.80952380952381 0.875 0.5 1 1 1 0.714285714285714 glm.fit-2 glm 0.75 1 1 0.545454545454545 0.733333333333333 0.9375 1 glm.fit-3 glm 0.7 0.857142857142857 0.5 0.714285714285714 1 0.857142857142857 0.692307692307692 glm.fit-4 glm 0.8 1 0.8 0.714285714285714 0.882352941176471 1 0.846153846153846 glm.fit-5 glm 0.8 0.7 1 0.8 1 0.933333333333333 0.8 glm.fit-split glm 0.709677419354839 0.857142857142857 0.75 0.583333333333333 0.833333333333333 0.947368421052632 0.789473684210526 modFisher01 lda 0.811881188118812 0.878787878787879 0.741935483870968 0.810810810810811 0.926470588235294 0.971428571428571 0.8125 modFisher01-split lda 0.709677419354839 0.857142857142857 0.75 0.583333333333333 0.833333333333333 0.947368421052632 0.789473684210526 modBayes01-prior 0.25 / 0.50 / 0.25 lda 0.782178217821782 0.878787878787879 0.774193548387097 0.702702702702703 0.926470588235294 0.914285714285714 0.828125 modBayes01-prior-0.25/0.50/0.25-split lda 0.741935483870968 0.857142857142857 0.75 0.666666666666667 0.875 0.947368421052632 0.789473684210526 modNaiveBayes01 naiveBayes 0.811881188118812 0.878787878787879 0.741935483870968 0.810810810810811 0.926470588235294 0.971428571428571 0.8125 modNaiveBayes01-split naiveBayes 0.709677419354839 0.857142857142857 0.75 0.583333333333333 0.833333333333333 0.947368421052632 0.789473684210526 modArvDec01 rpart 0.831683168316832 0.787878787878788 0.741935483870968 0.945945945945946 1 0.971428571428571 0.765625 modArvDec01-split rpart 0.67741935483871 1 0.75 0.416666666666667 0.75 0.947368421052632 0.842105263157895 modSVM01 svm 0.801980198019802 0.818181818181818 0.741935483870968 0.837837837837838 0.941176470588235 0.971428571428571 0.78125 modSVM01-split svm 0.709677419354839 0.857142857142857 0.75 0.583333333333333 0.833333333333333 0.947368421052632 0.789473684210526 modRedNeural01 neuralnet 0.811881188118812 0.878787878787879 0.741935483870968 0.810810810810811 0.926470588235294 0.971428571428571 0.8125 modRedNeural01-split neuralnet 0.741935483870968 1 0.75 0.583333333333333 0.833333333333333 0.947368421052632 0.842105263157895 modKnn3_01-k=3 knn3 0.861386138613861 0.909090909090909 0.838709677419355 0.837837837837838 0.955882352941177 0.957142857142857 0.875 modKnn3_01-k=3-split knn3 0.709677419354839 0.857142857142857 0.75 0.583333333333333 0.833333333333333 0.947368421052632 0.789473684210526 modKnn3_01-k=5 knn3 0.871287128712871 0.909090909090909 0.870967741935484 0.837837837837838 0.970588235294118 0.942857142857143 0.890625 modKnn5_01-k=5-split knn3 0.709677419354839 1 0.75 0.5 0.791666666666667 0.947368421052632 0.842105263157895 A estratégia de treinamento para o modelo glm foi aplicar dois métodos de treinamento: kfold e dividir o conjunto de dados em 70% para treinamento e 30% para testes. O método de kfold (execução 4 - glm.fit-4) obteve o melhor resultado, superior a simples divisão dos dados (split). Para os demais modelos o treinamento foi com toda a base e com a divisão 70/30 (split). Em geral, quando os dados foram divididos, os modelos obtiveram um resultado pior, o que sugere que a estratégia em diversificar os dados para treinamento ajuda os modelos a generalizar melhor. 3.4.13 Agrupamento tipofacialS = subset(tipofacial, select=c(&quot;sexo&quot;, &quot;idade&quot;, &quot;altfac&quot;, &quot;proffac&quot;)) dummy &lt;- dummyVars(&quot; ~ .&quot;, data=tipofacialS) newdata &lt;- data.frame(predict(dummy, newdata = tipofacialS)) d &lt;- dist(newdata, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) groups &lt;- cutree(grup, k=3) plot(grup, cex = 0.6) rect.hclust(grup , k = 3, border = 2:6) abline(h = 3, col = &#39;red&#39;) kable(sort(groups)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) x 1 1 4 1 14 1 18 1 19 1 23 1 26 1 27 1 29 1 30 1 31 1 33 1 34 1 35 1 37 1 40 1 41 1 43 1 45 1 47 1 48 1 49 1 52 1 53 1 54 1 55 1 57 1 59 1 61 1 62 1 63 1 66 1 68 1 69 1 70 1 71 1 72 1 75 1 76 1 78 1 80 1 84 1 85 1 87 1 90 1 93 1 94 1 98 1 99 1 101 1 2 2 5 2 6 2 11 2 12 2 15 2 17 2 22 2 32 2 36 2 38 2 39 2 42 2 44 2 46 2 50 2 51 2 56 2 58 2 60 2 64 2 67 2 73 2 79 2 83 2 86 2 88 2 91 2 92 2 95 2 96 2 97 2 100 2 3 3 7 3 8 3 9 3 10 3 13 3 16 3 20 3 21 3 24 3 25 3 28 3 65 3 74 3 77 3 81 3 82 3 89 3 Pelo dendograma, podemos particionar em 3 clusters: km1 = kmeans(newdata, 3) p1 = fviz_cluster(km1, data=newdata, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw()) p1 groups = km1$cluster table(groups, tipofacial$grupo) ## ## groups braq dolico meso ## 1 21 12 18 ## 2 4 9 9 ## 3 8 10 10 3.5 Análise do conjunto de dados USArrests Análise de agrupamento para os dados USArrests. 3.5.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) library(factoextra) 3.5.2 Conjunto de dados USArrests é um conjunto de dados contendo estatísticas de prisões por 100.000 habitantes por agressão, assassinato e estupro em cada um dos 50 estados dos EUA em 1973. Também é fornecida a porcentagem da população que vive em áreas urbanas. summary(USArrests) require(graphics) pairs(USArrests, panel = panel.smooth, main = &quot;USArrests data&quot;) ## Difference between &#39;USArrests&#39; and its correction USArrests[&quot;Maryland&quot;, &quot;UrbanPop&quot;] # 67 -- the transcription error UA.C &lt;- USArrests UA.C[&quot;Maryland&quot;, &quot;UrbanPop&quot;] &lt;- 76.6 ## also +/- 0.5 to restore the original &lt;n&gt;.5 percentages s5u &lt;- c(&quot;Colorado&quot;, &quot;Florida&quot;, &quot;Mississippi&quot;, &quot;Wyoming&quot;) s5d &lt;- c(&quot;Nebraska&quot;, &quot;Pennsylvania&quot;) UA.C[s5u, &quot;UrbanPop&quot;] &lt;- UA.C[s5u, &quot;UrbanPop&quot;] + 0.5 UA.C[s5d, &quot;UrbanPop&quot;] &lt;- UA.C[s5d, &quot;UrbanPop&quot;] - 0.5 3.5.3 Agrupamento df &lt;- scale(USArrests) summary(df) ## Murder Assault UrbanPop Rape ## Min. :-1.6044 Min. :-1.5090 Min. :-2.31714 Min. :-1.4874 ## 1st Qu.:-0.8525 1st Qu.:-0.7411 1st Qu.:-0.76271 1st Qu.:-0.6574 ## Median :-0.1235 Median :-0.1411 Median : 0.03178 Median :-0.1209 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 ## 3rd Qu.: 0.7949 3rd Qu.: 0.9388 3rd Qu.: 0.84354 3rd Qu.: 0.5277 ## Max. : 2.2069 Max. : 1.9948 Max. : 1.75892 Max. : 2.6444 d &lt;- dist(df, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) groups &lt;- cutree(grup, k=3) plot(grup, cex = 0.6) rect.hclust(grup , k = 3, border = 2:6) abline(h = 3, col = &#39;red&#39;) kable(sort(groups)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) x Alabama 1 Arizona 1 California 1 Colorado 1 Florida 1 Georgia 1 Illinois 1 Louisiana 1 Maryland 1 Michigan 1 Mississippi 1 Nevada 1 New Mexico 1 New York 1 North Carolina 1 South Carolina 1 Tennessee 1 Texas 1 Alaska 2 Arkansas 2 Connecticut 2 Delaware 2 Hawaii 2 Indiana 2 Kansas 2 Kentucky 2 Massachusetts 2 Missouri 2 New Jersey 2 Ohio 2 Oklahoma 2 Oregon 2 Pennsylvania 2 Rhode Island 2 Utah 2 Virginia 2 Washington 2 Wyoming 2 Idaho 3 Iowa 3 Maine 3 Minnesota 3 Montana 3 Nebraska 3 New Hampshire 3 North Dakota 3 South Dakota 3 Vermont 3 West Virginia 3 Wisconsin 3 Pelo dendograma podemos infererir uma divisão em 3 clusters: km1 = kmeans(df, 3, nstart = 25) km1 ## K-means clustering with 3 clusters of sizes 20, 17, 13 ## ## Cluster means: ## Murder Assault UrbanPop Rape ## 1 1.0049340 1.0138274 0.1975853 0.8469650 ## 2 -0.4469795 -0.3465138 0.4788049 -0.2571398 ## 3 -0.9615407 -1.1066010 -0.9301069 -0.9667633 ## ## Clustering vector: ## Alabama Alaska Arizona Arkansas California ## 1 1 1 2 1 ## Colorado Connecticut Delaware Florida Georgia ## 1 2 2 1 1 ## Hawaii Idaho Illinois Indiana Iowa ## 2 3 1 2 3 ## Kansas Kentucky Louisiana Maine Maryland ## 2 3 1 3 1 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 2 1 3 1 1 ## Montana Nebraska Nevada New Hampshire New Jersey ## 3 3 1 3 2 ## New Mexico New York North Carolina North Dakota Ohio ## 1 1 1 3 2 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 2 2 2 2 1 ## South Dakota Tennessee Texas Utah Vermont ## 3 1 1 2 3 ## Virginia Washington West Virginia Wisconsin Wyoming ## 2 2 3 3 2 ## ## Within cluster sum of squares by cluster: ## [1] 46.74796 19.62285 11.95246 ## (between_SS / total_SS = 60.0 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; aggregate(df, by=list(cluster=km1$cluster), mean) ## cluster Murder Assault UrbanPop Rape ## 1 1 1.0049340 1.0138274 0.1975853 0.8469650 ## 2 2 -0.4469795 -0.3465138 0.4788049 -0.2571398 ## 3 3 -0.9615407 -1.1066010 -0.9301069 -0.9667633 p1 = fviz_cluster(km1, data=df, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw(), main = &quot;Particionamento&quot;) p1 "],["final-work.html", "Cap. 4 Trabalho final 4.1 Introdução 4.2 Objetivo 4.3 Problema 4.4 Metodologia 4.5 Índice Bovespa 4.6 Indicador Selic 4.7 Pesquisa Focus 4.8 Reuniões COPOM 4.9 Ibovespa e Selic 4.10 Unindo os conjuntos de dados 4.11 Preço Ibovespa ajustado antes e depois da reunião 4.12 Salvando o conjunto de dados 4.13 Correlação Ibovespa e Selic 4.14 Conclusão 4.15 Referências 4.16 Apêndice A - Código-fonte", " Cap. 4 Trabalho final Influência da taxa Selic no índice Bovespa 4.1 Introdução Após a regulamentação do Anexo IV e da implementação do Plano Real, o mercado acionário brasileiro teve um grande salto no seu desenvolvimento, tanto em termos de volume dos negócios quanto na eficiência alocativa… A grande conquista do Plano Real foi o controle da inflação. Taxas de inflação em níveis aceitáveis foram e estão sendo mantidas sob controle, devido às constantes intervenções governamentais, aumentando ou reduzindo as taxas de juros, tanto as de curto quanto as de longo prazo, controlando a oferta monetária através dos depósitos compulsórios sobre depósitos à vista, bem como controlando a taxa de câmbio, instrumento utilizado pelo BACEN por um longo período. Há, portanto, por parte de todos os agentes, a necessidade de se conhecer como o mercado acionário responde às mudanças dessas variáveis macroeconômicas. (DE SOUZA GRÔPPO, 2005) Para os participantes do mercado financeiro, há um interesse dada a influência exercida pela política monetária sobre os mercados, conhecer tais estimativas representaria uma relevante informação para a construção de posições e estratégias, tanto de investimento, bem como para a gestão dos riscos implícitos às operações normalmente desenvolvidas em seu dia a dia. (GONÇALVES JUNIOR, 2007) De acordo com o conhecimento convencional, mudanças nas taxas de juros afetariam o custo de capital e as expectativas futuras quanto à lucratividade das empresas, impactando, portanto, sua geração de dividendos aos acionistas, especialmente se considerada a valores presentes; em última análise, isto repercute no valor de mercado atual da empresa, e portanto de suas ações em Bolsa. (GONÇALVES JUNIOR, 2007) Porém, a atividade econômica também exerce efeito importante sobre a determinação dos preços na Bolsa de Valores, particularmente no Ibovespa. Os preços das ações são positivamente relacionados com a atividade econômica, medida pelo nível de produção industrial. A política monetária possui efeito significativo sobre a produção industrial o que indicaria a existência do segundo canal pelo qual a política monetária afeta os preços das ações. (DOS SANTOS et al., 2006) 4.2 Objetivo O objetivo é mensurar e analisar as respostas do mercado acionário às decisões do Comitê de Política Monetária do Banco Central do Brasil (COPOM) em relação à taxa básica de juro, verificando o comportamento do índice agregado da Bolsa de Valores de São Paulo (Bovespa). 4.3 Problema Quantificar ou prever a influência da taxa Selic no índice Bovespa demanda modelos de predições dinâmicas. A modelagem prevê predizer o índice Bovespa (série temporal em base diária) em função das covariáveis (taxa Selic anunciada mês a mês). Pressupõe-se que cortes não previstos na taxa de juros Selic proporcionem uma valorização das ações, já que abriria espaço para expansão da economia, com aumentos dos lucros e dos dividendos distribuídos pelas empresas. Mercados eficientes antecipam as decisões do COPOM, o que não deveria sensibilizar o índice Bovespa, uma vez que que as mudanças já foram incorporadas ao valor das ações, no entanto, as surpresas, a diferença da expectativa e da decisão, deveriam causar uma resposta do índice. O estudo define uma medida que procura captar a surpresa gerada nos mercados. As medidas foram baseadas em dados do mercado de futuros DI 1 dia, pela sua similaridade à taxa Selic. (GONÇALVES JUNIOR, 2007) 4.4 Metodologia A meta para a taxa básica de juro, inicialmente referida como TBC, teve sua sistemática alterada em agosto de 1998, quando o Banco Central passou a fazer uso também da TBAN (taxa de assistência para redesconto bancário), ofertando e tomando recursos na banda assim definida, conforme necessidades, importância, garantia e riscos dos agentes envolvidos, aplicando-se também um sistema de quotas instituído na ocasião. Em 05/03/1999, a TBC foi extinta, tendo sido substituída pela Selic para fins de política monetária, passando-se a empregar esta última como a taxa básica de juro e principal instrumento das operações de mercado aberto. (GONÇALVES JUNIOR, 2007) O índice Bovespa (ou simplesmente Ibovespa) foi criado há mais de cinquenta anos, em 2 de janeiro de 1968. Antes do Ibovespa, os preços das ações negociadas na Bolsa de Valores de São Paulo (SPSE) eram divulgados por meio de boletim diário (Boletim Diário de Informação – BDI). (CASTRO et al., 2019) De modo geral, os mercados financeiros são forward looking, o que equivale a dizer que eles estão constantemente incorporando novas informações a seus preços, seja devido a fatos que impactam imediatamente suas operações, seja por razões que modificam expectativas sobre o futuro; é a constatação de que estes agentes são racionais, utilizando toda a informação disponível para realizar suas previsões, tomando valores passados e correntes de variáveis exógenas ou não (LUCAS et al., 1996). Cada decisão tomada pelo COPOM é estudada por esses agentes, que antecipam em alguma medida seus resultados; assim sendo, quaisquer mudanças observadas nas variáveis correntes e futuras através do tempo decorrerão exclusivamente de mudanças não antecipadas. (CASTRO et al., 2019) Consequentemente, analisar apenas a relação direta entre as variações na meta da taxa de juros e as do índice Bovespa pode mostrar baixa significância; de fato, se os mercados inevitavelmente não gerarão respostas a mudanças já esperadas, então é preciso distinguir na variação da meta suas componentes antecipadas e não antecipadas (CASTRO et al., 2019). Para tal, foi avaliado a evolução do índice Ibovespa, taxa Selic e o efeito da decisão do COPOM no índice Bovespa num período de dez dias antes e depois da decisão e qual foi o retorno nos respectivos períodos. Em seguida, foi calculado o coeficiente de correlação linear de Pearson entre o índice Bovespa e a taxa Selic para toda a série e para quatro períodos distintos: de 2004 a 2007: período de início da redução de juros até a crise dos Subprime, de 2008 a 2010: do auge da crise financeira mundial no Brasil até a fase de recuperação do mercado; de 2011 a 2012: período que inclui tendência de redução da taxa básica de juros e a crise financeira, iniciada na Europa em 2011, também conhecida como crise da zona do euro. de 2013 a 2016: período da crise econômica de 2014, conhecida como a “grande recessão brasileira”, foi uma profunda e duradoura crise econômica, sendo caracterizada por recessão por dois anos consecutivos (2015 e 2016) e por sua longa e lenta recuperação, a mais lenta da história do país. (WIKIPEDIA, 2022) de 2017 a 2022: período que incluem os efeitos da COVID-19, iniciada em 2020. Os dados utilizados neste trabalho foram obtidos de banco de dados, índices e relatórios contidos em sites da Bovespa, Banco Central do Brasil e Yahoo Finance, além de informações coletadas da literatura indicada. A análise de correlação utilizou o coeficiente de Pearson que varia de \\(-1\\) a \\(1\\), sendo que o sinal indica a direção, positiva ou negativa da relação e o valor sugere a força do relacionamento entre as variáveis. A condução do trabalhe seguiu os seguintes passos: Roteiro Fonte: Adaptado pelo autor de Yoshida (2022) 4.4.1 Estimando a Surpresa dos Mercados Para estimar a surpresa dos mercados, foi utilizado o conjunto de dados da pesquisa Focus que diariamente registra a expectativa do mercado em relação a alguns indicadores macroeconomicos, entre eles a Selic. Foi considerado que: Sendo: \\(s_t\\): surpresa na reunião \\(d_t\\): decisão do copom sobre o valor da taxa Selic \\(m_t\\): valor máximo informado pelos respondentes como expectativa para o valor da taxa Selic Temos que: \\(s_t = d_t - m_t\\) Sempre que \\(s_t\\) é negativo, o mercado esperava um valor menor do que o decidido pelo COPOM e vice-versa. Pacotes utilizados neste trabalho: library(quantmod) library(xts) library(sidrar) library(timetk) library(tidyverse) library(jsonlite) library(ggplot2) library(scales) library(BatchGetSymbols) library(dplyr) library(rb3) library(pastecs) library(ggpubr) library(bizdays) library(gridExtra) library(GGally) library(readxl) library(ggformula) library(kableExtra) #conflicts(detail = TRUE) 4.5 Índice Bovespa Dados obtidos pela biblioteca BatchGetSymbols que utiliza o Yahoo Finance como fonte de dados. first_date = as.Date(&#39;2001-12-19&#39;) # data reunião 66 do COPOM last_date = as.Date(&#39;2022-11-25&#39;) eval_period = last_date - first_date df_bvsp = BatchGetSymbols(&#39;^BVSP&#39;, first.date = first_date, last.date = last_date) O período de análise será de 20 anos, de 2001-12-19 até 2022-11-25. O inicio da série coincide com a 66º reunião do COPOM. O período anterior a 2002 não foi considerado pela situação de hiperinflação vivenciada antes do plano real e o período de consolidação (1994-2001). O comportamento do índice Bovespa neste período foi: bov_plot &lt;- ggplot(df_bvsp$df.tickers, aes(x = ref.date, y = price.adjusted)) + geom_line() + scale_y_discrete(limits=c(10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000)) + scale_x_date(breaks = date_breaks(&quot;1 years&quot;), labels = date_format(&quot;%Y&quot;)) + xlab(&#39;&#39;)+ylab(&#39;Pontos&#39;) + labs(title=&#39;Índice Bovespa&#39;, caption=&#39;Fonte: Yahoo Finance.&#39;) + geom_line() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + scale_y_continuous(labels = label_comma()) + theme(axis.text.x = element_text(angle = 90) ) bov_plot 4.6 Indicador Selic A meta da taxa Selic diária. O conjunto de dados foi obtido pela através da opção de download no gráfico em BCB - Meta Selic. df_selic_meta &lt;- read.csv2(file = &quot;../dados/Taxa de juros - Selic - fixada pelo Comitê de Política Monetária.csv&quot;, sep=&quot;;&quot;, dec=&quot;,&quot;, colClasses = c(&quot;character&quot;, &quot;numeric&quot;, &quot;NULL&quot;), col.names = c(&quot;ref.date&quot;, &quot;selic.tax&quot;, &quot;&quot;)) df_selic_meta &lt;- transform(df_selic_meta, ref.date = as.Date(ref.date,&#39;%d/%m/%Y&#39;)) df_selic_meta &lt;- na.omit(df_selic_meta) %&gt;% filter(ref.date &gt;= first_date) kable(tail(df_selic_meta)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ref.date selic.tax 7585 2022-12-06 13.75 7586 2022-12-07 13.75 7587 2022-12-08 13.75 7588 2022-12-09 13.75 7589 2022-12-10 13.75 7590 2022-12-11 13.75 Extraído em 11/12/2022. E o comportamento no período foi: selic_plot &lt;- ggplot(df_selic_meta, aes(x = ref.date, y = selic.tax)) + geom_line() + scale_x_date(breaks = date_breaks(&quot;1 years&quot;), labels = date_format(&quot;%Y&quot;)) + xlab(&#39;&#39;)+ylab(&#39;%&#39;) + labs(title=&#39;Meta taxa selic&#39;, caption=&#39;Fonte: Banco Central do Brasil.&#39;) + geom_line() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + scale_y_continuous(labels = label_comma()) + theme(axis.text.x = element_text(angle = 90)) selic_plot 4.7 Pesquisa Focus O Relatório Focus resume as estatísticas calculadas considerando as expectativas de mercado coletadas até a sexta-feira anterior à sua divulgação. Ele é divulgado toda segunda-feira. O relatório traz a evolução gráfica e o comportamento semanal das projeções para índices de preços, atividade econômica, câmbio, taxa Selic, entre outros indicadores. As projeções são do mercado, não do Banco Central. (FOCUS, 2022). Esse conjunto de dados é o resultado da pesquisa diária das expectativas para a taxa Selic. Os respondentes são cerca de 120 bancos, gestores de recursos e demais instituições (empresas do setor real, distribuidoras, corretoras, consultorias e outras). As expectativas são expressadas na forma de mínimos e máximos esperados. df_selic_focus = rbcb::get_market_expectations(&quot;annual&quot;, &quot;Selic&quot;,start_date = &quot;2002-01-01&quot;, end_date = &quot;2022-12-31&quot;) df_selic_focus &lt;- df_selic_focus %&gt;% dplyr::select(Data, DataReferencia, Minimo, Maximo, numeroRespondentes) %&gt;% filter(format(Data, &quot;%Y&quot;) == DataReferencia) names(df_selic_focus)[1] = &quot;ref.date&quot; names(df_selic_focus)[2] = &quot;ref.year&quot; names(df_selic_focus)[3] = &quot;min&quot; names(df_selic_focus)[4] = &quot;max&quot; names(df_selic_focus)[5] = &quot;respondents&quot; kable(tail(df_selic_focus)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ref.date ref.year min max respondents 2022-12-05 2022 13.75 14.00 129 2022-12-05 2022 13.75 13.75 77 2022-12-06 2022 13.75 14.00 131 2022-12-06 2022 13.75 13.75 75 2022-12-07 2022 13.75 14.00 131 2022-12-07 2022 13.75 13.75 74 4.8 Reuniões COPOM Este conjunto de dados foi preparado pelo autor utilizando a fonte de dados Taxas de juros básicas – Histórico do Banco Central que contém todas as datas das reuniões do COPOM, bem como a meta decidida. df_reuniao_copom &lt;- read_excel(&quot;../dados/reunioes-copom-2022.xlsx&quot;) df_reuniao_copom &lt;- df_reuniao_copom %&gt;% dplyr::select(reuniao_n, extraordinaria, reuniao_data, nota, vigencia_inicio, vigencia_fim, meta_selic_ano) df_reuniao_copom$reuniao_data = as.Date(df_reuniao_copom$reuniao_data) df_reuniao_copom$vigencia_inicio = as.Date(df_reuniao_copom$vigencia_inicio) df_reuniao_copom$vigencia_fim = as.Date(df_reuniao_copom$vigencia_fim) names(df_reuniao_copom)[1] = &quot;meeting.number&quot; names(df_reuniao_copom)[2] = &quot;extra&quot; names(df_reuniao_copom)[3] = &quot;ref.date&quot; names(df_reuniao_copom)[4] = &quot;note&quot; names(df_reuniao_copom)[5] = &quot;term.start&quot; names(df_reuniao_copom)[6] = &quot;term.end&quot; names(df_reuniao_copom)[7] = &quot;decision.target&quot; kable(tail(df_reuniao_copom)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) meeting.number extra ref.date note term.start term.end decision.target 246 0 2022-05-04 NA 2022-05-05 2022-06-16 12.75 247 0 2022-06-15 NA 2022-06-17 2022-08-03 13.25 248 0 2022-08-03 NA 2022-08-04 2022-09-21 13.75 249 0 2022-09-21 NA 2022-09-22 2022-10-26 13.75 250 0 2022-10-26 NA 2022-10-27 2022-12-07 13.75 251 0 2022-12-07 NA 2022-12-08 2023-01-31 13.75 4.9 Ibovespa e Selic Apresentando gráficamente as duas séries: grid.arrange(bov_plot, selic_plot, ncol=2) Elas seguem tendências opostas, indicando uma correlação negativa, o que aparentemente sustenta a hipotese de correlação entre as séries. 4.10 Unindo os conjuntos de dados Os conjuntos de dados do índice Ibovespa será aumentado com os conjuntos de dados da meta Selic e com as reuniões do COPOM. # Usando apenas valor ajustado df_bvsp_close &lt;- df_bvsp$df.tickers %&gt;% dplyr::select(ref.date, price.adjusted) df_bvsp_selic &lt;- merge(df_bvsp_close, df_selic_meta, by = &#39;ref.date&#39;, all.x = T) df_bvsp_selic &lt;- merge(df_bvsp_selic, df_selic_focus, by = &#39;ref.date&#39;, all.x = T) df_bvsp_selic &lt;- df_bvsp_selic %&gt;% fill(c(selic.tax, ref.year, `min`, `max`), .direction = &quot;downup&quot;) df_bvsp_selic$diff.meta.max = with(df_bvsp_selic, max - selic.tax) df_bvsp_selic &lt;- merge(df_bvsp_selic, df_reuniao_copom, by = &#39;ref.date&#39;, all.x = T) df_bvsp_selic &lt;- df_bvsp_selic %&gt;% fill(c(decision.target, meeting.number, term.start, term.end), .direction = &quot;downup&quot;) kable(round(stat.desc(df_bvsp_selic))) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ref.date price.adjusted selic.tax ref.year min max respondents diff.meta.max meeting.number extra note term.start term.end decision.target nbr.val 5578 5578 5578 NA 5578 5578 2409 5578 5578 198 NA 5578 5578 5578 nbr.null 0 0 0 NA 0 0 0 791 0 197 NA 0 0 0 nbr.na 0 0 0 NA 0 0 3169 0 0 5380 NA 0 0 0 min 11675 8371 2 NA 1 2 22 -4 66 0 NA 11676 11710 2 max 19320 130776 26 NA 22 27 137 7 250 1 NA 19292 19333 26 range 7645 122405 24 NA 21 25 115 11 184 1 NA 7616 7623 24 sum 87846608 351263219 64932 NA 57505 70250 239976 5318 950951 1 NA 87734996 87971230 64945 median 15796 58406 11 NA 10 13 104 0 173 0 NA 15771 15812 11 mean 15749 62973 12 NA 10 13 100 1 170 0 NA 15729 15771 12 SE.mean 31 422 0 NA 0 0 0 0 1 0 NA 31 31 0 CI.mean.0.95 61 827 0 NA 0 0 1 0 1 0 NA 61 61 0 var 5357435 992505261 26 NA 16 22 437 2 2813 0 NA 5347621 5366735 26 std.dev 2315 31504 5 NA 4 5 21 1 53 0 NA 2312 2317 5 coef.var 0 1 0 NA 0 0 0 1 0 14 NA 0 0 0 gf_point(price.adjusted ~ selic.tax, color = ~ selic.tax, size = ~ price.adjusted, alpha = 0.50, data = df_bvsp_selic) + scale_y_continuous(labels = label_comma()) O preço Ibovespa ajustado, explicado pela taxa Selic, parece seguir a tendência observada nos gráficos anteriores, quanto menor a taxa, maior é o preço ajustado. Ainda de acordo com o gráfico acima, o log retorno do preço ajustado das ações não parece ser influenciado pela diferença da meta vs o máximo esperado. Pela hipótese de que a surpresa faria o índice Bovespa recuar, quanto menor a diferença, menor deveria ser o retorno, o que não foi observado. As ocorrências de “surpresa”, são listadas abaixo: df_not_expected &lt;- df_bvsp_selic[order(-df_bvsp_selic$meeting.number), ] %&gt;% filter(!is.na(meeting.number) &amp; diff.meta.max &lt; 0) %&gt;% select(meeting.number, ref.date, diff.meta.max, price.adjusted) %&gt;% group_by(meeting.number) %&gt;% slice(which.min(ref.date)) kable(df_not_expected) %&gt;% kable_styling(latex_options = &quot;striped&quot;) meeting.number ref.date diff.meta.max price.adjusted 66 2002-01-21 -0.70 13155 67 2002-01-23 -0.70 13232 68 2002-02-20 -1.00 13303 69 2002-03-20 -0.75 14090 70 2002-04-17 -0.50 13732 71 2002-05-22 -0.50 12368 72 2002-06-19 -0.24 11493 73 2002-07-17 -0.25 10755 79 2003-01-14 -0.50 12175 80 2003-01-23 -0.50 11162 81 2003-02-21 -0.50 10331 82 2003-04-04 -1.00 12066 83 2003-04-24 -1.50 12120 84 2003-05-21 -1.00 13034 85 2003-06-18 -1.00 13511 86 2003-07-23 -4.00 13799 87 2003-08-20 -3.50 14467 88 2003-09-17 -1.50 16492 89 2003-10-22 -1.50 18235 90 2003-11-19 -0.50 18807 91 2004-01-02 -1.00 22445 92 2004-01-21 -1.50 23302 93 2004-02-18 -1.25 22000 94 2004-03-17 -1.50 21901 95 2004-04-14 -1.25 22312 96 2004-05-19 -0.25 18688 103 2005-01-03 -0.25 25722 104 2005-01-20 -0.25 23610 105 2005-02-17 -0.50 27091 108 2005-06-08 -0.25 24702 109 2005-06-15 -0.25 25481 110 2005-07-20 -0.75 25705 111 2005-08-17 -0.75 27416 112 2005-09-14 -0.75 29050 113 2005-10-19 -0.75 29297 114 2005-11-23 -0.75 31943 115 2005-12-14 -0.25 33629 116 2006-01-18 -0.50 35805 117 2006-03-08 -0.25 37289 119 2006-06-20 -0.25 33632 120 2006-07-19 -0.25 36785 121 2006-08-30 -0.25 36313 122 2006-10-17 -0.39 38898 126 2007-05-18 -0.50 52078 127 2007-06-06 -0.50 52049 128 2007-08-13 -0.50 52434 140 2009-02-27 -1.25 38183 141 2009-03-31 -0.25 40926 161 2011-10-13 -0.50 54601 162 2011-10-19 -0.50 54966 163 2011-11-30 -0.25 56875 165 2012-04-11 -0.25 61293 166 2012-04-18 -0.25 63010 168 2012-08-27 -0.25 58111 169 2012-08-29 -0.25 57369 202 2016-11-18 -0.25 59962 203 2017-01-02 -2.00 59589 204 2017-01-11 -2.00 62446 205 2017-02-22 -2.25 68590 206 2017-04-12 -2.25 63892 207 2017-05-31 -1.50 62711 208 2017-07-26 -1.00 65011 209 2017-09-06 -0.50 73412 210 2017-11-03 -0.25 73915 211 2017-12-06 -0.50 73268 224 2019-09-02 -0.50 100626 225 2019-09-18 -0.50 104532 230 2020-06-03 -0.25 93002 231 2020-06-17 -0.50 95547 log.retorno = diff(log(df_bvsp_selic$price.adjusted)) log.retorno[length(log.retorno) + 1] = 0 gf_point(log.retorno ~ diff.meta.max, color = ~ diff.meta.max, alpha = 0.50, data = df_bvsp_selic) + scale_y_continuous(labels = label_comma()) A diferença entre o esperado e a decisão tendem a variar de forma positiva, sugerindo que o mercado antecipa positivamente as decisões com razoável precisão. temp &lt;- df_bvsp_selic[order(-df_bvsp_selic$meeting.number), ] %&gt;% filter(!is.na(meeting.number)) %&gt;% select(ref.date, diff.meta.max) ggplot(temp, aes(x = ref.date, y = diff.meta.max))+ geom_line()+ labs(title=&#39;Variação expectativa meta vs focus&#39;, caption=&#39;Fonte: autor.&#39;)+ geom_line() + geom_smooth(method = &quot;loess&quot;, se = FALSE)+ theme(axis.text.x = element_text(angle = 90))+ geom_line(aes(y = 0), color = &quot;red&quot;, linetype = &quot;dotted&quot;) Reuniões cuja a meta foi superior ao máximo esparado pelos respondentes da pesquisa focus. kable(temp %&gt;% filter(diff.meta.max &lt; 0)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ref.date diff.meta.max 2020-06-17 -0.50 2020-06-03 -0.25 2020-06-04 -0.25 2020-06-05 -0.25 2020-06-08 -0.25 2020-06-09 -0.25 2020-06-10 -0.25 2020-06-12 -0.50 2020-06-15 -0.50 2020-06-16 -0.50 2019-09-18 -0.50 2019-09-02 -0.50 2019-09-03 -0.50 2019-09-04 -0.50 2019-09-05 -0.50 2019-09-06 -0.50 2019-09-09 -0.50 2019-09-10 -0.50 2019-09-11 -0.50 2019-09-12 -0.50 2019-09-13 -0.50 2019-09-16 -0.50 2019-09-17 -0.50 2017-12-06 -0.50 2017-11-03 -0.25 2017-11-06 -0.25 2017-11-07 -0.25 2017-11-08 -0.25 2017-11-09 -0.25 2017-11-10 -0.25 2017-11-13 -0.25 2017-11-14 -0.25 2017-11-16 -0.25 2017-11-17 -0.25 2017-11-21 -0.25 2017-11-22 -0.25 2017-11-23 -0.50 2017-11-24 -0.50 2017-11-27 -0.50 2017-11-28 -0.50 2017-11-29 -0.50 2017-11-30 -0.50 2017-12-01 -0.50 2017-12-04 -0.50 2017-12-05 -0.50 2017-09-06 -0.50 2017-07-26 -1.00 2017-07-31 -0.50 2017-08-01 -0.50 2017-08-02 -0.50 2017-08-03 -0.50 2017-08-04 -0.50 2017-08-07 -0.75 2017-08-08 -0.75 2017-08-09 -0.75 2017-08-10 -0.75 2017-08-11 -0.75 2017-08-14 -0.75 2017-08-15 -0.75 2017-08-16 -0.75 2017-08-17 -0.75 2017-08-18 -0.75 2017-08-21 -0.50 2017-08-22 -0.50 2017-08-23 -0.50 2017-08-24 -0.50 2017-08-25 -0.50 2017-08-28 -0.50 2017-08-29 -0.50 2017-08-30 -0.50 2017-08-31 -0.50 2017-09-01 -0.50 2017-09-04 -0.50 2017-09-05 -0.50 2017-05-31 -1.50 2017-06-01 -0.50 2017-06-02 -0.50 2017-06-05 -0.50 2017-06-06 -0.50 2017-06-07 -0.50 2017-06-08 -0.50 2017-06-09 -0.50 2017-06-12 -0.50 2017-06-13 -0.50 2017-06-14 -0.50 2017-06-16 -0.50 2017-06-19 -0.50 2017-06-20 -0.50 2017-06-21 -0.50 2017-06-22 -0.50 2017-06-23 -0.50 2017-06-26 -0.50 2017-06-27 -0.50 2017-06-28 -0.50 2017-06-29 -0.50 2017-06-30 -0.50 2017-07-03 -0.75 2017-07-04 -0.75 2017-07-05 -0.75 2017-07-06 -0.75 2017-07-07 -0.75 2017-07-10 -0.75 2017-07-11 -0.75 2017-07-12 -0.75 2017-07-13 -0.75 2017-07-14 -0.75 2017-07-17 -0.75 2017-07-18 -0.75 2017-07-19 -0.75 2017-07-20 -0.75 2017-07-21 -1.00 2017-07-24 -1.00 2017-07-25 -1.00 2017-04-12 -2.25 2017-04-13 -1.25 2017-04-17 -1.25 2017-04-18 -1.25 2017-04-19 -1.50 2017-04-20 -1.50 2017-04-24 -1.50 2017-04-25 -1.50 2017-04-26 -1.50 2017-04-27 -1.50 2017-04-28 -1.50 2017-05-02 -1.50 2017-05-03 -1.50 2017-05-04 -1.75 2017-05-05 -1.75 2017-05-08 -1.75 2017-05-09 -1.75 2017-05-10 -1.75 2017-05-11 -1.75 2017-05-12 -1.75 2017-05-15 -1.75 2017-05-16 -1.75 2017-05-17 -1.75 2017-05-23 -0.75 2017-05-24 -0.75 2017-05-25 -0.75 2017-05-26 -1.50 2017-05-29 -1.50 2017-05-30 -1.50 2017-02-22 -2.25 2017-02-23 -1.50 2017-02-24 -1.50 2017-03-01 -1.50 2017-03-02 -1.50 2017-03-03 -1.50 2017-03-06 -1.50 2017-03-07 -1.50 2017-03-08 -1.50 2017-03-09 -1.50 2017-03-10 -1.50 2017-03-13 -1.50 2017-03-14 -1.50 2017-03-15 -1.50 2017-03-16 -1.50 2017-03-17 -1.50 2017-03-20 -1.50 2017-03-21 -1.50 2017-03-22 -1.50 2017-03-23 -2.00 2017-03-24 -2.00 2017-03-27 -2.00 2017-03-28 -2.00 2017-03-29 -2.00 2017-03-30 -2.00 2017-03-31 -2.00 2017-04-03 -2.25 2017-04-04 -2.25 2017-04-05 -2.25 2017-04-06 -2.25 2017-04-07 -2.25 2017-04-10 -2.25 2017-04-11 -2.25 2017-01-11 -2.00 2017-01-12 -1.50 2017-01-13 -1.50 2017-01-16 -2.00 2017-01-17 -2.00 2017-01-18 -2.00 2017-01-19 -2.00 2017-01-20 -2.00 2017-01-23 -2.00 2017-01-24 -2.00 2017-01-26 -2.00 2017-01-27 -2.00 2017-01-30 -2.00 2017-01-31 -2.00 2017-02-01 -2.00 2017-02-02 -2.00 2017-02-03 -2.00 2017-02-06 -2.25 2017-02-07 -2.25 2017-02-08 -2.25 2017-02-09 -2.25 2017-02-10 -2.25 2017-02-13 -2.41 2017-02-14 -2.41 2017-02-15 -2.41 2017-02-16 -2.41 2017-02-17 -2.41 2017-02-20 -2.25 2017-02-21 -2.25 2017-01-02 -2.00 2017-01-03 -2.00 2017-01-04 -2.00 2017-01-05 -2.00 2017-01-06 -2.00 2017-01-09 -2.00 2017-01-10 -2.00 2016-11-18 -0.25 2016-11-21 -0.25 2012-08-29 -0.25 2012-08-27 -0.25 2012-08-28 -0.25 2012-04-18 -0.25 2012-04-11 -0.25 2012-04-12 -0.25 2012-04-13 -0.25 2012-04-16 -0.25 2012-04-17 -0.25 2011-11-30 -0.25 2011-10-19 -0.50 2011-10-24 -0.25 2011-10-25 -0.25 2011-11-18 -0.25 2011-11-21 -0.25 2011-11-22 -0.25 2011-11-23 -0.25 2011-11-24 -0.25 2011-11-25 -0.25 2011-11-28 -0.25 2011-11-29 -0.25 2011-10-13 -0.50 2011-10-14 -0.50 2011-10-17 -0.50 2011-10-18 -0.50 2009-03-31 -0.25 2009-04-01 -0.25 2009-04-02 -0.25 2009-04-03 -0.25 2009-04-06 -0.25 2009-04-07 -0.25 2009-04-08 -0.25 2009-04-09 -0.25 2009-04-13 -0.35 2009-04-14 -0.35 2009-04-15 -0.35 2009-04-16 -0.35 2009-04-17 -0.35 2009-04-20 -0.35 2009-04-22 -0.75 2009-04-23 -0.75 2009-04-24 -0.75 2009-04-27 -0.75 2009-04-28 -0.75 2009-02-27 -1.25 2009-03-02 -1.25 2009-03-03 -1.25 2009-03-04 -1.25 2009-03-05 -1.25 2009-03-06 -1.25 2009-03-09 -1.25 2009-03-10 -1.25 2007-08-13 -0.50 2007-08-14 -0.50 2007-08-15 -0.50 2007-08-16 -0.50 2007-08-17 -0.50 2007-08-20 -0.25 2007-08-21 -0.25 2007-08-22 -0.25 2007-08-23 -0.25 2007-08-24 -0.25 2007-08-27 -0.25 2007-08-28 -0.25 2007-06-06 -0.50 2007-05-18 -0.50 2007-05-21 -0.50 2007-05-22 -0.50 2007-05-23 -0.50 2007-05-24 -0.50 2007-05-25 -0.50 2007-05-28 -0.50 2007-05-29 -0.50 2007-05-30 -0.50 2007-05-31 -0.50 2007-06-01 -0.50 2007-06-04 -0.50 2007-06-05 -0.50 2006-10-17 -0.39 2006-10-18 -0.39 2006-08-30 -0.25 2006-10-06 -0.39 2006-10-09 -0.39 2006-10-10 -0.39 2006-10-11 -0.39 2006-10-13 -0.39 2006-10-16 -0.39 2006-07-19 -0.25 2006-07-27 -0.25 2006-07-28 -0.25 2006-07-31 -0.25 2006-08-01 -0.25 2006-08-02 -0.25 2006-08-18 -0.25 2006-08-21 -0.25 2006-08-22 -0.25 2006-08-23 -0.25 2006-08-24 -0.25 2006-08-25 -0.25 2006-08-28 -0.25 2006-08-29 -0.25 2006-06-20 -0.25 2006-06-21 -0.25 2006-06-22 -0.25 2006-06-23 -0.25 2006-06-26 -0.25 2006-06-27 -0.25 2006-06-28 -0.25 2006-06-29 -0.25 2006-06-30 -0.25 2006-07-03 -0.25 2006-07-04 -0.25 2006-07-05 -0.25 2006-07-06 -0.25 2006-07-07 -0.25 2006-07-10 -0.25 2006-07-11 -0.25 2006-07-12 -0.25 2006-07-13 -0.25 2006-07-14 -0.25 2006-07-17 -0.25 2006-07-18 -0.25 2006-03-08 -0.25 2006-03-10 -0.50 2006-03-13 -0.50 2006-03-14 -0.50 2006-03-15 -0.50 2006-03-16 -0.50 2006-03-17 -0.50 2006-03-20 -0.50 2006-03-21 -0.50 2006-03-22 -0.50 2006-03-23 -0.50 2006-03-24 -0.50 2006-03-27 -0.50 2006-03-28 -0.50 2006-03-29 -0.50 2006-03-30 -0.50 2006-03-31 -0.50 2006-04-03 -0.50 2006-04-04 -0.50 2006-04-05 -0.50 2006-04-06 -0.50 2006-04-07 -0.50 2006-04-10 -0.50 2006-04-11 -0.50 2006-04-12 -0.50 2006-04-13 -1.25 2006-01-18 -0.50 2006-02-17 -0.25 2006-02-20 -0.25 2006-02-21 -0.25 2006-02-22 -0.25 2006-02-23 -0.25 2006-02-24 -0.25 2006-03-01 -0.25 2006-03-02 -0.25 2006-03-03 -0.25 2006-03-06 -0.25 2006-03-07 -0.25 2005-12-14 -0.25 2006-01-02 -0.25 2006-01-03 -0.25 2006-01-04 -0.25 2006-01-05 -0.25 2006-01-06 -0.50 2006-01-09 -0.50 2006-01-10 -0.50 2006-01-11 -0.50 2006-01-12 -0.50 2006-01-13 -0.50 2006-01-16 -0.50 2006-01-17 -0.50 2005-11-23 -0.75 2005-11-24 -0.25 2005-11-25 -0.25 2005-11-28 -0.25 2005-11-29 -0.25 2005-11-30 -0.25 2005-12-01 -0.25 2005-12-02 -0.25 2005-12-05 -0.50 2005-12-06 -0.50 2005-12-07 -0.50 2005-12-08 -0.25 2005-12-09 -0.25 2005-12-12 -0.25 2005-12-13 -0.25 2005-10-19 -0.75 2005-10-20 -0.25 2005-10-21 -0.25 2005-10-24 -0.25 2005-10-25 -0.25 2005-10-26 -0.25 2005-10-27 -0.25 2005-10-28 -0.25 2005-10-31 -0.50 2005-11-01 -0.50 2005-11-03 -0.50 2005-11-04 -0.50 2005-11-07 -0.50 2005-11-08 -0.50 2005-11-09 -0.50 2005-11-10 -0.50 2005-11-11 -0.75 2005-11-14 -0.75 2005-11-16 -0.75 2005-11-17 -0.75 2005-11-18 -0.75 2005-11-21 -0.75 2005-11-22 -0.75 2005-09-14 -0.75 2005-09-15 -0.50 2005-09-16 -0.50 2005-09-19 -0.50 2005-09-20 -0.50 2005-09-21 -0.50 2005-09-22 -0.50 2005-09-23 -0.50 2005-09-26 -0.50 2005-09-27 -0.50 2005-09-28 -0.50 2005-09-29 -0.50 2005-09-30 -0.50 2005-10-03 -0.75 2005-10-04 -0.75 2005-10-05 -0.75 2005-10-06 -0.75 2005-10-07 -0.75 2005-10-10 -0.75 2005-10-11 -0.75 2005-10-13 -0.75 2005-10-14 -0.75 2005-10-17 -0.75 2005-10-18 -0.75 2005-08-17 -0.75 2005-08-18 -0.75 2005-08-19 -0.75 2005-08-22 -0.75 2005-08-23 -0.75 2005-08-24 -0.75 2005-08-25 -0.75 2005-08-26 -0.75 2005-08-29 -0.75 2005-08-30 -0.75 2005-08-31 -0.75 2005-09-01 -0.75 2005-09-02 -0.75 2005-09-05 -0.75 2005-09-06 -0.75 2005-09-08 -0.75 2005-09-09 -0.75 2005-09-12 -0.75 2005-09-13 -0.75 2005-07-20 -0.75 2005-07-21 -0.75 2005-07-22 -0.75 2005-07-25 -0.75 2005-07-26 -0.75 2005-07-27 -0.75 2005-07-28 -0.75 2005-07-29 -0.75 2005-08-01 -0.75 2005-08-02 -0.75 2005-08-03 -0.75 2005-08-04 -0.75 2005-08-05 -0.75 2005-08-08 -0.75 2005-08-09 -0.75 2005-08-10 -0.75 2005-08-11 -0.75 2005-08-12 -0.75 2005-08-15 -0.75 2005-08-16 -0.75 2005-06-15 -0.25 2005-06-16 -0.25 2005-06-17 -0.25 2005-06-20 -0.25 2005-06-21 -0.25 2005-06-22 -0.25 2005-06-23 -0.25 2005-06-24 -0.25 2005-06-27 -0.25 2005-06-28 -0.25 2005-06-29 -0.25 2005-06-30 -0.25 2005-07-01 -0.25 2005-07-04 -0.75 2005-07-05 -0.75 2005-07-06 -0.75 2005-07-07 -0.75 2005-07-08 -0.75 2005-07-11 -0.75 2005-07-12 -0.75 2005-07-13 -0.75 2005-07-14 -0.75 2005-07-15 -0.75 2005-07-18 -0.75 2005-07-19 -0.75 2005-06-08 -0.25 2005-06-09 -0.25 2005-06-10 -0.25 2005-06-13 -0.25 2005-06-14 -0.25 2005-02-17 -0.50 2005-02-18 -0.50 2005-02-21 -0.50 2005-02-22 -0.50 2005-02-23 -0.50 2005-02-24 -0.50 2005-01-20 -0.25 2005-01-21 -0.25 2005-01-24 -0.25 2005-01-26 -0.25 2005-01-03 -0.25 2005-01-04 -0.25 2005-01-05 -0.25 2005-01-06 -0.25 2005-01-07 -0.25 2005-01-10 -0.25 2005-01-11 -0.25 2005-01-12 -0.25 2005-01-13 -0.25 2004-05-19 -0.25 2004-05-20 -0.25 2004-04-14 -1.25 2004-04-15 -1.00 2004-04-16 -1.00 2004-04-19 -1.00 2004-04-20 -1.00 2004-04-22 -1.00 2004-04-23 -1.00 2004-04-26 -1.00 2004-04-27 -1.00 2004-04-28 -1.00 2004-04-29 -1.00 2004-04-30 -1.00 2004-05-03 -1.00 2004-05-04 -1.00 2004-05-05 -1.00 2004-05-06 -1.00 2004-05-07 -1.00 2004-05-10 -1.00 2004-05-11 -0.25 2004-05-12 -0.25 2004-05-13 -0.25 2004-05-14 -0.25 2004-05-17 -0.25 2004-05-18 -0.25 2004-03-17 -1.50 2004-03-18 -1.25 2004-03-19 -1.25 2004-03-22 -1.42 2004-03-23 -1.42 2004-03-24 -1.42 2004-03-25 -1.25 2004-03-26 -1.25 2004-03-29 -1.25 2004-03-30 -1.25 2004-03-31 -1.25 2004-04-01 -1.25 2004-04-02 -1.25 2004-04-05 -1.25 2004-04-06 -1.25 2004-04-07 -1.25 2004-04-08 -1.25 2004-04-12 -1.25 2004-04-13 -1.25 2004-02-18 -1.25 2004-02-19 -1.25 2004-02-20 -1.25 2004-02-25 -1.25 2004-02-26 -1.25 2004-02-27 -1.25 2004-03-01 -1.25 2004-03-02 -1.25 2004-03-03 -1.25 2004-03-04 -1.50 2004-03-05 -1.50 2004-03-08 -1.50 2004-03-09 -1.50 2004-03-10 -1.50 2004-03-11 -1.50 2004-03-12 -1.50 2004-03-15 -1.50 2004-03-16 -1.50 2004-01-21 -1.50 2004-01-22 -1.50 2004-01-23 -1.50 2004-01-26 -1.50 2004-01-27 -1.50 2004-01-28 -1.50 2004-01-29 -1.50 2004-01-30 -1.50 2004-02-02 -1.25 2004-02-03 -1.25 2004-02-04 -1.25 2004-02-05 -1.25 2004-02-06 -1.25 2004-02-09 -1.25 2004-02-10 -1.25 2004-02-11 -1.25 2004-02-12 -1.25 2004-02-13 -1.25 2004-02-16 -1.25 2004-02-17 -1.25 2004-01-02 -1.00 2004-01-05 -1.00 2004-01-06 -1.50 2004-01-07 -1.50 2004-01-08 -1.50 2004-01-09 -1.50 2004-01-12 -1.50 2004-01-13 -1.50 2004-01-14 -1.50 2004-01-15 -1.50 2004-01-16 -1.50 2004-01-19 -1.50 2004-01-20 -1.50 2003-11-19 -0.50 2003-12-15 -0.50 2003-10-22 -1.50 2003-10-23 -0.50 2003-10-24 -0.50 2003-10-27 -0.50 2003-10-28 -0.50 2003-10-29 -0.50 2003-10-30 -0.50 2003-10-31 -0.50 2003-11-03 -1.00 2003-11-04 -1.00 2003-11-05 -0.50 2003-11-06 -0.50 2003-11-07 -0.50 2003-11-10 -0.50 2003-11-11 -0.50 2003-11-12 -0.50 2003-11-13 -0.50 2003-11-14 -0.50 2003-11-17 -0.50 2003-11-18 -0.50 2003-09-17 -1.50 2003-09-22 -1.00 2003-09-23 -1.00 2003-09-24 -1.00 2003-09-25 -1.00 2003-09-26 -1.00 2003-09-29 -1.00 2003-09-30 -1.00 2003-10-01 -1.00 2003-10-02 -1.00 2003-10-03 -1.00 2003-10-06 -1.00 2003-10-07 -1.35 2003-10-08 -1.35 2003-10-09 -1.35 2003-10-10 -1.35 2003-10-13 -1.35 2003-10-14 -1.35 2003-10-15 -1.35 2003-10-16 -1.35 2003-10-17 -1.35 2003-10-20 -1.35 2003-10-21 -1.50 2003-08-20 -3.50 2003-08-21 -1.00 2003-08-22 -1.00 2003-08-25 -1.00 2003-08-26 -1.00 2003-08-27 -1.00 2003-08-28 -1.00 2003-08-29 -1.00 2003-09-01 -1.00 2003-09-02 -1.00 2003-09-03 -1.00 2003-09-04 -1.00 2003-09-05 -1.00 2003-09-08 -1.50 2003-09-09 -1.50 2003-09-10 -1.50 2003-09-11 -1.50 2003-09-12 -1.50 2003-09-15 -1.50 2003-09-16 -1.50 2003-07-23 -4.00 2003-07-24 -2.50 2003-07-25 -2.50 2003-07-28 -2.50 2003-07-29 -2.50 2003-07-30 -2.50 2003-07-31 -2.50 2003-08-01 -2.50 2003-08-04 -2.50 2003-08-05 -2.50 2003-08-06 -2.50 2003-08-07 -2.50 2003-08-08 -2.50 2003-08-11 -2.50 2003-08-12 -2.50 2003-08-13 -2.50 2003-08-14 -2.50 2003-08-15 -3.00 2003-08-18 -3.50 2003-08-19 -3.50 2003-06-18 -1.00 2003-06-20 -0.50 2003-06-23 -0.50 2003-06-24 -0.50 2003-06-25 -0.50 2003-06-26 -0.50 2003-06-27 -0.50 2003-06-30 -0.50 2003-07-01 -0.50 2003-07-02 -0.50 2003-07-03 -0.50 2003-07-04 -3.00 2003-07-07 -3.00 2003-07-08 -3.00 2003-07-10 -3.00 2003-07-11 -3.00 2003-07-14 -3.15 2003-07-15 -3.50 2003-07-16 -3.50 2003-07-17 -3.50 2003-07-18 -3.50 2003-07-21 -4.00 2003-07-22 -4.00 2003-05-21 -1.00 2003-05-22 -1.00 2003-05-23 -1.00 2003-05-26 -1.00 2003-05-27 -1.00 2003-05-28 -1.00 2003-05-29 -1.00 2003-05-30 -1.00 2003-06-02 -1.00 2003-06-03 -1.00 2003-06-04 -1.00 2003-06-05 -1.00 2003-06-06 -1.00 2003-06-09 -1.00 2003-06-10 -1.00 2003-06-11 -1.00 2003-06-12 -1.00 2003-06-13 -1.00 2003-06-16 -1.00 2003-06-17 -1.00 2003-04-24 -1.50 2003-04-25 -1.50 2003-04-28 -1.50 2003-04-29 -1.50 2003-04-30 -1.50 2003-05-02 -1.50 2003-05-05 -1.50 2003-05-06 -1.50 2003-05-07 -1.50 2003-05-08 -1.00 2003-05-09 -1.00 2003-05-12 -1.00 2003-05-13 -1.00 2003-05-14 -1.00 2003-05-15 -1.00 2003-05-16 -1.00 2003-05-19 -1.00 2003-05-20 -1.00 2003-04-04 -1.00 2003-04-07 -1.00 2003-04-08 -1.00 2003-04-09 -1.00 2003-04-10 -1.00 2003-04-11 -1.00 2003-02-21 -0.50 2003-02-24 -0.50 2003-02-25 -0.50 2003-02-26 -0.50 2003-02-27 -0.50 2003-02-28 -0.50 2003-03-05 -0.50 2003-03-06 -0.50 2003-01-23 -0.50 2003-01-24 -0.50 2003-01-27 -0.50 2003-01-14 -0.50 2002-07-17 -0.25 2002-06-19 -0.24 2002-06-20 -0.24 2002-06-21 -0.24 2002-06-24 -0.25 2002-06-25 -0.25 2002-06-26 -0.25 2002-06-27 -0.25 2002-06-28 -0.25 2002-07-01 -0.25 2002-07-02 -0.25 2002-07-03 -0.25 2002-07-04 -0.25 2002-07-05 -0.25 2002-07-08 -0.25 2002-07-10 -0.25 2002-07-11 -0.25 2002-07-12 -0.40 2002-07-15 -0.40 2002-07-16 -0.40 2002-05-22 -0.50 2002-05-23 -0.50 2002-05-24 -0.50 2002-05-27 -0.50 2002-05-28 -0.50 2002-05-29 -0.50 2002-05-31 -0.50 2002-06-03 -0.50 2002-06-04 -0.40 2002-06-11 -0.25 2002-06-12 -0.25 2002-06-13 -0.25 2002-06-14 -0.25 2002-06-17 -0.24 2002-06-18 -0.24 2002-04-17 -0.50 2002-04-18 -0.50 2002-04-19 -0.50 2002-04-22 -0.50 2002-04-23 -0.50 2002-04-24 -0.50 2002-04-25 -0.50 2002-04-26 -0.50 2002-04-29 -0.50 2002-04-30 -0.50 2002-05-02 -0.50 2002-05-03 -0.50 2002-05-06 -0.50 2002-05-07 -0.50 2002-05-08 -0.50 2002-05-09 -0.50 2002-05-10 -0.50 2002-05-13 -0.50 2002-05-14 -0.50 2002-05-15 -0.50 2002-05-16 -0.50 2002-05-17 -0.50 2002-05-20 -0.50 2002-05-21 -0.50 2002-03-20 -0.75 2002-03-21 -0.50 2002-03-22 -0.50 2002-03-25 -0.50 2002-03-26 -0.50 2002-03-27 -0.50 2002-03-28 -0.50 2002-04-01 -0.50 2002-04-02 -0.50 2002-04-03 -0.50 2002-04-04 -0.50 2002-04-05 -0.50 2002-04-08 -0.50 2002-04-09 -0.50 2002-04-10 -0.50 2002-04-11 -0.50 2002-04-12 -0.50 2002-04-15 -0.50 2002-04-16 -0.50 2002-02-20 -1.00 2002-02-21 -0.75 2002-02-22 -0.75 2002-02-25 -0.75 2002-02-26 -0.75 2002-02-27 -0.75 2002-02-28 -0.75 2002-03-01 -0.75 2002-03-04 -0.75 2002-03-05 -0.75 2002-03-06 -0.75 2002-03-07 -0.75 2002-03-08 -0.75 2002-03-11 -0.75 2002-03-12 -0.75 2002-03-13 -0.75 2002-03-14 -0.75 2002-03-15 -0.75 2002-03-18 -0.75 2002-03-19 -0.75 2002-01-23 -0.70 2002-01-24 -0.70 2002-01-28 -0.70 2002-01-29 -0.70 2002-01-30 -0.70 2002-01-31 -0.70 2002-02-01 -0.70 2002-02-04 -0.70 2002-02-05 -0.70 2002-02-06 -0.70 2002-02-07 -0.70 2002-02-08 -0.70 2002-02-13 -1.00 2002-02-14 -0.70 2002-02-15 -0.70 2002-02-18 -0.70 2002-02-19 -1.00 2002-01-21 -0.70 2002-01-22 -0.70 Retorno sobre o preço ajustado do Ibovespa entre as reuniões do copom. ts_temp &lt;- xts(temp[,-1], order.by=as.Date(temp[,1], &quot;%Y-%m-%d&quot;)) names(ts_temp) &lt;- &quot;price.adjusted&quot; ret_simples &lt;- diff(log(ts_temp$price.adjusted)) ggplot(ret_simples, aes(x = Index, y = price.adjusted))+ geom_line()+ labs(title=glue::glue(&#39;Ibovespa na data das reuniões&#39;), caption=&#39;Fonte: autor.&#39;)+ geom_line() + geom_smooth(method = &quot;loess&quot;, se = FALSE)+ theme(axis.text.x = element_text(angle = 90)) Computando o log retorno entre as datas das reuniões do COPOM e o preço ajustado, não há uma tendência explícita, sendo que os ganhos de (log retorno maior que zero) superam as perdas (-). Totalizando um retorno de NA. 4.11 Preço Ibovespa ajustado antes e depois da reunião offset &lt;- 10 Preço Ibovespa ajustado 10 dias antes e depois da reunião do copom. meeting_eval &lt;- data.frame(matrix(ncol = 4, nrow = 0)) colnames(meeting_eval) &lt;- c(&#39;meeting.date&#39;, &#39;period.start&#39;, &#39;period.end&#39;, &#39;return&#39;) for (row in 1:nrow(df_not_expected)) { meeting_date = df_not_expected[row, ]$ref.date start = meeting_date - offset end = meeting_date + offset temp &lt;- df_bvsp_selic %&gt;% filter(ref.date &gt;= start &amp; ref.date &lt;= end) %&gt;% select(ref.date, price.adjusted) ret_simples &lt;- diff(log(temp$price.adjusted)) # Adiciona as métricas no df meeting_eval[nrow(meeting_eval) + 1, ] &lt;- list(meeting_date, start, end, sum(ret_simples)) print(ggplot(temp, aes(x = ref.date, y = price.adjusted))+ geom_line()+ labs(title=glue::glue(&#39;Variação preço fechamento -{offset}d, reunião de {meeting_date}, +{offset}d&#39;), caption=&#39;Fonte: autor.&#39;)+ geom_line() + geom_smooth(method = &quot;loess&quot;, se = FALSE)+ theme(axis.text.x = element_text(angle = 90))+ geom_line(aes(x = meeting_date), color = &quot;red&quot;, linetype = &quot;dotted&quot;)) } meeting_eval$period.start &lt;- as.Date(meeting_eval$period.start) meeting_eval$period.end = as.Date(meeting_eval$period.end) meeting_eval$meeting.date = as.Date(meeting_eval$meeting.date) kable(meeting_eval) %&gt;% kable_styling(latex_options = &quot;striped&quot;) meeting.date period.start period.end return 2002-01-21 2002-01-11 2002-01-31 -0.0658593 2002-01-23 2002-01-13 2002-02-02 -0.0359218 2002-02-20 2002-02-10 2002-03-02 0.1013172 2002-03-20 2002-03-10 2002-03-30 -0.0327290 2002-04-17 2002-04-07 2002-04-27 -0.0064034 2002-05-22 2002-05-12 2002-06-01 0.0691262 2002-06-19 2002-06-09 2002-06-29 -0.1232444 2002-07-17 2002-07-07 2002-07-27 -0.1479784 2003-01-14 2003-01-04 2003-01-24 -0.1085084 2003-01-23 2003-01-13 2003-02-02 -0.1015144 2003-02-21 2003-02-11 2003-03-03 -0.0219345 2003-04-04 2003-03-25 2003-04-14 0.0555843 2003-04-24 2003-04-14 2003-05-04 0.0758750 2003-05-21 2003-05-11 2003-05-31 0.0076285 2003-06-18 2003-06-08 2003-06-28 -0.0612026 2003-07-23 2003-07-13 2003-08-02 -0.0342874 2003-08-20 2003-08-10 2003-08-30 0.1124592 2003-09-17 2003-09-07 2003-09-27 -0.0150029 2003-10-22 2003-10-12 2003-11-01 -0.0044390 2003-11-19 2003-11-09 2003-11-29 0.0832351 2004-01-02 2003-12-23 2004-01-12 0.1111213 2004-01-21 2004-01-11 2004-01-31 -0.1036337 2004-02-18 2004-02-08 2004-02-28 -0.0094701 2004-03-17 2004-03-07 2004-03-27 -0.0652317 2004-04-14 2004-04-04 2004-04-24 -0.0695917 2004-05-19 2004-05-09 2004-05-29 0.1108160 2005-01-03 2004-12-24 2005-01-13 -0.0445850 2005-01-20 2005-01-10 2005-01-30 -0.0134275 2005-02-17 2005-02-07 2005-02-27 0.0771679 2005-06-08 2005-05-29 2005-06-18 0.0259735 2005-06-15 2005-06-05 2005-06-25 -0.0253218 2005-07-20 2005-07-10 2005-07-30 0.0401950 2005-08-17 2005-08-07 2005-08-27 0.0142737 2005-09-14 2005-09-04 2005-09-24 0.0927507 2005-10-19 2005-10-09 2005-10-29 -0.0321867 2005-11-23 2005-11-13 2005-12-03 0.0829328 2005-12-14 2005-12-04 2005-12-24 0.0190822 2006-01-18 2006-01-08 2006-01-28 0.0679604 2006-03-08 2006-02-26 2006-03-18 -0.0292406 2006-06-20 2006-06-10 2006-06-30 0.0877090 2006-07-19 2006-07-09 2006-07-29 0.0337346 2006-08-30 2006-08-20 2006-09-09 -0.0163598 2006-10-17 2006-10-07 2006-10-27 0.0237230 2007-05-18 2007-05-08 2007-05-28 0.0359811 2007-06-06 2007-05-27 2007-06-16 0.0450005 2007-08-13 2007-08-03 2007-08-23 -0.0190657 2009-02-27 2009-02-17 2009-03-09 -0.0811538 2009-03-31 2009-03-21 2009-04-10 0.0705014 2011-10-13 2011-10-03 2011-10-23 0.0842200 2011-10-19 2011-10-09 2011-10-29 0.1107651 2011-11-30 2011-11-20 2011-12-10 0.0340757 2012-04-11 2012-04-01 2012-04-21 -0.0426343 2012-04-18 2012-04-08 2012-04-28 -0.0197737 2012-08-27 2012-08-17 2012-09-06 -0.0129641 2012-08-29 2012-08-19 2012-09-08 -0.0163604 2016-11-18 2016-11-08 2016-11-28 -0.0205183 2017-01-02 2016-12-23 2017-01-12 0.0988079 2017-01-11 2017-01-01 2017-01-21 0.0795198 2017-02-22 2017-02-12 2017-03-04 -0.0027214 2017-04-12 2017-04-02 2017-04-22 -0.0224864 2017-05-31 2017-05-21 2017-06-10 0.0086856 2017-07-26 2017-07-16 2017-08-05 0.0255256 2017-09-06 2017-08-27 2017-09-16 0.0646116 2017-11-03 2017-10-24 2017-11-13 -0.0520864 2017-12-06 2017-11-26 2017-12-16 -0.0197870 2019-09-02 2019-08-23 2019-09-12 0.0663881 2019-09-18 2019-09-08 2019-09-28 0.0182182 2020-06-03 2020-05-24 2020-06-13 0.0799718 2020-06-17 2020-06-07 2020-06-27 -0.0398112 positive_return = meeting_eval %&gt;% filter(return &gt; 0) %&gt;% pull(return) %&gt;% sum negative_return = meeting_eval %&gt;% filter(return &lt; 0) %&gt;% pull(return) %&gt;% sum overall_return = sum(meeting_eval$return) count_positive_return = meeting_eval %&gt;% filter(return &gt; 0) %&gt;% count count_negative_return = meeting_eval %&gt;% filter(return &lt; 0) %&gt;% count count_overall_return = count(meeting_eval) Avaliando o retorno nos períodos 10 dias antes e depois das reuniões, as perdas foram de -1.5174365 e os ganhos 2.1149377, gerando um acumulado de 0.5975011. Embora em cerca de 35 das amostra houve um retorno negativo, isso representa 0.51% das amostras. 4.12 Salvando o conjunto de dados # antes de proceder com o restante do exercicio, vamos salvar o dataset para # o usar nos proximos modulos write_rds(df_bvsp_selic, &quot;../dados/df_bvsp_selic.rds&quot;) 4.13 Correlação Ibovespa e Selic Comparando o Ibovespa e a Selic, percebemos que há uma relação, inversa, entre as duas séries. bvsp_norm &lt;- sd(df_bvsp_selic$price.adjusted, na.rm=TRUE) selic_norm &lt;- sd(df_bvsp_selic$selic.tax, na.rm=TRUE) df_bvsp_selic$price.adjusted.norm &lt;- log(df_bvsp_selic$price.adjusted) df_bvsp_selic$selic.tax.norm &lt;- log(df_bvsp_selic$selic.tax) ggplot(df_bvsp_selic, aes(x=ref.date)) + geom_line(aes(y=price.adjusted.norm), size=2, color=&quot;blue&quot;, na.rm=TRUE, alpha=.6) + stat_smooth(size=2, aes(y=price.adjusted.norm), color=&quot;green&quot;, size=1) + geom_line(aes(y=selic.tax.norm), size=2, color=&quot;red&quot;, na.rm=TRUE, alpha=.6) + stat_smooth(size=2, aes(y=selic.tax.norm), color = &quot;green&quot;) + scale_y_continuous( # Features of the first axis name = &quot;bvsp&quot;, # Add a second axis and specify its features sec.axis = sec_axis(~./selic_norm, name=&quot;selic&quot;) ) + #ggtitle(&quot;Ibovespa vs Selic&quot;) + labs(title = &quot;Ibovespa vs Selic&quot;, subtitle = &#39;Relação entre ativos e indicadores macroeconomicos&#39;, x = &#39;Anos&#39;, fill = &#39;&#39;) Avaliando a correlação cruzada (CCF - Cross Correlation Function) para toda a série, temos: # https://www.lobdata.com.br/2020/09/15/how-to-perform-correlation-analysis-in-time-series-data-using-r/ df_bvsp_selic_long &lt;- df_bvsp_selic %&gt;% pivot_longer(cols = c(price.adjusted, selic.tax), names_to = &quot;names&quot;, values_to = &quot;value&quot;) df_bvsp_selic_long %&gt;% plot_time_series(.date_var = ref.date, .value = value, .facet_vars = names) As séries tem comportamento muito próximo no tempo. # CCF plot df_bvsp_selic_long %&gt;% plot_acf_diagnostics(.date_var = ref.date, .value = price.adjusted.norm, .ccf_vars = selic.tax.norm, .show_ccf_vars_only = T) E todas as correlações são negativas. correlation &lt;- cor(df_bvsp_selic$price.adjusted, df_bvsp_selic$selic.tax, method = c(&quot;pearson&quot;)) Indicando que há uma relação inversa e forte, como indicado pelo coeficiente de Pearson de -0.7735688, Teste correlação cor.test(df_bvsp_selic$price.adjusted, df_bvsp_selic$selic.tax, method=c(&quot;pearson&quot;)) ## ## Pearson&#39;s product-moment correlation ## ## data: df_bvsp_selic$price.adjusted and df_bvsp_selic$selic.tax ## t = -91.152, df = 5576, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.7838984 -0.7628111 ## sample estimates: ## cor ## -0.7735688 Outra forma de exibir a correlação entre Ibovespa e Selic. A relação foi calculada tanto para o valor nominal, quanto para seus respectivos logs. ggcorr(df_bvsp_selic %&gt;% select(price.adjusted, selic.tax, price.adjusted.norm, selic.tax.norm), method = c(&quot;pairwise&quot;, &quot;pearson&quot;), label = TRUE, label_alpha = TRUE) A próxima seção avalia a correlação entre as séries em períodos distintos da história econômica. 4.13.1 Crise dos Subprime Avaliando para o período de 2004 a 2007: período de início da redução de juros até a crise dos Subprime: start_date = &#39;2004-01-01&#39; end_date = &#39;2007-12-31&#39; df_bvsp_selic_long %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% plot_time_series(.date_var = ref.date, .value = value, .facet_vars = names) correlation = df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor(price.adjusted, selic.tax, method = c(&quot;pearson&quot;))) correlation ## [1] -0.8018678 Teste correlação df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor.test(price.adjusted, selic.tax, method=c(&quot;pearson&quot;))) ## ## Pearson&#39;s product-moment correlation ## ## data: price.adjusted and selic.tax ## t = -42.162, df = 987, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8230632 -0.7784420 ## sample estimates: ## cor ## -0.8018678 period_eval[nrow(period_eval) + 1, ] &lt;- list(&quot;Crise dos subprime&quot;, start_date, end_date, correlation) 4.13.2 Crise financeira mundial No período de 2008 a 2010 houve o auge da crise financeira mundial no Brasil até a fase de recuperação do mercado. start_date = &#39;2008-01-01&#39; end_date = &#39;2010-12-31&#39; df_bvsp_selic_long %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% plot_time_series(.date_var = ref.date, .value = value, .facet_vars = names) correlation = df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor(price.adjusted, selic.tax, method = c(&quot;pearson&quot;))) correlation ## [1] -0.5980702 Teste correlação df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor.test(price.adjusted, selic.tax, method=c(&quot;pearson&quot;))) ## ## Pearson&#39;s product-moment correlation ## ## data: price.adjusted and selic.tax ## t = -20.3, df = 740, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.6423919 -0.5497611 ## sample estimates: ## cor ## -0.5980702 period_eval[nrow(period_eval) + 1, ] &lt;- list(&quot;Crise financeira mundial&quot;, start_date, end_date, correlation) 4.13.3 Crise da zona do euro De 2011 a 2012 foi o período que inclui tendência de redução da taxa básica de juros e a crise financeira, iniciada na Europa em 2011. start_date = &#39;2011-01-01&#39; end_date = &#39;2012-12-31&#39; df_bvsp_selic_long %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% plot_time_series(.date_var = ref.date, .value = value, .facet_vars = names) correlation = df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor(price.adjusted, selic.tax, method = c(&quot;pearson&quot;))) correlation ## [1] 0.2267515 Teste correlação df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor.test(price.adjusted, selic.tax, method=c(&quot;pearson&quot;))) ## ## Pearson&#39;s product-moment correlation ## ## data: price.adjusted and selic.tax ## t = 5.1589, df = 491, p-value = 3.612e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1412689 0.3088778 ## sample estimates: ## cor ## 0.2267515 period_eval[nrow(period_eval) + 1, ] &lt;- list(&quot;Crise da zona do euro&quot;, start_date, end_date, correlation) 4.13.4 Grande recessão brasileira No período de 2013 a 2016 houve a crise econômica de 2014, conhecida como a “grande recessão brasileira”, foi uma profunda e duradoura crise econômica, sendo caracterizada por recessão por dois anos consecutivos (2015 e 2016) e por sua longa e lenta recuperação, a mais lenta da história do país. (WIKIPEDIA, 2022). start_date = &#39;2013-01-01&#39; end_date = &#39;2016-12-31&#39; df_bvsp_selic_long %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% plot_time_series(.date_var = ref.date, .value = value, .facet_vars = names) correlation = df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor(price.adjusted, selic.tax, method = c(&quot;pearson&quot;))) correlation ## [1] -0.2077289 Teste correlação df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor.test(price.adjusted, selic.tax, method=c(&quot;pearson&quot;))) ## ## Pearson&#39;s product-moment correlation ## ## data: price.adjusted and selic.tax ## t = -6.6784, df = 989, p-value = 4.023e-11 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2665547 -0.1473611 ## sample estimates: ## cor ## -0.2077289 period_eval[nrow(period_eval) + 1, ] &lt;- list(&quot;Grande recessão brasileira&quot;, start_date, end_date, correlation) 4.13.5 COVID-19 Mais recente, o período de 2017 a 2022 inclue os efeitos da COVID-19, iniciada em 2020. start_date = &#39;2017-01-01&#39; end_date = &#39;2022-12-31&#39; df_bvsp_selic_long %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% plot_time_series(.date_var = ref.date, .value = value, .facet_vars = names) correlation = df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor(price.adjusted, selic.tax, method = c(&quot;pearson&quot;))) correlation ## [1] -0.1745323 Teste correlação df_bvsp_selic %&gt;% filter(ref.date &gt;= start_date &amp; ref.date &lt;= end_date) %&gt;% with(cor.test(price.adjusted, selic.tax, method=c(&quot;pearson&quot;))) ## ## Pearson&#39;s product-moment correlation ## ## data: price.adjusted and selic.tax ## t = -7.6363, df = 1856, p-value = 3.556e-14 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2182752 -0.1300894 ## sample estimates: ## cor ## -0.1745323 period_eval[nrow(period_eval) + 1, ] &lt;- list(&quot;COVID-19&quot;, start_date, end_date, correlation) 4.13.6 Resumo kable(period_eval) %&gt;% kable_styling(latex_options = &quot;striped&quot;) period_name period.start period.end cor-pearson Crise dos subprime 2004-01-01 2007-12-31 -0.8018678 Crise financeira mundial 2008-01-01 2010-12-31 -0.5980702 Crise da zona do euro 2011-01-01 2012-12-31 0.2267515 Grande recessão brasileira 2013-01-01 2016-12-31 -0.2077289 COVID-19 2017-01-01 2022-12-31 -0.1745323 4.14 Conclusão A hipótese de que Ibovespa é influenciado pela taxa Selic foi testada e há indícios que existe uma forte correlação, como demonstrado no quadro resumo. Ao avaliar toda a série, encontramos coeficientes de correlação superiores a \\(0.8\\) e mesmo nas grandes crises, a maior parte das correlações foi negativa, porém fraca. Quando analisamos os retornos do índice próximos as decisões do COPOM, os resultados não indicam uma tendência, sendo que aproximadamente \\(50%\\) das vezes as decisões consideradas surpresa afetaram negativamente, porém o acumulado dos retornos para esses períodos foi positivo. É de opinião deste autor, que com base nos dados obtidos, existe uma correlação forte entre a Selic e o Ibovespa, porém, essa relação não necessariamente é unilateral ou exclusiva, outros indicadores macroeconômicos e fatores socioeconômicos podem influenciar ambas as séries e suas relações. Como trabalho futuro, revisar a forma como a surpresa do mercado é calculada, seguindo o modelo usado por Gonçalves Junior (2007) e incluir outros indicadores macroeconômicos, como utilizados por De Souza Grôppo (2005). 4.15 Referências FOCUS. Pesquisa Focus. Disponível em: https://www.bcb.gov.br/publicacoes/focus. Acesso em: 03 dez. 2022. CASTRO, F. Henrique et al. Fifty-year History of the Ibovespa. Brazilian Review of Finance, v. 17, n. 3, p. 47-65, 2019. DE SOUZA GRÔPPO, Gustavo. Co-integração e causalidade entre variáveis de política monetária e Ibovespa. Revista de Economia e Administração, v. 4, n. 2, 2005. DOS SANTOS, Fernando Siqueira; PRADO, Roberto RA. Causalidade Selic-Ibovespa revisada. Revista de Economia e Administração, v. 5, n. 1, 2006. GONÇALVES JUNIOR, Walter. Surpresas com relação à política monetária e o mercado de capitais: Evidências do caso brasileiro. 2007. Tese de Doutorado. LUCAS, Robert E.; SARGENT, Thomas. After Keynesian Macroeconomics. In: Miller, P. J. (org.) The Rational Expectations Revolutions: Readings from the Front Line. [S.1]: Massachussetts Institute of Technology, p.05-30, 1996. YOSHIDA, Olga Satomi. BIGDATA ANALITYCS: data 22-11-2022. São Paulo: Instituto de Pesquisas Tecnológicas do Estado de São Paulo, 2022. 4 slides, color. WIKIPEDIA. Lista de crises econômicas no Brasil. Disponível em: https://pt.wikipedia.org/wiki/Lista_de_crises_econ%C3%B4micas_no_Brasil. Acesso em: 03 dez. 2022. 4.16 Apêndice A - Código-fonte Todos os códigos e dados, com exceção dos obtidos on-line, podem ser encontrados em https://github.com/asantos2000/caderno-bd. O caderno pode ser acessado em https://bookdown.org/adsantos/caderno-bd/. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
