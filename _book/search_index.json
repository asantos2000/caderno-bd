[["index.html", "Caderno de exercícos Mestrado de computação aplicada Big Data Analytics 2022 Organização", " Caderno de exercícos Mestrado de computação aplicada Anderson dos Santos 2022-11-15 Big Data Analytics 2022 Autor: Anderson dos Santos Prof: Olga Satomi Yoshida Organização Analise de ações (Gráficos + Discritivos) Lista de regressões (Aula 6) - EXERCICIOS.pdf Lab 3 (Aula 8 - Classificação) LABORATÓRIO R.pdf Trabalho de conclusão "],["descriptive.html", "Cap. 1 Estatística descritiva 1.1 Exercício 1.2 Pacotes 1.3 Conjunto de dados 1.4 Informações dos atributos 1.5 Análise descritiva de algumas ações 1.6 Avaliando NUBR33", " Cap. 1 Estatística descritiva Utilizamos métodos de Estatística Descritiva para organizar, resumir e descrever os aspectos importantes de um conjunto de características observadas ou comparar tais características entre dois ou mais conjuntos 1.1 Exercício Descrever o conjunto de dados da bolsa de valores brasileira (B3) 1.2 Pacotes Pacotes necessários para executar este capítulo: library(readxl) library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.4.0 ✔ purrr 0.3.5 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(readxl) library(tibble) library(knitr) library(kableExtra) ## ## Attaching package: &#39;kableExtra&#39; ## ## The following object is masked from &#39;package:dplyr&#39;: ## ## group_rows library(shiny) library(dplyr) 1.3 Conjunto de dados Este conjunto de dados de ações da bovespa foi criado pela professora Dra.Olga Satomi Yoshida para aula de Big Data no IPT. 1.4 Informações dos atributos Enconta no forma de planilha excel - 03 B2 DADOS Time Series Preços Açoes Diario.xlsx, contém 10 séries com as ações: ELET3; ITUB4; ITSA4; PETR4; BBDC4; VALE3; BBAS3; LREN3; todos para o período de 01/04/2010 à 19/09/2022 e NUBR33 período 12/09/2021 a 19/09/2022; MGLU3 perído 05/02/2011 a 19/09/2022. Os atributos para cada série são a data e valor no fechamento do pregão. series = read_excel(&quot;dados/03 B1 ESTATISTICAS DESC E REGRESSAO.xlsx&quot;, sheet = 2, n_max = 1) ## New names: ## • `44877` -&gt; `44877...2` ## • `44877` -&gt; `44877...4` ## • `44877` -&gt; `44877...6` ## • `44877` -&gt; `44877...8` ## • `44877` -&gt; `44877...10` ## • `44877` -&gt; `44877...12` ## • `44877` -&gt; `44877...14` ## • `44877` -&gt; `44877...16` ## • `44877` -&gt; `44877...18` ## • `44877` -&gt; `44877...20` nomes = names(series)[seq(1, 20, 2)] series = read_excel(&quot;dados/03 B1 ESTATISTICAS DESC E REGRESSAO.xlsx&quot;, sheet = 2, skip = 1) ## New names: ## • `Date` -&gt; `Date...1` ## • `Close` -&gt; `Close...2` ## • `Date` -&gt; `Date...3` ## • `Close` -&gt; `Close...4` ## • `Date` -&gt; `Date...5` ## • `Close` -&gt; `Close...6` ## • `Date` -&gt; `Date...7` ## • `Close` -&gt; `Close...8` ## • `Date` -&gt; `Date...9` ## • `Close` -&gt; `Close...10` ## • `Date` -&gt; `Date...11` ## • `Close` -&gt; `Close...12` ## • `Date` -&gt; `Date...13` ## • `Close` -&gt; `Close...14` ## • `Date` -&gt; `Date...15` ## • `Close` -&gt; `Close...16` ## • `Date` -&gt; `Date...17` ## • `Close` -&gt; `Close...18` ## • `Date` -&gt; `Date...19` ## • `Close` -&gt; `Close...20` names(series)[seq(1, 20, 2)] = paste0(&quot;DATA_&quot;, nomes) names(series)[seq(2, 20, 2)] = nomes ## Transforma o tipo das colunas de Datetime para Date. for (i in seq(1, 20, 2)) { series[[i]] = as.Date(series[[i]]) } kable(head(series, 10)) %&gt;% kable_styling(latex_options = &quot;striped&quot;) DATA_ELET3 ELET3 DATA_ITUB4 ITUB4 DATA_ITSA4 ITSA4 DATA_PETR4 PETR4 DATA_NUBR33 NUBR33 DATA_MGLU3 MGLU3 DATA_BBDC4 BBDC4 DATA_VALE3 VALE3 DATA_BBAS3 BBAS3 DATA_LREN3 LREN3 2010-01-04 37.37 2010-01-04 20.10 2010-01-04 8.05 2010-01-04 37.32 2021-12-09 10.04 2011-05-02 0.51 2010-01-04 12.16 2010-01-04 51.49 2010-01-04 29.90 2010-01-04 6.05 2010-01-05 37.07 2010-01-05 20.23 2010-01-05 8.02 2010-01-05 37.00 2021-12-10 11.50 2011-05-03 0.51 2010-01-05 12.10 2010-01-05 51.97 2010-01-05 29.60 2010-01-05 5.83 2010-01-06 36.59 2010-01-06 20.05 2010-01-06 7.92 2010-01-06 37.50 2021-12-13 10.65 2011-05-04 0.52 2010-01-06 12.00 2010-01-06 53.07 2010-01-06 29.64 2010-01-06 5.72 2010-01-07 37.44 2010-01-07 19.84 2010-01-07 7.88 2010-01-07 37.15 2021-12-14 9.25 2011-05-05 0.51 2010-01-07 11.97 2010-01-07 53.29 2010-01-07 29.65 2010-01-07 5.66 2010-01-08 38.07 2010-01-08 19.54 2010-01-08 7.82 2010-01-08 36.95 2021-12-15 9.54 2011-05-06 0.51 2010-01-08 11.95 2010-01-08 53.81 2010-01-08 29.82 2010-01-08 5.69 2010-01-11 37.48 2010-01-11 19.37 2010-01-11 7.79 2010-01-11 36.83 2021-12-16 9.49 2011-05-10 0.50 2010-01-11 11.96 2010-01-11 53.65 2010-01-11 30.05 2010-01-11 5.80 2010-01-12 37.13 2010-01-12 19.19 2010-01-12 7.79 2010-01-12 36.36 2021-12-17 8.95 2011-05-11 0.51 2010-01-12 12.00 2010-01-12 53.50 2010-01-12 29.80 2010-01-12 5.94 2010-01-13 37.49 2010-01-13 19.25 2010-01-13 7.77 2010-01-13 36.30 2021-12-20 8.50 2011-05-12 0.51 2010-01-13 12.06 2010-01-13 54.16 2010-01-13 30.18 2010-01-13 6.00 2010-01-14 36.89 2010-01-14 18.99 2010-01-14 7.66 2010-01-14 35.67 2021-12-21 8.70 2011-05-13 0.51 2010-01-14 11.81 2010-01-14 54.15 2010-01-14 29.75 2010-01-14 5.87 2010-01-15 35.59 2010-01-15 18.67 2010-01-15 7.47 2010-01-15 35.75 2021-12-22 8.54 2011-05-17 0.50 2010-01-15 11.64 2010-01-15 53.45 2010-01-15 29.25 2010-01-15 5.66 1.5 Análise descritiva de algumas ações plot(x=series$DATA_ELET3, y= series$ELET3, type = &quot;l&quot;, xlab = &quot;Tempo (dias)&quot;, ylab = &quot;Preço fechamento (R$)&quot;, main = &quot;Performance de algumas ações ao longo do tempo&quot;, ylim = c(0, 60) ) lines(x=series$DATA_ITUB4, y= series$ITUB4, col = 2) lines(x=series$DATA_PETR4, y= series$PETR4, col = 3) lines(x=series$DATA_NUBR33, y= series$NUBR33, col = 4) legend(&quot;top&quot;, legend = c(&quot;ELET3&quot;, &quot;ITUB4&quot;, &quot;PETR4&quot;, &quot;NUBR33&quot;), fill = c(1,2,3,4)) &gt; NUBR33 tem uma série menor que as demais ações. Parâmetros stock_symbols = c(&quot;ELET3&quot;,&quot;ITUB4&quot;,&quot;ITSA4&quot;,&quot;PETR4&quot;,&quot;NUBR33&quot;,&quot;MGLU3&quot;,&quot;BBDC4&quot;,&quot;VALE3&quot;,&quot;BBAS3&quot;,&quot;LREN3&quot;) stock_symbol = 5 # 1 a 10 dataInicial = NULL # as.Date(&quot;2022-01-01&quot;) dataFinal = NULL # as.Date(&quot;2022-12-31&quot;) 1.6 Avaliando NUBR33 symbol_series = series %&gt;% dplyr::select(matches(stock_symbols[stock_symbol]) ) names(symbol_series)[1] = &quot;stock_date&quot; names(symbol_series)[2] = &quot;stock_value&quot; if (is.null(dataInicial)) { dataInicial = min(symbol_series$stock_date, na.rm = TRUE) } if (is.null(dataFinal)) { dataFinal = max(symbol_series$stock_date, na.rm = TRUE) } subConjunto = filter(symbol_series, stock_date &gt;= dataInicial &amp; stock_date &lt;= dataFinal) Data = subConjunto$stock_date Ser = subConjunto$stock_value getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } cont = count(subConjunto) menor = min(Ser) maior = max(Ser) p = quantile(Ser, c(.025, .10, .25, .50, .75, .90, .975)) media = mean(Ser, na.rm = T) desvio = sd(Ser, na.rm = T) mediana = median(Ser, na.rm = T) moda = getmode(Ser) ep = desvio / sqrt(cont) periodo = dataFinal - dataInicial Medidas de posição: Contagem de resultados 195 Menor valor: 2.72 Percentil: 2.9885, 3.352, 3.6, 4.76, 6.585, 8.6, 9.3285 Maior valor: 11.5 Medidas de tendência central: Moda: 3.6 Mediana: 4.76 Média dos resultados: 5.3922564 Medidas de dispersão dos dados individuais em relação a média: desvio padrão dos resultados em relação a média: 2.0007468 coeficiente de variação = d.p. / média: 0.3710407 c.v. % = (d.p. / média ) x 100 %: 37.1040734 Medidas de disperção da média: e.p. = d.p. da média = d.p. / Raiz(n): 0.1432765 c.v.e.p. = e.p. / média: 0.0265708 c.v.e.p.% = (e.p. / média) x 100 %: 2.6570779 Para NUBR33 o preço médio da ação, no período de 2021-12-09 a 2022-09-19 (período de 284 dias) é de 5.3922564 e o desvio padrão de 2.0007468. plot(subConjunto, type = &quot;l&quot;, xlab = &quot;Tempo (dias)&quot;, ylab = &quot;Preço fechamento (R$)&quot;, main=stock_symbols[stock_symbol]) caixa = boxplot(Ser, add = T) limSup = caixa$stats[5,1] Q3 = caixa$stats[4,1] Q2 = caixa$stats[3,1] Q1 = caixa$stats[2,1] limInf = caixa$stats[1,1] abline(h = Q2, col = &quot;black&quot;) abline(h = Q3, col = &quot;red&quot;, lwd = 2) abline(h = Q1, col = &quot;red&quot;, lwd = 2) abline(h = limSup, col = &quot;green&quot;, lwd = 2) abline(h = limInf, col = &quot;green&quot;, lwd = 2) legend(&quot;top&quot;, legend = c(&quot;Sup&quot;, &quot;Q3&quot;, &quot;Q2&quot;, &quot;Q1&quot;, &quot;Inf&quot;), fill = c(&quot;green&quot;,&quot;red&quot;,&quot;black&quot;,&quot;red&quot;,&quot;green&quot;)) Usando boxplot para obter limites e percentíls. "],["regression.html", "Cap. 2 Regressão 2.1 Conjunto de dados 2.2 Preço das casas 2.3 Análise conjunto energia 2.4 Análise Conjunto vendas vs fontes de publidades 2.5 Análise conjunto ST vs demais variáveisCREDIT SCORE X RENDA E OUTRAS V 2.6 Consumo alimentar médio", " Cap. 2 Regressão A regressão em geral tem como objetivo: Medir a influência de uma ou mais variáveis explicativas (x) sobre a variável resposta (y); Predição de uma variável resposta (y) a partir de uma ou mais variáveis explicativas (x). 2.1 Conjunto de dados Este conjunto de dados foram criados pela professora Dra.Olga Satomi Yoshida para aula de Big Data no IPT. Preço das casas: Data_HousePrice_Area.xlsx Consumo de energia: Data_ConsumoEnergia.xlsx SALES_X_YOUTUBE: DadosAula06.xlsx CREDIT SCORE X RENDA E OUTRAS V: DadosAula06.xlsx 2.2 Preço das casas Análise descritiva e regressão linear sobre o conjunto de dados Data_HousePrice_Area.xlsx. 2.2.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(knitr) library(kableExtra) 2.2.2 Conjunto de dados dadosCen01 = read_excel(&quot;../dados/Data_HousePrice_Area.xlsx&quot;, sheet = 1) dadosCen02 = read_excel(&quot;../dados/Data_HousePrice_Area.xlsx&quot;, sheet = 2) Dados do cenário 01 Square Feet House Price 1400 245 1600 312 1700 279 1875 308 1100 199 1550 219 2350 405 2450 324 1425 319 1700 255 Dados do cenário 02 Square Feet House Price 1400 245 1800 312 1700 279 1875 308 1200 199 1480 219 2350 405 2100 324 2000 319 1700 255 No gráfico: Comparando os dois gráficos, podemos observar: O primeiro conjunto é mais esparso O segundo cenário os dados estão agrupados de forma linear 2.2.3 Descrevendo os dados: 2.2.3.1 Cenário 1 House Price summary(dadosCen01$`House Price`) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 199.0 247.5 293.5 286.5 317.2 405.0 Square Feet summary(dadosCen01$`Square Feet`) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1100 1456 1650 1715 1831 2450 Distribuição dos valores hist(dadosCen01$`House Price`) 2.2.3.2 Cenário 2 House Price summary(dadosCen02$`House Price`) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 199.0 247.5 293.5 286.5 317.2 405.0 Square Feet summary(dadosCen02$`Square Feet`) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1200 1535 1750 1760 1969 2350 Distribuição dos valores hist(dadosCen01$`House Price`) 2.2.4 Ajustes de modelos lineares simples Vamos agora ajustar um modelo de regressão para ambos os cenários. 2.2.4.1 Cenário 01 modelCen01 &lt;- lm(dadosCen01$`Square Feet` ~ dadosCen01$`House Price`) modelCen01 ## ## Call: ## lm(formula = dadosCen01$`Square Feet` ~ dadosCen01$`House Price`) ## ## Coefficients: ## (Intercept) dadosCen01$`House Price` ## 199.034 5.291 \\(y = 199.034 + 5.291 x\\) Cálculo do resíduo: \\(y = 199.034 + 5.291 x\\) para \\(x = 1400\\), \\(y = valor\\). Diferença entre o y dado e o calculado é o resíduo. resumoMod01 = summary(modelCen01) resumoMod01 ## ## Call: ## lm(formula = dadosCen01$`Square Feet` ~ dadosCen01$`House Price`) ## ## Residuals: ## Min 1Q Median 3Q Max ## -461.97 -137.86 16.33 125.32 536.58 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 199.034 464.284 0.429 0.6795 ## dadosCen01$`House Price` 5.291 1.589 3.329 0.0104 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 287 on 8 degrees of freedom ## Multiple R-squared: 0.5808, Adjusted R-squared: 0.5284 ## F-statistic: 11.08 on 1 and 8 DF, p-value: 0.01039 \\(R^2\\): 0.58 Vamos analisar os resíduos: plot(modelCen01$residuals ~ dadosCen01$`House Price`) plot(modelCen01, pch = 16, col = &quot;blue&quot;) plot(dadosCen01$`Square Feet` ~ dadosCen01$`House Price`) abline(modelCen01) #Add a regression line Como as observações são mais esparças, um modelo linear simples não se ajusta muito bem. 2.2.4.2 Cenário 02 modelCen02 &lt;- lm(dadosCen02$`Square Feet` ~ dadosCen02$`House Price`) modelCen02 ## ## Call: ## lm(formula = dadosCen02$`Square Feet` ~ dadosCen02$`House Price`) ## ## Coefficients: ## (Intercept) dadosCen02$`House Price` ## 186.202 5.495 \\(y = 186.202 + 5.495 x\\) plot(modelCen02, pch = 16, col = &quot;blue&quot;) plot(dadosCen02$`Square Feet` ~ dadosCen01$`House Price`) abline(modelCen01) #Add a regression line Cálculo do resíduo: \\(y = 186.202 + 5.495 x\\) para \\(x = 1400\\), \\(y = valor\\). Diferença entre o y dado e o calculado é o resíduo. resumoMod02 = summary(modelCen02) resumoMod02 ## ## Call: ## lm(formula = dadosCen02$`Square Feet` ~ dadosCen02$`House Price`) ## ## Residuals: ## Min 1Q Median 3Q Max ## -132.46 -75.18 -11.46 83.03 133.44 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 186.2023 162.3765 1.147 0.285 ## dadosCen02$`House Price` 5.4949 0.5558 9.886 9.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 100.4 on 8 degrees of freedom ## Multiple R-squared: 0.9243, Adjusted R-squared: 0.9149 ## F-statistic: 97.73 on 1 and 8 DF, p-value: 9.246e-06 \\(R^2\\): 0.92 Vamos analisar os resíduos: plot(modelCen02$residuals ~ dadosCen02$`House Price`) Neste cenário as observações estão mais agrupadas próximas a uma reta, sendo assim o modelo linear simples descreveu melhor as observações. 2.2.5 Predição p = predict(modelCen02, newdata = data.frame(x = 1300)) ## Warning: &#39;newdata&#39; had 1 row but variables found have 10 rows Usando o modelo um, uma casa de \\(1300 ft^2\\) custaria 1532.460361, 1900.620742, 1719.2880171, 1878.6410178, 1279.6935323, 1389.5921535, 2411.6493305, 1966.5599147, 1939.0852594, 1587.4096716 2.3 Análise conjunto energia Análise descritiva e regressão linear sobre o conjunto de dados Data_ConsumoEnergia.xlsx. 2.3.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) 2.3.2 Conjunto de dados Ajuste na maquina Consumo de energia 11.15 21.6 15.70 4.0 18.90 1.8 19.40 1.0 21.40 1.0 21.70 0.8 25.30 3.8 26.40 7.4 26.70 4.3 29.10 36.2 Um gráfico dos dados: Ajustando os modelos: modelEner01 = lm(`Consumo de energia`~`Ajuste na maquina`, data = dados02) modelEner02 = lm(`Consumo de energia`~`Ajuste na maquina` + I(`Ajuste na maquina`^2), data = dados02) modelEner03 = lm(`Consumo de energia`~`Ajuste na maquina` + I(`Ajuste na maquina`^2) + I(`Ajuste na maquina`^3), data = dados02) modelEner04 = lm(`Consumo de energia`~`Ajuste na maquina` + I(`Ajuste na maquina`^2) + I(`Ajuste na maquina`^3)+ I(`Ajuste na maquina`^4), data = dados02) Gráfico do ajuste dos modelos: x = seq(0,40, 0.1) y1 = modelEner01$coefficients[1] + modelEner01$coefficients[2]*x y2 = modelEner02$coefficients[1] + modelEner02$coefficients[2]*x + modelEner02$coefficients[3]*x^2 y3 = modelEner03$coefficients[1] + modelEner03$coefficients[2]*x + modelEner03$coefficients[3]*x^2 + modelEner03$coefficients[4]*x^3 y4 = modelEner04$coefficients[1] + modelEner04$coefficients[2]*x + modelEner04$coefficients[3]*x^2 + modelEner04$coefficients[4]*x^3 + modelEner04$coefficients[5]*x^4 par(mfrow = c(1,1)) plot(`Consumo de energia`~`Ajuste na maquina`, data = dados02, col = 1, pch = 16, xlab = &quot;Ajuste na máquina&quot;, ylab = &quot;Consumo de energia&quot;, main = &quot;Consumo de energia x Ajuste na máquina&quot;, ylim = c(-10, 40)) lines(y1~x, col = 2, lty = 1, lwd = 2) lines(y2~x, col = 3, lty = 2, lwd = 2) lines(y3~x, col = 4, lty = 3, lwd = 2) lines(y4~x, col = 5, lty = 4, lwd = 2) legend(&quot;top&quot;, legend = c(&quot;Linear&quot;, &quot;Grau 2&quot;, &quot;Grau 3&quot;, &quot;Grau 4&quot;), fill = c(2,3,4,5)) O modelo polinomial de grau 2 representou bem as observações sem o risco de perda de generalização. Polinômios de graus mais altos correm o risco de não generalizarem o fenômeno, embora se saiam muito bem com as observações do treinamento. 2.4 Análise Conjunto vendas vs fontes de publidades Análise descritiva e regressão linear sobre o conjunto de dados SALES_X_YOUTUBE em DadosAula06.xlsx. 2.4.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) 2.4.2 Conjunto de dados dados03 = read_excel(path = &quot;../dados/04_LABORATORIO REGRESSAO COM DADOS 03_DADOS.xlsx&quot;, sheet = 3) dados03 = dados03[,2:5] # tail(dados03, 3) # dados03_t = pivot_longer(dados03, c(2:5)) # names(dados03_t) = c(&quot;Indice&quot;, &quot;Grupo&quot;, &quot;Valor&quot;) kable(dados03) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Vendas em relação aos anúncios no youtube. ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Ajustando um modelo linear simples model = lm(sales ~ sqrtYou, data = dados03_mod) summary(model) ## ## Call: ## lm(formula = sales ~ sqrtYou, data = dados03_mod) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.916 -2.344 -0.131 2.326 9.316 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.20688 0.80092 4.004 8.8e-05 *** ## sqrtYou 1.09042 0.06029 18.085 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.854 on 198 degrees of freedom ## Multiple R-squared: 0.6229, Adjusted R-squared: 0.621 ## F-statistic: 327.1 on 1 and 198 DF, p-value: &lt; 2.2e-16 O gráfico mostra as observações em relação ao modelo. plot(sales ~ sqrtYou, data = dados03_mod) abline(model) Analisando os resíduos. plot(model) Ajustando o modelo a mais variáveis (multiclass). modelMult = lm(sales ~ youtube + facebook + newspaper, data = dados03) summary(modelMult) ## ## Call: ## lm(formula = sales ~ youtube + facebook + newspaper, data = dados03) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.5932 -1.0690 0.2902 1.4272 3.3951 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.526667 0.374290 9.422 &lt;2e-16 *** ## youtube 0.045765 0.001395 32.809 &lt;2e-16 *** ## facebook 0.188530 0.008611 21.893 &lt;2e-16 *** ## newspaper -0.001037 0.005871 -0.177 0.86 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.023 on 196 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 ## F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 plot(modelMult) Newspaper tem pouca influência no modelo, sendo o youtube a que mais influência nas vendas. modelMult2 = lm(sales ~ youtube + facebook, data = dados03) summary(modelMult2) ## ## Call: ## lm(formula = sales ~ youtube + facebook, data = dados03) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.5572 -1.0502 0.2906 1.4049 3.3994 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.50532 0.35339 9.919 &lt;2e-16 *** ## youtube 0.04575 0.00139 32.909 &lt;2e-16 *** ## facebook 0.18799 0.00804 23.382 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.018 on 197 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8962 ## F-statistic: 859.6 on 2 and 197 DF, p-value: &lt; 2.2e-16 plot(modelMult2) 2.5 Análise conjunto ST vs demais variáveisCREDIT SCORE X RENDA E OUTRAS V Análise descritiva e regressão linear sobre o conjunto de dados CREDIT SCORE X RENDA E OUTRAS V em DadosAula06.xlsx. 2.5.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) 2.5.2 Conjunto de dados Analise se o cliente pode receber o crédito de acordo com a análise. As variáveis são: ST - Situação (0 - Passou na análise, 1 - Nâo passou na análise) - Y R - Renda - X ND - Num Dependentes - X VE - Vinculo Empregaticio - X dados04 = read_excel(path = &quot;../dados/04_LABORATORIO REGRESSAO COM DADOS 03_DADOS.xlsx&quot;, sheet = 4) dados04 = dados04[,18:21] dados04$ST = factor(dados04$ST) dados04$VE = factor(dados04$VE) kable(dados04) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Situação explicada pela renda. plot(dados04$R ~ dados04$ST) # table(dados04$VE, dados04$ST) O modelo é \\[ \\log{\\left(\\frac{P(y_i=1)}{1-P(y_i=1)}\\right)} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon_i \\] \\[ \\frac{P(y=1)}{1-P(y=1)} = e^{(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3)} \\] O ajuste é modelo04 = glm(dados04$ST ~ dados04$R + dados04$ND + dados04$VE, family = binomial(link=&#39;logit&#39;)) valoresPredito = predict.glm(modelo04, type = &quot;response&quot;) summary(modelo04) ## ## Call: ## glm(formula = dados04$ST ~ dados04$R + dados04$ND + dados04$VE, ## family = binomial(link = &quot;logit&quot;)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6591 -0.2633 -0.0531 0.4187 2.0147 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.1117 1.5725 0.707 0.479578 ## dados04$R -1.7872 0.4606 -3.880 0.000105 *** ## dados04$ND 0.9031 0.3857 2.341 0.019212 * ## dados04$VE1 2.9113 0.8506 3.423 0.000620 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 126.450 on 91 degrees of freedom ## Residual deviance: 51.382 on 88 degrees of freedom ## AIC: 59.382 ## ## Number of Fisher Scoring iterations: 6 Os valores preditos são: ## Warning in confusionMatrix.default(valoresPredito_cl, dados04$ST): Levels are not in ## the same order for reference and data. Refactoring data to match. ## Warning in confusionMatrix.default(valoresPredito_cl, dados04$ST): Levels are not in ## the same order for reference and data. Refactoring data to match. ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 46 5 ## 1 5 36 ## ## Accuracy : 0.8913 ## 95% CI : (0.8092, 0.9466) ## No Information Rate : 0.5543 ## P-Value [Acc &gt; NIR] : 2.554e-12 ## ## Kappa : 0.78 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.9020 ## Specificity : 0.8780 ## Pos Pred Value : 0.9020 ## Neg Pred Value : 0.8780 ## Prevalence : 0.5543 ## Detection Rate : 0.5000 ## Detection Prevalence : 0.5543 ## Balanced Accuracy : 0.8900 ## ## &#39;Positive&#39; Class : 0 ## Matriz de confusão draw_confusion_matrix(cm) A acurácia do modelo é de 89% e a sensibilidade é alta, em torno de 90%. Nos dados treinados o acerto de “passou na análise” foi de 100% (46/46). Já a especificidade é de 88% havendo confusão com 5/46 observações. O mesmo ocorreu para “não passou na análise”, onde 36/46 observações estão corretas e 5/46 não. 2.6 Consumo alimentar médio Análise descritiva e regressão linear. 2.6.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) library(factoextra) ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa 2.6.2 Conjunto de dados Considere os dados a seguir do consumo alimentar médio de diferentes tipos de alimentos para famílias classificadas de acordo com o número de filhos (2, 3, 4 ou 5) e principal área de trabalho (MA: Setor de Trabalho Manual, EM: Empregados do Setor Público ou CA: Cargos Administrativos). Fonte: LABORATORIO-R.pdf dados = tibble(AreaTrabalho = as.factor(rep(c(&quot;MA&quot;, &quot;EM&quot;, &quot;CA&quot;), 4)), Filhos = as.factor(rep(2:5, each = 3)), Paes = c(332, 293, 372, 406, 386, 438, 534, 460, 385, 655, 584, 515), Vegetais = c(428, 559, 767, 563, 608, 843, 660, 699, 789, 776, 995, 1097), Frutas = c(354, 388, 562, 341, 396, 689, 367, 484, 621, 423, 548, 887), Carnes = c(1437,1527,1948,1507,1501,2345,1620,1856,2366,1848,2056,2630), Aves = c(526, 567, 927, 544, NA, 1148,638, 762, 1149,759, 893, 1167), Leite = c(247, 239, 235, 324, 319, 243, 414, 400, 304, 495, 518, 561), Alcoolicos = c(427, 258, 433, 407, 363, 341, 407, 416, 282, 486, 319, 284)) kable(dados) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 2.6.3 Regressão #dummy &lt;- dummyVars(&quot; ~ .&quot;, data=dados) #dadosS &lt;- data.frame(predict(dummy, newdata = dados)) dadosS = subset(dados, select=c(&quot;Aves&quot;, &quot;Filhos&quot;, &quot;Paes&quot;, &quot;Vegetais&quot;, &quot;Frutas&quot;, &quot;Carnes&quot;, &quot;Leite&quot;, &quot;Alcoolicos&quot;)) #modelo = lm(dadosS$Aves ~ dadosS$AreaTrabalho.CA + dadosS$AreaTrabalho.EM + dadosS$AreaTrabalho.MA + dadosS$Filhos.2 + dadosS$Filhos.3 + dadosS$Filhos.4 + dadosS$Filhos.5 + dadosS$Paes + dadosS$Vegetais + dadosS$Frutas + dadosS$Carnes + dadosS$Leite + dadosS$Alcoolicos) modelo = lm(Aves ~ Filhos + Paes + Vegetais + Frutas + Carnes + Leite + Alcoolicos, data = dadosS) valoresPredito = predict.lm(modelo, type = &quot;response&quot;) summary(modelo) ## ## Call: ## lm(formula = Aves ~ Filhos + Paes + Vegetais + Frutas + Carnes + ## Leite + Alcoolicos, data = dadosS) ## ## Residuals: ## 1 2 3 4 6 7 8 9 10 11 12 ## 11.874 -9.816 -2.058 -1.250 1.250 3.533 -5.322 1.789 -8.056 9.567 -1.510 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -746.3706 632.2722 -1.180 0.447 ## Filhos3 -75.6290 123.1415 -0.614 0.649 ## Filhos4 -105.4167 262.6281 -0.401 0.757 ## Filhos5 -192.0193 421.1901 -0.456 0.728 ## Paes 0.1803 0.3299 0.547 0.681 ## Vegetais 0.2438 0.2282 1.068 0.479 ## Frutas -0.7431 0.9391 -0.791 0.574 ## Carnes 0.9335 0.5221 1.788 0.325 ## Leite -0.1543 1.1509 -0.134 0.915 ## Alcoolicos 0.1313 0.2243 0.586 0.663 ## ## Residual standard error: 21.15 on 1 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.9993, Adjusted R-squared: 0.9928 ## F-statistic: 153.7 on 9 and 1 DF, p-value: 0.06252 confint(modelo) ## 2.5 % 97.5 % ## (Intercept) -8780.151071 7287.409914 ## Filhos3 -1640.289906 1489.031864 ## Filhos4 -3442.422544 3231.589190 ## Filhos5 -5543.747107 5159.708466 ## Paes -4.010944 4.371643 ## Vegetais -2.655840 3.143461 ## Frutas -12.674990 11.188748 ## Carnes -5.701068 7.567978 ## Leite -14.778470 14.469823 ## Alcoolicos -2.718471 2.981129 rse = sigma(modelo)/mean(dadosS$Aves, na.rm = TRUE) rse ## [1] 0.02562551 plot(dadosS, pch = 16, col = &quot;blue&quot;) #Plot the results plot(modelo$residuals, pch = 16, col = &quot;red&quot;) Verificando modelo library(performance) check_model(modelo) 2.6.4 Predição predict(modelo, interval = &quot;prediction&quot;) ## Warning in predict.lm(modelo, interval = &quot;prediction&quot;): predictions on current data refer to _future_ responses ## fit lwr upr ## 1 514.1259 165.2536 862.9982 ## 2 576.8163 217.7647 935.8679 ## 3 929.0578 549.8593 1308.2563 ## 4 545.2501 165.4832 925.0169 ## 6 1146.7499 766.9831 1526.5168 ## 7 634.4669 257.0284 1011.9055 ## 8 767.3224 393.2881 1141.3567 ## 9 1147.2107 767.7924 1526.6290 ## 10 767.0563 401.0010 1133.1117 ## 11 883.4332 523.2959 1243.5705 ## 12 1168.5105 788.8965 1548.1245 p = predict.lm(modelo, newdata = data.frame(Filhos = as.factor(3), Paes = 386, Vegetais = 608, Frutas = 396, Carnes = 1501, Leite = 319, Alcoolicos = 363)) p ## 1 ## 501.1353 O valor da Ave na linha 5 é 501.135316 Ajustando o conjunto de dados: # Antes ajuste dados[5, ] ## # A tibble: 1 × 9 ## AreaTrabalho Filhos Paes Vegetais Frutas Carnes Aves Leite Alcoolicos ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EM 3 386 608 396 1501 NA 319 363 dados[5, ][&#39;Aves&#39;] = p # Depois ajuste dados[5, ] ## # A tibble: 1 × 9 ## AreaTrabalho Filhos Paes Vegetais Frutas Carnes Aves Leite Alcoolicos ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EM 3 386 608 396 1501 501. 319 363 2.6.5 Agrupamento dadosS = subset(dados, select=c(&quot;Paes&quot;, &quot;Vegetais&quot;, &quot;Frutas&quot;, &quot;Carnes&quot;, &quot;Aves&quot;, &quot;Leite&quot;, &quot;Alcoolicos&quot;)) d &lt;- dist(dadosS, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) plot(grup, cex = 0.6) groups &lt;- cutree(grup, k=3) table(groups, dados$Aves) ## ## groups 501.135316011243 526 544 567 638 759 762 893 927 1148 1149 1167 ## 1 1 1 1 1 1 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 1 1 1 1 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 1 1 1 km1 = kmeans(dadosS, 3) p1 = fviz_cluster(km1, data=dadosS, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw()) p1 "],["classification.html", "Cap. 3 Classificação 3.1 Conjunto de dados 3.2 Inibina B como marcador 3.3 Ultrassom para medir deslocamento do disco 3.4 Classificação de tipos faciais 3.5 Análise do conjunto de dados USArrests", " Cap. 3 Classificação A classificação estatística é uma ampla abordagem de aprendizado supervisionado que treina um programa para categorizar informações novas e não rotuladas com base em sua relevância para dados rotulados conhecidos. A análise de cluster visa segmentar objetos em grupos com membros semelhantes e, portanto, ajuda a descobrir a distribuição de propriedades e correlações em grandes conjuntos de dados. Os exercícios deste capítulo estão relacionados a esses dois tópicos. 3.1 Conjunto de dados Este conjunto de dados foram criados pela professora Dra.Olga Satomi Yoshida para aula de Big Data no IPT. Inibina: inibina.xlsx avaliação do deslocamento do disco da articulação temporomandibular: disco.xls Tipos faciais: tipofacial.xls Consumo alimentar médio: LABORATORIO-R.pdf (ex 5.) 3.2 Inibina B como marcador Avaliar a inibina B como marcador da reserva ovariana de pacientes submetidas à fertilização in vitro 3.2.1 Pacotes Pacotes necessários para estes exercícios: ## Carregando pacotes exigidos: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## ## Attaching package: &#39;httr&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## progress ## The following object is masked from &#39;package:plotly&#39;: ## ## config ## ## Attaching package: &#39;neuralnet&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## compute 3.2.2 Conjunto de dados #GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/inibina.xls&quot;, write_disk(tf &lt;- tempfile(fileext = &quot;.xls&quot;))) httr::GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/inibina.xls&quot;, httr::write_disk(&quot;../dados/inibina.xls&quot;, overwrite = TRUE)) ## Response [https://www.ime.usp.br/~jmsinger/MorettinSinger/inibina.xls] ## Date: 2022-11-15 23:40 ## Status: 200 ## Content-Type: application/vnd.ms-excel ## Size: 8.7 kB ## &lt;ON DISK&gt; G:\\onedrive\\obsidian\\adsantos\\Mestrado\\BD\\trabalhos\\caderno-bd\\dados\\inibina.xls inibina &lt;- read_excel(&quot;../dados/inibina.xls&quot;) str(inibina) ## tibble [32 × 4] (S3: tbl_df/tbl/data.frame) ## $ ident : num [1:32] 1 2 3 4 5 6 7 8 9 10 ... ## $ resposta: chr [1:32] &quot;positiva&quot; &quot;positiva&quot; &quot;positiva&quot; &quot;positiva&quot; ... ## $ inibpre : num [1:32] 54 159.1 98.3 85.3 127.9 ... ## $ inibpos : num [1:32] 65.9 281.1 305.4 434.4 229.3 ... nrow(inibina) ## [1] 32 sum() ## [1] 0 inibina$difinib = inibina$inibpos - inibina$inibpre inibina$resposta = as.factor(inibina$resposta) plot(inibina$difinib ~ inibina$resposta, ylim = c(0, 400)) print(inibina, n = 32) ## # A tibble: 32 × 5 ## ident resposta inibpre inibpos difinib ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 positiva 54.0 65.9 11.9 ## 2 2 positiva 159. 281. 122. ## 3 3 positiva 98.3 305. 207. ## 4 4 positiva 85.3 434. 349. ## 5 5 positiva 128. 229. 101. ## 6 6 positiva 144. 354. 210. ## 7 7 positiva 111. 254. 143. ## 8 8 positiva 47.5 199. 152. ## 9 9 positiva 123. 328. 205. ## 10 10 positiva 166. 339. 174. ## 11 11 positiva 145. 377. 232. ## 12 12 positiva 186. 1055. 869. ## 13 13 positiva 149. 354. 204. ## 14 14 positiva 33.3 100. 66.8 ## 15 15 positiva 182. 358. 177. ## 16 16 positiva 58.4 168. 110. ## 17 17 positiva 128. 228. 100. ## 18 18 positiva 153. 312. 159. ## 19 19 positiva 149. 406. 257. ## 20 20 negativa 81 201. 120. ## 21 21 negativa 24.7 45.2 20.4 ## 22 22 negativa 3.02 6.03 3.01 ## 23 23 negativa 4.27 17.8 13.5 ## 24 24 negativa 99.3 128. 28.6 ## 25 25 negativa 108. 129. 21.1 ## 26 26 negativa 7.36 21.3 13.9 ## 27 27 negativa 161. 320. 158. ## 28 28 negativa 184. 311. 127. ## 29 29 negativa 23.1 45.6 22.5 ## 30 30 negativa 111. 192. 81.0 ## 31 31 negativa 106. 131. 24.8 ## 32 32 negativa 3.98 6.46 2.48 # Hmisc::describe(inibina) summary(inibina) ## ident resposta inibpre inibpos difinib ## Min. : 1.00 negativa:13 Min. : 3.02 Min. : 6.03 Min. : 2.48 ## 1st Qu.: 8.75 positiva:19 1st Qu.: 52.40 1st Qu.: 120.97 1st Qu.: 24.22 ## Median :16.50 Median :109.44 Median : 228.89 Median :121.18 ## Mean :16.50 Mean :100.53 Mean : 240.80 Mean :140.27 ## 3rd Qu.:24.25 3rd Qu.:148.93 3rd Qu.: 330.77 3rd Qu.:183.77 ## Max. :32.00 Max. :186.38 Max. :1055.19 Max. :868.81 sd(inibina$difinib) ## [1] 159.2217 Criando um dataframe para comparar as métricas dos modelos. model_eval &lt;- data.frame(matrix(ncol = 5, nrow = 0)) colnames(model_eval) &lt;- c(&#39;Model&#39;, &#39;Algorithm&#39;, &#39;Accuracy&#39;, &#39;Sensitivity&#39;, &#39;Specificity&#39;) 3.2.3 Generalized Linear Models modLogist01 = glm(resposta ~ difinib, family = binomial, data = inibina) summary(modLogist01) ## ## Call: ## glm(formula = resposta ~ difinib, family = binomial, data = inibina) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9770 -0.5594 0.1890 0.5589 2.0631 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.310455 0.947438 -2.439 0.01474 * ## difinib 0.025965 0.008561 3.033 0.00242 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 24.758 on 30 degrees of freedom ## AIC: 28.758 ## ## Number of Fisher Scoring iterations: 6 predito = predict.glm(modLogist01, type = &quot;response&quot;) classPred = ifelse(predito&gt;0.5, &quot;positiva&quot;, &quot;negativa&quot;) classPred = as.factor(classPred) cm = confusionMatrix(classPred, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 10 2 ## positiva 3 17 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6721 ## ## Mcnemar&#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.8947 ## Specificity : 0.7692 ## Pos Pred Value : 0.8500 ## Neg Pred Value : 0.8333 ## Prevalence : 0.5938 ## Detection Rate : 0.5312 ## Detection Prevalence : 0.6250 ## Balanced Accuracy : 0.8320 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modLogist01&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) Validação cruzada leave-one-out trControl &lt;- trainControl(method = &quot;LOOCV&quot;) modLogist02 &lt;- train(resposta ~ difinib, method = &quot;glm&quot;, data = inibina, family = binomial, trControl = trControl, metric = &quot;Accuracy&quot;) summary(modLogist02) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9770 -0.5594 0.1890 0.5589 2.0631 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.310455 0.947438 -2.439 0.01474 * ## difinib 0.025965 0.008561 3.033 0.00242 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.230 on 31 degrees of freedom ## Residual deviance: 24.758 on 30 degrees of freedom ## AIC: 28.758 ## ## Number of Fisher Scoring iterations: 6 predito = predict(modLogist02, newdata = inibina) classPred = as.factor(predito) cm = confusionMatrix(classPred, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 10 2 ## positiva 3 17 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6721 ## ## Mcnemar&#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.8947 ## Specificity : 0.7692 ## Pos Pred Value : 0.8500 ## Neg Pred Value : 0.8333 ## Prevalence : 0.5938 ## Detection Rate : 0.5312 ## Detection Prevalence : 0.6250 ## Balanced Accuracy : 0.8320 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modLogist02-LOOCV&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) Treinar o modelo com o método LOOCV, neste conjunto de dados, não mudou o resultado. 3.2.4 Linear Discriminant Analysis - Fisher modFisher01 = lda(resposta ~ difinib, data = inibina, prior = c(0.5, 0.5)) predito = predict(modFisher01) classPred = predito$class cm = confusionMatrix(classPred, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 11 6 ## positiva 2 13 ## ## Accuracy : 0.75 ## 95% CI : (0.566, 0.8854) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.04978 ## ## Kappa : 0.5058 ## ## Mcnemar&#39;s Test P-Value : 0.28884 ## ## Sensitivity : 0.6842 ## Specificity : 0.8462 ## Pos Pred Value : 0.8667 ## Neg Pred Value : 0.6471 ## Prevalence : 0.5938 ## Detection Rate : 0.4062 ## Detection Prevalence : 0.4688 ## Balanced Accuracy : 0.7652 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modFisher01-prior 0.5 / 0.5&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.5 Bayes inibina$resposta ## [1] positiva positiva positiva positiva positiva positiva positiva positiva positiva ## [10] positiva positiva positiva positiva positiva positiva positiva positiva positiva ## [19] positiva negativa negativa negativa negativa negativa negativa negativa negativa ## [28] negativa negativa negativa negativa negativa ## Levels: negativa positiva modBayes01 = lda(resposta ~ difinib, data = inibina, prior = c(0.65, 0.35)) predito = predict(modBayes01) classPred = predito$class cm = confusionMatrix(classPred, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 13 13 ## positiva 0 6 ## ## Accuracy : 0.5938 ## 95% CI : (0.4064, 0.763) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.5755484 ## ## Kappa : 0.2727 ## ## Mcnemar&#39;s Test P-Value : 0.0008741 ## ## Sensitivity : 0.3158 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 0.5000 ## Prevalence : 0.5938 ## Detection Rate : 0.1875 ## Detection Prevalence : 0.1875 ## Balanced Accuracy : 0.6579 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modBayes01-prior 0.65 / 0.35&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) table(classPred) ## classPred ## negativa positiva ## 26 6 print(inibina, n = 32) ## # A tibble: 32 × 5 ## ident resposta inibpre inibpos difinib ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 positiva 54.0 65.9 11.9 ## 2 2 positiva 159. 281. 122. ## 3 3 positiva 98.3 305. 207. ## 4 4 positiva 85.3 434. 349. ## 5 5 positiva 128. 229. 101. ## 6 6 positiva 144. 354. 210. ## 7 7 positiva 111. 254. 143. ## 8 8 positiva 47.5 199. 152. ## 9 9 positiva 123. 328. 205. ## 10 10 positiva 166. 339. 174. ## 11 11 positiva 145. 377. 232. ## 12 12 positiva 186. 1055. 869. ## 13 13 positiva 149. 354. 204. ## 14 14 positiva 33.3 100. 66.8 ## 15 15 positiva 182. 358. 177. ## 16 16 positiva 58.4 168. 110. ## 17 17 positiva 128. 228. 100. ## 18 18 positiva 153. 312. 159. ## 19 19 positiva 149. 406. 257. ## 20 20 negativa 81 201. 120. ## 21 21 negativa 24.7 45.2 20.4 ## 22 22 negativa 3.02 6.03 3.01 ## 23 23 negativa 4.27 17.8 13.5 ## 24 24 negativa 99.3 128. 28.6 ## 25 25 negativa 108. 129. 21.1 ## 26 26 negativa 7.36 21.3 13.9 ## 27 27 negativa 161. 320. 158. ## 28 28 negativa 184. 311. 127. ## 29 29 negativa 23.1 45.6 22.5 ## 30 30 negativa 111. 192. 81.0 ## 31 31 negativa 106. 131. 24.8 ## 32 32 negativa 3.98 6.46 2.48 Naive Bayes modNaiveBayes01 = naiveBayes(resposta ~ difinib, data = inibina) predito = predict(modNaiveBayes01, inibina) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 11 5 ## positiva 2 14 ## ## Accuracy : 0.7812 ## 95% CI : (0.6003, 0.9072) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.02102 ## ## Kappa : 0.5625 ## ## Mcnemar&#39;s Test P-Value : 0.44969 ## ## Sensitivity : 0.7368 ## Specificity : 0.8462 ## Pos Pred Value : 0.8750 ## Neg Pred Value : 0.6875 ## Prevalence : 0.5938 ## Detection Rate : 0.4375 ## Detection Prevalence : 0.5000 ## Balanced Accuracy : 0.7915 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modNaiveBayes01&quot;, &quot;naiveBayes&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.6 Decison tree modArvDec01 = rpart(resposta ~ difinib, data = inibina) prp(modArvDec01, faclen=0, #use full names for factor labels extra=1, #display number of observations for each terminal node roundint=F, #don&#39;t round to integers in output digits=5) predito = predict(modArvDec01, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 9 1 ## positiva 4 18 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6639 ## ## Mcnemar&#39;s Test P-Value : 0.371093 ## ## Sensitivity : 0.9474 ## Specificity : 0.6923 ## Pos Pred Value : 0.8182 ## Neg Pred Value : 0.9000 ## Prevalence : 0.5938 ## Detection Rate : 0.5625 ## Detection Prevalence : 0.6875 ## Balanced Accuracy : 0.8198 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modArvDec01&quot;, &quot;rpart&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) x = 1:32 plot(inibina$difinib ~x, col = inibina$resposta) 3.2.7 SVM modSVM01 = svm(resposta ~ difinib, data = inibina, kernel = &quot;linear&quot;) predito = predict(modSVM01, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modSVM01&quot;, &quot;svm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.8 neuralnet modRedNeural01 = neuralnet(resposta ~ difinib, data = inibina, hidden = c(2,4,3)) plot(modRedNeural01) ypred = neuralnet::compute(modRedNeural01, inibina) yhat = ypred$net.result round(yhat) ## [,1] [,2] ## [1,] 1 0 ## [2,] 1 0 ## [3,] 0 1 ## [4,] 0 1 ## [5,] 0 1 ## [6,] 0 1 ## [7,] 0 1 ## [8,] 0 1 ## [9,] 0 1 ## [10,] 0 1 ## [11,] 0 1 ## [12,] 0 1 ## [13,] 0 1 ## [14,] 1 0 ## [15,] 0 1 ## [16,] 0 1 ## [17,] 0 1 ## [18,] 0 1 ## [19,] 0 1 ## [20,] 1 0 ## [21,] 1 0 ## [22,] 1 0 ## [23,] 1 0 ## [24,] 1 0 ## [25,] 1 0 ## [26,] 1 0 ## [27,] 0 1 ## [28,] 1 0 ## [29,] 1 0 ## [30,] 1 0 ## [31,] 1 0 ## [32,] 1 0 yhat=data.frame(&quot;yhat&quot;=ifelse(max.col(yhat[ ,1:2])==1, &quot;negativa&quot;, &quot;positiva&quot;)) cm = confusionMatrix(inibina$resposta, as.factor(yhat$yhat)) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 12 1 ## positiva 3 16 ## ## Accuracy : 0.875 ## 95% CI : (0.7101, 0.9649) ## No Information Rate : 0.5312 ## P-Value [Acc &gt; NIR] : 4.151e-05 ## ## Kappa : 0.747 ## ## Mcnemar&#39;s Test P-Value : 0.6171 ## ## Sensitivity : 0.8000 ## Specificity : 0.9412 ## Pos Pred Value : 0.9231 ## Neg Pred Value : 0.8421 ## Prevalence : 0.4688 ## Detection Rate : 0.3750 ## Detection Prevalence : 0.4062 ## Balanced Accuracy : 0.8706 ## ## &#39;Positive&#39; Class : negativa ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modRedNeural01&quot;, &quot;neuralnet&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.9 KNN modKnn1_01 = knn3(resposta ~ difinib, data = inibina, k = 1) predito = predict(modKnn1_01, inibina, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 13 0 ## positiva 0 19 ## ## Accuracy : 1 ## 95% CI : (0.8911, 1) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 5.693e-08 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.5938 ## Detection Rate : 0.5938 ## Detection Prevalence : 0.5938 ## Balanced Accuracy : 1.0000 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn1_01-k=1&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) k = 3 modKnn3_01 = knn3(resposta ~ difinib, data = inibina, k = 3) predito = predict(modKnn3_01, inibina, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=3&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) k = 5 modKnn5_01 = knn3(resposta ~ difinib, data = inibina, k = 5) predito = predict(modKnn5_01, inibina, type = &quot;class&quot;) cm = confusionMatrix(predito, inibina$resposta, positive = &quot;positiva&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction negativa positiva ## negativa 9 1 ## positiva 4 18 ## ## Accuracy : 0.8438 ## 95% CI : (0.6721, 0.9472) ## No Information Rate : 0.5938 ## P-Value [Acc &gt; NIR] : 0.002273 ## ## Kappa : 0.6639 ## ## Mcnemar&#39;s Test P-Value : 0.371093 ## ## Sensitivity : 0.9474 ## Specificity : 0.6923 ## Pos Pred Value : 0.8182 ## Neg Pred Value : 0.9000 ## Prevalence : 0.5938 ## Detection Rate : 0.5625 ## Detection Prevalence : 0.6875 ## Balanced Accuracy : 0.8198 ## ## &#39;Positive&#39; Class : positiva ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn5_01-k=5&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.2.10 Avaliando os modelos Model Algorithm Accuracy Sensitivity Specificity modLogist01 glm 0.84375 0.894736842105263 0.769230769230769 modLogist02-LOOCV glm 0.84375 0.894736842105263 0.769230769230769 modFisher01-prior 0.5 / 0.5 lda 0.75 0.684210526315789 0.846153846153846 modBayes01-prior 0.65 / 0.35 lda 0.59375 0.315789473684211 1 modNaiveBayes01 naiveBayes 0.78125 0.736842105263158 0.846153846153846 modArvDec01 rpart 0.84375 0.947368421052632 0.692307692307692 modSVM01 svm 0.84375 0.894736842105263 0.769230769230769 modRedNeural01 neuralnet 0.875 0.8 0.941176470588235 modKnn1_01-k=1 knn3 1 1 1 modKnn3_01-k=3 knn3 0.875 0.894736842105263 0.846153846153846 modKnn5_01-k=5 knn3 0.84375 0.947368421052632 0.692307692307692 Os modelos modLogist01, modFisher01 com \\(prior = 0.5, 0.5\\), modArvDec01 e modKnn3_01 com \\(k=3\\) tiveram performance muito parecidas no conjunto de dados. Uma possível escolha seria o modKnn3_01 que combinado obteve melhor acurácia, sensibilidade e especificidade. 3.2.11 Agrupamento inibinaS = inibina[, 3:5] d &lt;- dist(inibinaS, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) plot(grup, cex = 0.6) groups &lt;- cutree(grup, k=3) table(groups, inibina$resposta) ## ## groups negativa positiva ## 1 11 7 ## 2 2 11 ## 3 0 1 km1 = kmeans(inibinaS, 4) p1 = fviz_cluster(km1, data=inibinaS, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw()) p1 groups = km1$cluster table(groups, inibina$resposta) ## ## groups negativa positiva ## 1 2 10 ## 2 2 6 ## 3 9 2 ## 4 0 1 3.3 Ultrassom para medir deslocamento do disco Os dados disponíveis aqui foram extraídos de um estudo realizado no Hospital Universitário da Universidade de São Paulo com o objetivo de avaliar se algumas medidas obtidas ultrassonograficamente poderiam ser utilizadas como substitutas de medidas obtidas por métodos de ressonância magnética, considerada como padrão ouro para avaliação do deslocamento do disco da articulação temporomandibular (referido simplesmente como disco). 3.3.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) library(rpart) library(rpart.plot) library(caret) library(MASS) library(httr) library(readxl) library(tibble) library(e1071) library(neuralnet) library(factoextra) library(ggpubr) 3.3.2 Conjunto de dados #GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/disco.xls&quot;, write_disk(tf &lt;- tempfile(fileext = &quot;.xls&quot;))) httr::GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/disco.xls&quot;, httr::write_disk(&quot;../dados/disco.xls&quot;, overwrite = TRUE)) ## Response [https://www.ime.usp.br/~jmsinger/MorettinSinger/disco.xls] ## Date: 2022-11-15 23:40 ## Status: 200 ## Content-Type: application/vnd.ms-excel ## Size: 11.3 kB ## &lt;ON DISK&gt; G:\\onedrive\\obsidian\\adsantos\\Mestrado\\BD\\trabalhos\\caderno-bd\\dados\\disco.xls disco &lt;- read_excel(&quot;../dados/disco.xls&quot;) str(disco) ## tibble [104 × 3] (S3: tbl_df/tbl/data.frame) ## $ deslocamento: num [1:104] 0 0 0 1 0 1 0 0 0 0 ... ## $ distanciaA : num [1:104] 2.2 2.4 2.6 3.5 1.3 2.8 1.5 2.6 1.2 1.7 ... ## $ distanciaF : num [1:104] 1.4 1.2 2 1.8 1 1.1 1.2 1.1 0.6 1.5 ... Número de observações 104. 3.3.2.1 Categorizando a variável de deslocamento. disco$sumdistancia = disco$distanciaA + disco$distanciaF disco$deslocamento = as.factor(disco$deslocamento) plot(disco$sumdistancia ~ disco$deslocamento) print(disco, n = 32) ## # A tibble: 104 × 4 ## deslocamento distanciaA distanciaF sumdistancia ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 2.2 1.4 3.6 ## 2 0 2.4 1.2 3.6 ## 3 0 2.6 2 4.6 ## 4 1 3.5 1.8 5.3 ## 5 0 1.3 1 2.3 ## 6 1 2.8 1.1 3.9 ## 7 0 1.5 1.2 2.7 ## 8 0 2.6 1.1 3.7 ## 9 0 1.2 0.6 1.8 ## 10 0 1.7 1.5 3.2 ## 11 0 1.3 1.2 2.5 ## 12 0 1.2 1 2.2 ## 13 1 4 2.5 6.5 ## 14 0 1.2 1 2.2 ## 15 1 3.1 1.7 4.8 ## 16 1 2.6 0.6 3.2 ## 17 0 1.8 0.8 2.6 ## 18 0 1.2 1 2.2 ## 19 0 1.9 1 2.9 ## 20 0 1.2 0.9 2.1 ## 21 1 1.7 0.9 2.6 ## 22 0 1.2 0.8 2 ## 23 1 3.9 3.2 7.1 ## 24 0 1.7 1.1 2.8 ## 25 0 1.4 1 2.4 ## 26 0 1.6 1.3 2.9 ## 27 0 1.3 0.5 1.8 ## 28 0 1.7 0.7 2.4 ## 29 1 2.6 1.8 4.4 ## 30 0 1.5 1.5 3 ## 31 0 1.8 1.4 3.2 ## 32 0 1.2 0.9 2.1 ## # … with 72 more rows summary(disco) ## deslocamento distanciaA distanciaF sumdistancia ## 0:75 Min. :0.500 Min. :0.400 Min. :1.100 ## 1:29 1st Qu.:1.400 1st Qu.:1.000 1st Qu.:2.500 ## Median :1.700 Median :1.200 Median :2.900 ## Mean :1.907 Mean :1.362 Mean :3.268 ## 3rd Qu.:2.300 3rd Qu.:1.600 3rd Qu.:3.700 ## Max. :4.900 Max. :3.300 Max. :7.100 sd(disco$sumdistancia) ## [1] 1.183976 Criando um dataframe para comparar as métricas dos modelos. model_eval &lt;- data.frame(matrix(ncol = 5, nrow = 0)) colnames(model_eval) &lt;- c(&#39;Model&#39;, &#39;Algorithm&#39;, &#39;Accuracy&#39;, &#39;Sensitivity&#39;, &#39;Specificity&#39;) 3.3.3 Generalized Linear Models modLogist01 = glm(deslocamento ~ sumdistancia, family = binomial, data = disco) summary(modLogist01) ## ## Call: ## glm(formula = deslocamento ~ sumdistancia, family = binomial, ## data = disco) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2291 -0.4957 -0.3182 0.1560 2.3070 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.3902 1.3740 -5.379 7.51e-08 *** ## sumdistancia 1.8467 0.3799 4.861 1.17e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.107 on 103 degrees of freedom ## Residual deviance: 72.567 on 102 degrees of freedom ## AIC: 76.567 ## ## Number of Fisher Scoring iterations: 5 predito = predict.glm(modLogist01, type = &quot;response&quot;) classPred = ifelse(predito&gt;0.5, &quot;0&quot;, &quot;1&quot;) classPred = as.factor(classPred) cm = confusionMatrix(classPred, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 5 16 ## 1 70 13 ## ## Accuracy : 0.1731 ## 95% CI : (0.1059, 0.2597) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : -0.3088 ## ## Mcnemar&#39;s Test P-Value : 1.096e-08 ## ## Sensitivity : 0.06667 ## Specificity : 0.44828 ## Pos Pred Value : 0.23810 ## Neg Pred Value : 0.15663 ## Prevalence : 0.72115 ## Detection Rate : 0.04808 ## Detection Prevalence : 0.20192 ## Balanced Accuracy : 0.25747 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modLogist01&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) Validação cruzada leave-one-out trControl &lt;- trainControl(method = &quot;LOOCV&quot;) modLogist02 &lt;- train(deslocamento ~ sumdistancia, method = &quot;glm&quot;, data = disco, family = binomial, trControl = trControl, metric = &quot;Accuracy&quot;) summary(modLogist02) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2291 -0.4957 -0.3182 0.1560 2.3070 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.3902 1.3740 -5.379 7.51e-08 *** ## sumdistancia 1.8467 0.3799 4.861 1.17e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.107 on 103 degrees of freedom ## Residual deviance: 72.567 on 102 degrees of freedom ## AIC: 76.567 ## ## Number of Fisher Scoring iterations: 5 predito = predict(modLogist02, newdata = disco) classPred = as.factor(predito) cm = confusionMatrix(classPred, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 70 13 ## 1 5 16 ## ## Accuracy : 0.8269 ## 95% CI : (0.7403, 0.8941) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.00857 ## ## Kappa : 0.5299 ## ## Mcnemar&#39;s Test P-Value : 0.09896 ## ## Sensitivity : 0.9333 ## Specificity : 0.5517 ## Pos Pred Value : 0.8434 ## Neg Pred Value : 0.7619 ## Prevalence : 0.7212 ## Detection Rate : 0.6731 ## Detection Prevalence : 0.7981 ## Balanced Accuracy : 0.7425 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modLogist02-LOOCV&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) kable(model_eval) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Model Algorithm Accuracy Sensitivity Specificity modLogist01 glm 0.173076923076923 0.0666666666666667 0.448275862068966 modLogist02-LOOCV glm 0.826923076923077 0.933333333333333 0.551724137931034 Treinar o modelo com o método LOOCV melhorou consideravelmente todas as métricas do modelo. 3.3.4 Linear Discriminant Analysis - Fisher modFisher01 = lda(deslocamento ~ sumdistancia, data = disco, prior = c(0.5, 0.5)) predito = predict(modFisher01) classPred = predito$class cm = confusionMatrix(classPred, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 67 6 ## 1 8 23 ## ## Accuracy : 0.8654 ## 95% CI : (0.7845, 0.9244) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.0003676 ## ## Kappa : 0.6722 ## ## Mcnemar&#39;s Test P-Value : 0.7892680 ## ## Sensitivity : 0.8933 ## Specificity : 0.7931 ## Pos Pred Value : 0.9178 ## Neg Pred Value : 0.7419 ## Prevalence : 0.7212 ## Detection Rate : 0.6442 ## Detection Prevalence : 0.7019 ## Balanced Accuracy : 0.8432 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modFisher01-prior 0.5 / 0.5&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.5 Bayes modBayes01 = lda(deslocamento ~ sumdistancia, data = disco, prior = c(0.65, 0.35)) predito = predict(modBayes01) classPred = predito$class cm = confusionMatrix(classPred, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 70 12 ## 1 5 17 ## ## Accuracy : 0.8365 ## 95% CI : (0.7512, 0.9018) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.004313 ## ## Kappa : 0.5611 ## ## Mcnemar&#39;s Test P-Value : 0.145610 ## ## Sensitivity : 0.9333 ## Specificity : 0.5862 ## Pos Pred Value : 0.8537 ## Neg Pred Value : 0.7727 ## Prevalence : 0.7212 ## Detection Rate : 0.6731 ## Detection Prevalence : 0.7885 ## Balanced Accuracy : 0.7598 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modBayes01-prior 0.65 / 0.35&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) table(classPred) ## classPred ## 0 1 ## 82 22 print(disco, n = 32) ## # A tibble: 104 × 4 ## deslocamento distanciaA distanciaF sumdistancia ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 2.2 1.4 3.6 ## 2 0 2.4 1.2 3.6 ## 3 0 2.6 2 4.6 ## 4 1 3.5 1.8 5.3 ## 5 0 1.3 1 2.3 ## 6 1 2.8 1.1 3.9 ## 7 0 1.5 1.2 2.7 ## 8 0 2.6 1.1 3.7 ## 9 0 1.2 0.6 1.8 ## 10 0 1.7 1.5 3.2 ## 11 0 1.3 1.2 2.5 ## 12 0 1.2 1 2.2 ## 13 1 4 2.5 6.5 ## 14 0 1.2 1 2.2 ## 15 1 3.1 1.7 4.8 ## 16 1 2.6 0.6 3.2 ## 17 0 1.8 0.8 2.6 ## 18 0 1.2 1 2.2 ## 19 0 1.9 1 2.9 ## 20 0 1.2 0.9 2.1 ## 21 1 1.7 0.9 2.6 ## 22 0 1.2 0.8 2 ## 23 1 3.9 3.2 7.1 ## 24 0 1.7 1.1 2.8 ## 25 0 1.4 1 2.4 ## 26 0 1.6 1.3 2.9 ## 27 0 1.3 0.5 1.8 ## 28 0 1.7 0.7 2.4 ## 29 1 2.6 1.8 4.4 ## 30 0 1.5 1.5 3 ## 31 0 1.8 1.4 3.2 ## 32 0 1.2 0.9 2.1 ## # … with 72 more rows Naive Bayes modNaiveBayes01 = naiveBayes(deslocamento ~ sumdistancia, data = disco) predito = predict(modNaiveBayes01, disco) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 70 13 ## 1 5 16 ## ## Accuracy : 0.8269 ## 95% CI : (0.7403, 0.8941) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.00857 ## ## Kappa : 0.5299 ## ## Mcnemar&#39;s Test P-Value : 0.09896 ## ## Sensitivity : 0.9333 ## Specificity : 0.5517 ## Pos Pred Value : 0.8434 ## Neg Pred Value : 0.7619 ## Prevalence : 0.7212 ## Detection Rate : 0.6731 ## Detection Prevalence : 0.7981 ## Balanced Accuracy : 0.7425 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modNaiveBayes01&quot;, &quot;naiveBayes&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.6 Decison tree modArvDec01 = rpart(deslocamento ~ sumdistancia, data = disco) prp(modArvDec01, faclen=0, #use full names for factor labels extra=1, #display number of observations for each terminal node roundint=F, #don&#39;t round to integers in output digits=5) predito = predict(modArvDec01, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 67 6 ## 1 8 23 ## ## Accuracy : 0.8654 ## 95% CI : (0.7845, 0.9244) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.0003676 ## ## Kappa : 0.6722 ## ## Mcnemar&#39;s Test P-Value : 0.7892680 ## ## Sensitivity : 0.8933 ## Specificity : 0.7931 ## Pos Pred Value : 0.9178 ## Neg Pred Value : 0.7419 ## Prevalence : 0.7212 ## Detection Rate : 0.6442 ## Detection Prevalence : 0.7019 ## Balanced Accuracy : 0.8432 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modArvDec01&quot;, &quot;rpart&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) x = 1:nrow(disco) plot(disco$sumdistancia ~ x, col = disco$deslocamento) 3.3.7 SVM modSVM01 = svm(deslocamento ~ sumdistancia, data = disco, kernel = &quot;linear&quot;) predito = predict(modSVM01, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modSVM01&quot;, &quot;svm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.8 neuralnet modRedNeural01 = neuralnet(deslocamento ~ sumdistancia, data = disco, hidden = c(2,4,3)) plot(modRedNeural01) ypred = neuralnet::compute(modRedNeural01, disco) yhat = ypred$net.result round(yhat) ## [,1] [,2] ## [1,] 1 0 ## [2,] 1 0 ## [3,] 0 1 ## [4,] 0 1 ## [5,] 1 0 ## [6,] 1 0 ## [7,] 1 0 ## [8,] 1 0 ## [9,] 1 0 ## [10,] 1 0 ## [11,] 1 0 ## [12,] 1 0 ## [13,] 0 1 ## [14,] 1 0 ## [15,] 0 1 ## [16,] 1 0 ## [17,] 1 0 ## [18,] 1 0 ## [19,] 1 0 ## [20,] 1 0 ## [21,] 1 0 ## [22,] 1 0 ## [23,] 0 1 ## [24,] 1 0 ## [25,] 1 0 ## [26,] 1 0 ## [27,] 1 0 ## [28,] 1 0 ## [29,] 0 1 ## [30,] 1 0 ## [31,] 1 0 ## [32,] 1 0 ## [33,] 1 0 ## [34,] 1 0 ## [35,] 1 0 ## [36,] 1 0 ## [37,] 1 0 ## [38,] 0 1 ## [39,] 1 0 ## [40,] 1 0 ## [41,] 1 0 ## [42,] 1 0 ## [43,] 1 0 ## [44,] 1 0 ## [45,] 1 0 ## [46,] 1 0 ## [47,] 1 0 ## [48,] 1 0 ## [49,] 1 0 ## [50,] 1 0 ## [51,] 1 0 ## [52,] 1 0 ## [53,] 1 0 ## [54,] 1 0 ## [55,] 1 0 ## [56,] 1 0 ## [57,] 1 0 ## [58,] 1 0 ## [59,] 1 0 ## [60,] 0 1 ## [61,] 0 1 ## [62,] 0 1 ## [63,] 0 1 ## [64,] 0 1 ## [65,] 1 0 ## [66,] 1 0 ## [67,] 1 0 ## [68,] 1 0 ## [69,] 1 0 ## [70,] 1 0 ## [71,] 0 1 ## [72,] 1 0 ## [73,] 1 0 ## [74,] 1 0 ## [75,] 1 0 ## [76,] 1 0 ## [77,] 1 0 ## [78,] 1 0 ## [79,] 0 1 ## [80,] 1 0 ## [81,] 1 0 ## [82,] 1 0 ## [83,] 1 0 ## [84,] 1 0 ## [85,] 0 1 ## [86,] 1 0 ## [87,] 0 1 ## [88,] 1 0 ## [89,] 1 0 ## [90,] 1 0 ## [91,] 0 1 ## [92,] 0 1 ## [93,] 1 0 ## [94,] 1 0 ## [95,] 1 0 ## [96,] 0 1 ## [97,] 0 1 ## [98,] 0 1 ## [99,] 1 0 ## [100,] 1 0 ## [101,] 1 0 ## [102,] 1 0 ## [103,] 1 0 ## [104,] 0 1 yhat=data.frame(&quot;yhat&quot;=ifelse(max.col(yhat[ ,1:2])==1, &quot;0&quot;, &quot;1&quot;)) cm = confusionMatrix(disco$deslocamento, as.factor(yhat$yhat)) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 70 5 ## 1 12 17 ## ## Accuracy : 0.8365 ## 95% CI : (0.7512, 0.9018) ## No Information Rate : 0.7885 ## P-Value [Acc &gt; NIR] : 0.1388 ## ## Kappa : 0.5611 ## ## Mcnemar&#39;s Test P-Value : 0.1456 ## ## Sensitivity : 0.8537 ## Specificity : 0.7727 ## Pos Pred Value : 0.9333 ## Neg Pred Value : 0.5862 ## Prevalence : 0.7885 ## Detection Rate : 0.6731 ## Detection Prevalence : 0.7212 ## Balanced Accuracy : 0.8132 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modRedNeural01&quot;, &quot;neuralnet&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.9 KNN modKnn1_01 = knn3(deslocamento ~ sumdistancia, data = disco, k = 1) predito = predict(modKnn1_01, disco, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 72 11 ## 1 3 18 ## ## Accuracy : 0.8654 ## 95% CI : (0.7845, 0.9244) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 0.0003676 ## ## Kappa : 0.6344 ## ## Mcnemar&#39;s Test P-Value : 0.0613688 ## ## Sensitivity : 0.9600 ## Specificity : 0.6207 ## Pos Pred Value : 0.8675 ## Neg Pred Value : 0.8571 ## Prevalence : 0.7212 ## Detection Rate : 0.6923 ## Detection Prevalence : 0.7981 ## Balanced Accuracy : 0.7903 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn1_01-k=1&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) k = 3 modKnn3_01 = knn3(deslocamento ~ sumdistancia, data = disco, k = 3) predito = predict(modKnn3_01, disco, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=3&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) k = 5 modKnn5_01 = knn3(deslocamento ~ sumdistancia, data = disco, k = 5) predito = predict(modKnn5_01, disco, type = &quot;class&quot;) cm = confusionMatrix(predito, disco$deslocamento, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 72 9 ## 1 3 20 ## ## Accuracy : 0.8846 ## 95% CI : (0.8071, 0.9389) ## No Information Rate : 0.7212 ## P-Value [Acc &gt; NIR] : 4.877e-05 ## ## Kappa : 0.6937 ## ## Mcnemar&#39;s Test P-Value : 0.1489 ## ## Sensitivity : 0.9600 ## Specificity : 0.6897 ## Pos Pred Value : 0.8889 ## Neg Pred Value : 0.8696 ## Prevalence : 0.7212 ## Detection Rate : 0.6923 ## Detection Prevalence : 0.7788 ## Balanced Accuracy : 0.8248 ## ## &#39;Positive&#39; Class : 0 ## # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn5_01-k=5&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[&#39;Sensitivity&#39;], cm$byClass[&#39;Specificity&#39;]) 3.3.10 Avaliando os modelos Model Algorithm Accuracy Sensitivity Specificity modLogist01 glm 0.173076923076923 0.0666666666666667 0.448275862068966 modLogist02-LOOCV glm 0.826923076923077 0.933333333333333 0.551724137931034 modFisher01-prior 0.5 / 0.5 lda 0.865384615384615 0.893333333333333 0.793103448275862 modBayes01-prior 0.65 / 0.35 lda 0.836538461538462 0.933333333333333 0.586206896551724 modNaiveBayes01 naiveBayes 0.826923076923077 0.933333333333333 0.551724137931034 modArvDec01 rpart 0.865384615384615 0.893333333333333 0.793103448275862 modSVM01 svm 0.836538461538462 0.946666666666667 0.551724137931034 modRedNeural01 neuralnet 0.836538461538462 0.853658536585366 0.772727272727273 modKnn1_01-k=1 knn3 0.865384615384615 0.96 0.620689655172414 modKnn3_01-k=3 knn3 0.865384615384615 0.933333333333333 0.689655172413793 modKnn5_01-k=5 knn3 0.884615384615385 0.96 0.689655172413793 Todos os modelos tiveram performance semelhante, com destaque para modFisher01 com \\(prior 0.5, 0.5\\). 3.3.11 Agrupamento discoS = disco[, 2:3] d &lt;- dist(discoS, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) plot(grup, cex = 0.6) groups &lt;- cutree(grup, k=3) table(groups, disco$deslocamento) ## ## groups 0 1 ## 1 10 21 ## 2 58 5 ## 3 7 3 km1 = kmeans(discoS, 4) p1 = fviz_cluster(km1, data=discoS, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw()) p1 groups = km1$cluster table(groups, disco$deslocamento) ## ## groups 0 1 ## 1 0 8 ## 2 7 8 ## 3 54 3 ## 4 14 10 3.4 Classificação de tipos faciais Os dados do arquivo tipofacial (disponível aqui) foram extraídos de um estudo odontológico realizado pelo Dr. Flávio Cotrim Vellini. Um dos objetivos era utilizar medidas entre diferentes pontos do crânio para caracterizar indivíduos com diferentes tipos faciais, a saber, braquicéfalos, mesocéfalos e dolicocéfalos (grupos). O conjunto de dados contém observações de 11 variáveis em 101 pacientes. Para efeitos didáticos, considere apenas a altura facial (altfac) e a profundidade facial (proffac) como variáveis preditoras. 3.4.1 Pacotes Pacotes necessários para estes exercícios: 3.4.2 Conjunto de dados httr::GET(&quot;http://www.ime.usp.br/~jmsinger/MorettinSinger/tipofacial.xls&quot;, httr::write_disk(&quot;../dados/tipofacial.xls&quot;, overwrite = TRUE)) ## Response [https://www.ime.usp.br/~jmsinger/MorettinSinger/tipofacial.xls] ## Date: 2022-11-15 23:40 ## Status: 200 ## Content-Type: application/vnd.ms-excel ## Size: 20.5 kB ## &lt;ON DISK&gt; G:\\onedrive\\obsidian\\adsantos\\Mestrado\\BD\\trabalhos\\caderno-bd\\dados\\tipofacial.xls tipofacial &lt;- read_excel(&quot;../dados/tipofacial.xls&quot;) str(tipofacial) ## tibble [101 × 13] (S3: tbl_df/tbl/data.frame) ## $ paciente: num [1:101] 10 10 27 39 39 39 55 76 77 133 ... ## $ sexo : chr [1:101] &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; ... ## $ grupo : chr [1:101] &quot;braq&quot; &quot;braq&quot; &quot;braq&quot; &quot;braq&quot; ... ## $ idade : num [1:101] 5.58 11.42 16.17 4.92 10.92 ... ## $ nsba : num [1:101] 132 134 122 130 130 ... ## $ ns : num [1:101] 58 63 77.5 64 70 68.5 71 72 70 80 ... ## $ sba : num [1:101] 36 42.5 48 34.5 36.5 41.5 42 46.5 44 52 ... ## $ altfac : num [1:101] 1.2 1.2 2.6 3.1 3.1 3.3 2.4 1.9 2.1 2.8 ... ## $ proffac : num [1:101] 0.8 0.4 0.2 -1 0.6 -0.6 0.3 0.5 -0.1 0.2 ... ## $ eixofac : num [1:101] 0.4 1 0.3 1.9 1.2 1.1 1.1 1.4 2.2 0.4 ... ## $ planmand: num [1:101] 0.4 1 0.9 1.3 2.2 1.2 1.2 0.6 0.8 1.1 ... ## $ arcomand: num [1:101] 2.5 3.6 3.4 1.6 2.3 2.1 3.5 3.5 0.7 1.8 ... ## $ vert : num [1:101] 1.06 1.44 1.48 1.38 1.88 1.42 1.7 1.58 1.14 1.26 ... Número de observações 101. 3.4.2.1 Categorizando a variável de grupo. # Considerar (altfac) e a profundidade facial (proffac) como variáveis preditoras. tipofacial$sumfac = tipofacial$altfac + tipofacial$proffac tipofacial$grupo = as.factor(tipofacial$grupo) plot(tipofacial$sumfac ~ tipofacial$grupo) print(tipofacial, n = 32) ## # A tibble: 101 × 14 ## pacie…¹ sexo grupo idade nsba ns sba altfac proffac eixofac planm…² arcom…³ ## &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 M braq 5.58 132 58 36 1.2 0.8 0.4 0.4 2.5 ## 2 10 M braq 11.4 134 63 42.5 1.2 0.4 1 1 3.6 ## 3 27 F braq 16.2 122. 77.5 48 2.6 0.2 0.3 0.9 3.4 ## 4 39 F braq 4.92 130. 64 34.5 3.1 -1 1.9 1.3 1.6 ## 5 39 F braq 10.9 130. 70 36.5 3.1 0.6 1.2 2.2 2.3 ## 6 39 F braq 12.9 128 68.5 41.5 3.3 -0.6 1.1 1.2 2.1 ## 7 55 F braq 16.8 130 71 42 2.4 0.3 1.1 1.2 3.5 ## 8 76 F braq 16 125 72 46.5 1.9 0.5 1.4 0.6 3.5 ## 9 77 F braq 17.1 130. 70 44 2.1 -0.1 2.2 0.8 0.7 ## 10 133 M braq 14.8 130 80 52 2.8 0.2 0.4 1.1 1.8 ## 11 145 F braq 10.7 118. 67 43.5 1.2 0.7 1.7 1.5 0.9 ## 12 148 F braq 11.3 129 68 41.5 2.9 -0.7 2.2 1.3 2.2 ## 13 165 F braq 15 137 74 43 2.1 -0.3 1.3 1.2 2.5 ## 14 176 M braq 5.67 134 65 39 2.5 0.2 2.1 2.2 3.1 ## 15 176 M braq 13 130 72 48 2.2 0.2 2.9 2.4 4 ## 16 208 M braq 13.8 130 67.5 43 0.9 1.5 -0.4 1.2 2.3 ## 17 246 F braq 11.8 129 67 43 0.8 1.1 1.5 0.3 1.4 ## 18 27 F braq 4.67 126 63 35 1.9 0.2 0.3 0.1 1.6 ## 19 39 F braq 3.67 131 61 32 3 -1.4 1.8 0.7 0.8 ## 20 45 M braq 22.7 133 77 46 1.7 -0.9 -0.6 0.3 2.7 ## 21 49 F braq 16.6 136. 71 42 1 0.5 0.1 0.2 2.7 ## 22 52 M braq 12.6 126. 69 41.5 1.4 0 0.4 0.5 2.5 ## 23 63 F braq 6.17 130. 63 37 1.9 -0.5 0.8 0.5 1.3 ## 24 63 F braq 15.7 132 72.5 43 1.9 0 2 0.4 0.3 ## 25 68 M braq 13.8 128 72 43 1.2 -0.6 1 1.1 2.3 ## 26 76 F braq 5.33 124 61 39.5 1.3 -0.3 -0.1 0.6 1.6 ## 27 86 F braq 5.25 126. 61.5 42.5 1.1 0 -0.1 0.2 2.7 ## 28 109 M braq 14.9 131 78.5 50 1.8 -0.4 1.1 0.8 1.3 ## 29 145 F braq 3.67 129 60 37.5 1 0 1.1 0.1 0.4 ## 30 148 F braq 4.25 127 61.5 34 1.9 0.2 1.6 0 1.4 ## 31 155 F braq 5.25 127 65 39.5 1.2 -0.5 1.2 0.2 1.4 ## 32 155 F braq 11.3 128 71.5 45 2.3 -0.3 0.7 0.4 2.2 ## # … with 69 more rows, 2 more variables: vert &lt;dbl&gt;, sumfac &lt;dbl&gt;, and abbreviated ## # variable names ¹​paciente, ²​planmand, ³​arcomand summary(tipofacial) ## paciente sexo grupo idade nsba ## Min. : 10.0 Length:101 braq :33 Min. : 3.670 Min. :117.0 ## 1st Qu.: 61.0 Class :character dolico:31 1st Qu.: 5.250 1st Qu.:125.0 ## Median :105.0 Mode :character meso :37 Median :10.000 Median :128.0 ## Mean :108.4 Mean : 9.204 Mean :127.7 ## 3rd Qu.:148.0 3rd Qu.:12.580 3rd Qu.:130.0 ## Max. :246.0 Max. :22.670 Max. :137.0 ## ns sba altfac proffac ## Min. :57.00 Min. :32.0 Min. :-2.3000 Min. :-2.7000 ## 1st Qu.:63.00 1st Qu.:38.5 1st Qu.:-0.3000 1st Qu.:-1.2000 ## Median :66.50 Median :41.5 Median : 0.6000 Median :-0.7000 ## Mean :67.03 Mean :41.5 Mean : 0.6238 Mean :-0.6119 ## 3rd Qu.:71.00 3rd Qu.:44.0 3rd Qu.: 1.7000 3rd Qu.: 0.0000 ## Max. :81.00 Max. :54.0 Max. : 3.3000 Max. : 1.5000 ## eixofac planmand arcomand vert ## Min. :-3.5000 Min. :-2.9000 Min. :-2.3000 Min. :-1.92000 ## 1st Qu.:-1.0000 1st Qu.:-1.3000 1st Qu.: 0.0000 1st Qu.:-0.68000 ## Median :-0.2000 Median :-0.5000 Median : 0.8000 Median : 0.00000 ## Mean :-0.1129 Mean :-0.4693 Mean : 0.8386 Mean : 0.04931 ## 3rd Qu.: 0.8000 3rd Qu.: 0.4000 3rd Qu.: 1.6000 3rd Qu.: 0.80000 ## Max. : 2.9000 Max. : 2.4000 Max. : 4.0000 Max. : 2.34000 ## sumfac ## Min. :-4.00000 ## 1st Qu.:-0.90000 ## Median :-0.10000 ## Mean : 0.01188 ## 3rd Qu.: 1.40000 ## Max. : 3.70000 sd(tipofacial$sumfac) ## [1] 1.750331 Criando um dataframe para comparar as métricas dos modelos. model_eval &lt;- data.frame(matrix(ncol = 9, nrow = 0)) colnames(model_eval) &lt;- c(&#39;Model&#39;, &#39;Algorithm&#39;, &#39;Accuracy&#39;, &#39;Sensitivity_C1&#39;, &#39;Sensitivity_C2&#39;, &#39;Sensitivity_C3&#39;, &#39;Specificity_C1&#39;, &#39;Specificity_C2&#39;, &#39;Specificity_C3&#39;) 3.4.3 Generalized Linear Models glm.fit = multinom(grupo ~ sumfac, data=tipofacial) ## # weights: 9 (4 variable) ## initial value 110.959841 ## iter 10 value 38.508219 ## final value 38.456349 ## converged summary(glm.fit) ## Call: ## multinom(formula = grupo ~ sumfac, data = tipofacial) ## ## Coefficients: ## (Intercept) sumfac ## dolico 1.532730 -6.173138 ## meso 3.781329 -3.762457 ## ## Std. Errors: ## (Intercept) sumfac ## dolico 1.238970 1.174600 ## meso 1.107069 1.005516 ## ## Residual Deviance: 76.9127 ## AIC: 84.9127 predito = predict(glm.fit, newdata = tipofacial) cm = confusionMatrix(predito, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 27 0 4 ## dolico 0 23 2 ## meso 6 8 31 ## ## Overall Statistics ## ## Accuracy : 0.802 ## 95% CI : (0.7109, 0.8746) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7002 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8182 0.7419 0.8378 ## Specificity 0.9412 0.9714 0.7812 ## Pos Pred Value 0.8710 0.9200 0.6889 ## Neg Pred Value 0.9143 0.8947 0.8929 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2673 0.2277 0.3069 ## Detection Prevalence 0.3069 0.2475 0.4455 ## Balanced Accuracy 0.8797 0.8567 0.8095 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;glm.fit&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Seprando dados de treinamento (70%) e testes (30%). alpha=0.7 d = sort(sample(nrow(tipofacial), nrow(tipofacial)*alpha)) train = tipofacial[d,] test = tipofacial[-d,] glm.fit = multinom(grupo ~ sumfac, data=train) ## # weights: 9 (4 variable) ## initial value 76.902860 ## iter 10 value 22.374781 ## final value 22.241040 ## converged summary(glm.fit) ## Call: ## multinom(formula = grupo ~ sumfac, data = train) ## ## Coefficients: ## (Intercept) sumfac ## dolico 3.014243 -7.473771 ## meso 5.434374 -4.808327 ## ## Std. Errors: ## (Intercept) sumfac ## dolico 2.183394 1.929194 ## meso 2.055686 1.743646 ## ## Residual Deviance: 44.48208 ## AIC: 52.48208 predito = predict(glm.fit, newdata = test) cm = confusionMatrix(predito, test$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 7 0 2 ## dolico 0 6 1 ## meso 5 2 8 ## ## Overall Statistics ## ## Accuracy : 0.6774 ## 95% CI : (0.4863, 0.8332) ## No Information Rate : 0.3871 ## P-Value [Acc &gt; NIR] : 0.001009 ## ## Kappa : 0.5095 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.5833 0.7500 0.7273 ## Specificity 0.8947 0.9565 0.6500 ## Pos Pred Value 0.7778 0.8571 0.5333 ## Neg Pred Value 0.7727 0.9167 0.8125 ## Prevalence 0.3871 0.2581 0.3548 ## Detection Rate 0.2258 0.1935 0.2581 ## Detection Prevalence 0.2903 0.2258 0.4839 ## Balanced Accuracy 0.7390 0.8533 0.6886 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;glm.fit-split&quot;, &quot;glm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) kable(model_eval) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Model Algorithm Accuracy Sensitivity_C1 Sensitivity_C2 Sensitivity_C3 Specificity_C1 Specificity_C2 Specificity_C3 glm.fit glm 0.801980198019802 0.818181818181818 0.741935483870968 0.837837837837838 0.941176470588235 0.971428571428571 0.78125 glm.fit-split glm 0.67741935483871 0.583333333333333 0.75 0.727272727272727 0.894736842105263 0.956521739130435 0.65 Treinar o modelo com um conjunto de dados diferente do de teste mostrou que o modelo tem uma capcidade razoável de generalização. 3.4.4 Linear Discriminant Analysis - Fisher modFisher01 = lda(grupo ~ sumfac, data = tipofacial) predito = predict(modFisher01) classPred = predito$class cm = confusionMatrix(classPred, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 29 0 5 ## dolico 0 23 2 ## meso 4 8 30 ## ## Overall Statistics ## ## Accuracy : 0.8119 ## 95% CI : (0.7219, 0.8828) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7157 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8788 0.7419 0.8108 ## Specificity 0.9265 0.9714 0.8125 ## Pos Pred Value 0.8529 0.9200 0.7143 ## Neg Pred Value 0.9403 0.8947 0.8814 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2871 0.2277 0.2970 ## Detection Prevalence 0.3366 0.2475 0.4158 ## Balanced Accuracy 0.9026 0.8567 0.8117 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modFisher01&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.5 Bayes modBayes01 = lda(grupo ~ sumfac, data = tipofacial, prior = c(0.25, 0.50, 0.25)) predito = predict(modBayes01) classPred = predito$class cm = confusionMatrix(classPred, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 29 0 5 ## dolico 0 24 6 ## meso 4 7 26 ## ## Overall Statistics ## ## Accuracy : 0.7822 ## 95% CI : (0.689, 0.8582) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.6723 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8788 0.7742 0.7027 ## Specificity 0.9265 0.9143 0.8281 ## Pos Pred Value 0.8529 0.8000 0.7027 ## Neg Pred Value 0.9403 0.9014 0.8281 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2871 0.2376 0.2574 ## Detection Prevalence 0.3366 0.2970 0.3663 ## Balanced Accuracy 0.9026 0.8442 0.7654 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modBayes01-prior 0.25 / 0.50 / 0.25&quot;, &quot;lda&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) Naive Bayes modNaiveBayes01 = naiveBayes(grupo ~ sumfac, data = tipofacial) predito = predict(modNaiveBayes01, tipofacial) cm = confusionMatrix(predito, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 29 0 5 ## dolico 0 23 2 ## meso 4 8 30 ## ## Overall Statistics ## ## Accuracy : 0.8119 ## 95% CI : (0.7219, 0.8828) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7157 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8788 0.7419 0.8108 ## Specificity 0.9265 0.9714 0.8125 ## Pos Pred Value 0.8529 0.9200 0.7143 ## Neg Pred Value 0.9403 0.8947 0.8814 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2871 0.2277 0.2970 ## Detection Prevalence 0.3366 0.2475 0.4158 ## Balanced Accuracy 0.9026 0.8567 0.8117 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modNaiveBayes01&quot;, &quot;naiveBayes&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.6 Decison tree modArvDec01 = rpart(grupo ~ sumfac, data = tipofacial) prp(modArvDec01, faclen=0, #use full names for factor labels extra=1, #display number of observations for each terminal node roundint=F, #don&#39;t round to integers in output digits=5) predito = predict(modArvDec01, type = &quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 26 0 0 ## dolico 0 23 2 ## meso 7 8 35 ## ## Overall Statistics ## ## Accuracy : 0.8317 ## 95% CI : (0.7442, 0.8988) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7444 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.7879 0.7419 0.9459 ## Specificity 1.0000 0.9714 0.7656 ## Pos Pred Value 1.0000 0.9200 0.7000 ## Neg Pred Value 0.9067 0.8947 0.9608 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2574 0.2277 0.3465 ## Detection Prevalence 0.2574 0.2475 0.4950 ## Balanced Accuracy 0.8939 0.8567 0.8558 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modArvDec01&quot;, &quot;rpart&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) x = 1:nrow(tipofacial) plot(tipofacial$sumfac ~ x, col = tipofacial$grupo) 3.4.7 SVM modSVM01 = svm(grupo ~ sumfac, data = tipofacial, kernel = &quot;linear&quot;) predito = predict(modSVM01, type = &quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modSVM01&quot;, &quot;svm&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.8 neuralnet modRedNeural01 = neuralnet(grupo ~ sumfac, data = tipofacial, hidden = c(2,4,3)) plot(modRedNeural01) ypred = neuralnet::compute(modRedNeural01, tipofacial) yhat = ypred$net.result yhat = round(yhat) yhat=data.frame(&quot;yhat&quot;= dplyr::case_when(yhat[ ,1:1]==1 ~ &quot;braq&quot;, yhat[ ,2:2]==1 ~ &quot;dolico&quot;, TRUE ~ &quot;meso&quot;)) cm = confusionMatrix(tipofacial$grupo, as.factor(yhat$yhat)) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 30 0 3 ## dolico 0 23 8 ## meso 3 2 32 ## ## Overall Statistics ## ## Accuracy : 0.8416 ## 95% CI : (0.7555, 0.9067) ## No Information Rate : 0.4257 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.7605 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.9091 0.9200 0.7442 ## Specificity 0.9559 0.8947 0.9138 ## Pos Pred Value 0.9091 0.7419 0.8649 ## Neg Pred Value 0.9559 0.9714 0.8281 ## Prevalence 0.3267 0.2475 0.4257 ## Detection Rate 0.2970 0.2277 0.3168 ## Detection Prevalence 0.3267 0.3069 0.3663 ## Balanced Accuracy 0.9325 0.9074 0.8290 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modRedNeural01&quot;, &quot;neuralnet&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.9 KNN modKnn1_01 = knn3(grupo ~ sumfac, data = tipofacial, k = 1) predito = predict(modKnn1_01, tipofacial, type = &quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 32 0 1 ## dolico 0 30 3 ## meso 1 1 33 ## ## Overall Statistics ## ## Accuracy : 0.9406 ## 95% CI : (0.8752, 0.9779) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9108 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.9697 0.9677 0.8919 ## Specificity 0.9853 0.9571 0.9688 ## Pos Pred Value 0.9697 0.9091 0.9429 ## Neg Pred Value 0.9853 0.9853 0.9394 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.3168 0.2970 0.3267 ## Detection Prevalence 0.3267 0.3267 0.3465 ## Balanced Accuracy 0.9775 0.9624 0.9303 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn1_01-k=1&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) k = 3 modKnn3_01 = knn3(grupo ~ sumfac, data = tipofacial, k = 3) predito = predict(modKnn3_01, tipofacial, type = &quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo, positive = &quot;0&quot;) # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=3&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) k = 5 modKnn5_01 = knn3(grupo ~ sumfac, data = tipofacial, k = 5) predito = predict(modKnn5_01, tipofacial, type = &quot;class&quot;) cm = confusionMatrix(predito, tipofacial$grupo, positive = &quot;0&quot;) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction braq dolico meso ## braq 28 0 1 ## dolico 0 27 4 ## meso 5 4 32 ## ## Overall Statistics ## ## Accuracy : 0.8614 ## 95% CI : (0.7784, 0.9221) ## No Information Rate : 0.3663 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.791 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: braq Class: dolico Class: meso ## Sensitivity 0.8485 0.8710 0.8649 ## Specificity 0.9853 0.9429 0.8594 ## Pos Pred Value 0.9655 0.8710 0.7805 ## Neg Pred Value 0.9306 0.9429 0.9167 ## Prevalence 0.3267 0.3069 0.3663 ## Detection Rate 0.2772 0.2673 0.3168 ## Detection Prevalence 0.2871 0.3069 0.4059 ## Balanced Accuracy 0.9169 0.9069 0.8621 # Adiciona as métricas no df model_eval[nrow(model_eval) + 1,] &lt;- c(&quot;modKnn3_01-k=5&quot;, &quot;knn3&quot;, cm$overall[&#39;Accuracy&#39;], cm$byClass[,1], cm$byClass[,2]) 3.4.10 Avaliando os modelos Model Algorithm Accuracy Sensitivity_C1 Sensitivity_C2 Sensitivity_C3 Specificity_C1 Specificity_C2 Specificity_C3 glm.fit glm 0.801980198019802 0.818181818181818 0.741935483870968 0.837837837837838 0.941176470588235 0.971428571428571 0.78125 glm.fit-split glm 0.67741935483871 0.583333333333333 0.75 0.727272727272727 0.894736842105263 0.956521739130435 0.65 modFisher01 lda 0.811881188118812 0.878787878787879 0.741935483870968 0.810810810810811 0.926470588235294 0.971428571428571 0.8125 modBayes01-prior 0.25 / 0.50 / 0.25 lda 0.782178217821782 0.878787878787879 0.774193548387097 0.702702702702703 0.926470588235294 0.914285714285714 0.828125 modNaiveBayes01 naiveBayes 0.811881188118812 0.878787878787879 0.741935483870968 0.810810810810811 0.926470588235294 0.971428571428571 0.8125 modArvDec01 rpart 0.831683168316832 0.787878787878788 0.741935483870968 0.945945945945946 1 0.971428571428571 0.765625 modSVM01 svm 0.801980198019802 0.818181818181818 0.741935483870968 0.837837837837838 0.941176470588235 0.971428571428571 0.78125 modRedNeural01 neuralnet 0.841584158415842 0.909090909090909 0.92 0.744186046511628 0.955882352941177 0.894736842105263 0.913793103448276 modKnn1_01-k=1 knn3 0.940594059405941 0.96969696969697 0.967741935483871 0.891891891891892 0.985294117647059 0.957142857142857 0.96875 modKnn3_01-k=3 knn3 0.861386138613861 0.909090909090909 0.838709677419355 0.837837837837838 0.955882352941177 0.957142857142857 0.875 modKnn3_01-k=5 knn3 0.861386138613861 0.848484848484849 0.870967741935484 0.864864864864865 0.985294117647059 0.942857142857143 0.859375 Todos os modelos tiveram uma performace bem semelhante, sendo que knn, com \\(k=3\\), obteve o melhor resultado em todas as medidas. 3.4.11 Agrupamento tipofacialS = subset(tipofacial, select=c(&quot;sexo&quot;, &quot;idade&quot;, &quot;altfac&quot;, &quot;proffac&quot;)) dummy &lt;- dummyVars(&quot; ~ .&quot;, data=tipofacialS) newdata &lt;- data.frame(predict(dummy, newdata = tipofacialS)) d &lt;- dist(newdata, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) plot(grup, cex = 0.6) groups &lt;- cutree(grup, k=3) table(groups, tipofacial$grupo) ## ## groups braq dolico meso ## 1 12 19 19 ## 2 9 12 12 ## 3 12 0 6 km1 = kmeans(newdata, 4) p1 = fviz_cluster(km1, data=newdata, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw()) p1 groups = km1$cluster table(groups, tipofacial$grupo) ## ## groups braq dolico meso ## 1 0 12 7 ## 2 12 19 19 ## 3 11 0 5 ## 4 10 0 6 3.5 Análise do conjunto de dados USArrests Análise de agrupamento para os dados USArrests. 3.5.1 Pacotes Pacotes necessários para estes exercícios: library(readxl) library(tidyverse) library(readxl) library(ggthemes) library(plotly) library(knitr) library(kableExtra) library(factoextra) 3.5.2 Conjunto de dados USArrests é um conjunto de dados contendo estatísticas de prisões por 100.000 habitantes por agressão, assassinato e estupro em cada um dos 50 estados dos EUA em 1973. Também é fornecida a porcentagem da população que vive em áreas urbanas. summary(USArrests) require(graphics) pairs(USArrests, panel = panel.smooth, main = &quot;USArrests data&quot;) ## Difference between &#39;USArrests&#39; and its correction USArrests[&quot;Maryland&quot;, &quot;UrbanPop&quot;] # 67 -- the transcription error UA.C &lt;- USArrests UA.C[&quot;Maryland&quot;, &quot;UrbanPop&quot;] &lt;- 76.6 ## also +/- 0.5 to restore the original &lt;n&gt;.5 percentages s5u &lt;- c(&quot;Colorado&quot;, &quot;Florida&quot;, &quot;Mississippi&quot;, &quot;Wyoming&quot;) s5d &lt;- c(&quot;Nebraska&quot;, &quot;Pennsylvania&quot;) UA.C[s5u, &quot;UrbanPop&quot;] &lt;- UA.C[s5u, &quot;UrbanPop&quot;] + 0.5 UA.C[s5d, &quot;UrbanPop&quot;] &lt;- UA.C[s5d, &quot;UrbanPop&quot;] - 0.5 3.5.3 Agrupamento df &lt;- scale(USArrests) summary(df) ## Murder Assault UrbanPop Rape ## Min. :-1.6044 Min. :-1.5090 Min. :-2.31714 Min. :-1.4874 ## 1st Qu.:-0.8525 1st Qu.:-0.7411 1st Qu.:-0.76271 1st Qu.:-0.6574 ## Median :-0.1235 Median :-0.1411 Median : 0.03178 Median :-0.1209 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 ## 3rd Qu.: 0.7949 3rd Qu.: 0.9388 3rd Qu.: 0.84354 3rd Qu.: 0.5277 ## Max. : 2.2069 Max. : 1.9948 Max. : 1.75892 Max. : 2.6444 d &lt;- dist(df, method = &quot;maximum&quot;) grup = hclust(d, method = &quot;ward.D&quot;) plot(grup, cex = 0.6) groups &lt;- cutree(grup, k=3) groups ## Alabama Alaska Arizona Arkansas California ## 1 2 1 2 1 ## Colorado Connecticut Delaware Florida Georgia ## 1 2 2 1 1 ## Hawaii Idaho Illinois Indiana Iowa ## 2 3 1 2 3 ## Kansas Kentucky Louisiana Maine Maryland ## 2 2 1 3 1 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 2 1 3 1 2 ## Montana Nebraska Nevada New Hampshire New Jersey ## 3 3 1 3 2 ## New Mexico New York North Carolina North Dakota Ohio ## 1 1 1 3 2 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 2 2 2 2 1 ## South Dakota Tennessee Texas Utah Vermont ## 3 1 1 2 3 ## Virginia Washington West Virginia Wisconsin Wyoming ## 2 2 3 3 2 km1 = kmeans(df, 4, nstart = 25) km1 ## K-means clustering with 4 clusters of sizes 8, 13, 13, 16 ## ## Cluster means: ## Murder Assault UrbanPop Rape ## 1 1.4118898 0.8743346 -0.8145211 0.01927104 ## 2 0.6950701 1.0394414 0.7226370 1.27693964 ## 3 -0.9615407 -1.1066010 -0.9301069 -0.96676331 ## 4 -0.4894375 -0.3826001 0.5758298 -0.26165379 ## ## Clustering vector: ## Alabama Alaska Arizona Arkansas California ## 1 2 2 1 2 ## Colorado Connecticut Delaware Florida Georgia ## 2 4 4 2 1 ## Hawaii Idaho Illinois Indiana Iowa ## 4 3 2 4 3 ## Kansas Kentucky Louisiana Maine Maryland ## 4 3 1 3 2 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 4 2 3 1 2 ## Montana Nebraska Nevada New Hampshire New Jersey ## 3 3 2 3 4 ## New Mexico New York North Carolina North Dakota Ohio ## 2 2 1 3 4 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 4 4 4 4 1 ## South Dakota Tennessee Texas Utah Vermont ## 3 1 2 4 3 ## Virginia Washington West Virginia Wisconsin Wyoming ## 4 4 3 3 4 ## ## Within cluster sum of squares by cluster: ## [1] 8.316061 19.922437 11.952463 16.212213 ## (between_SS / total_SS = 71.2 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; aggregate(df, by=list(cluster=km1$cluster), mean) ## cluster Murder Assault UrbanPop Rape ## 1 1 1.4118898 0.8743346 -0.8145211 0.01927104 ## 2 2 0.6950701 1.0394414 0.7226370 1.27693964 ## 3 3 -0.9615407 -1.1066010 -0.9301069 -0.96676331 ## 4 4 -0.4894375 -0.3826001 0.5758298 -0.26165379 p1 = fviz_cluster(km1, data=df, palette = c(&quot;#2E9FDF&quot;, &quot;#FC4E07&quot;, &quot;#E7B800&quot;, &quot;#E7B700&quot;), star.plot=FALSE, # repel=TRUE, ggtheme=theme_bw(), main = &quot;Particionamento&quot;) p1 "],["final-work.html", "Cap. 4 Trabalho final 4.1 Fonte de dados 4.2 Introdução 4.3 Referências", " Cap. 4 Trabalho final Qual o impacto das decisôes do COMPOM em relação a taxa Selic no índice Bovespa? 4.1 Fonte de dados Taxa selic: selic.json. selic.csv Índices Bovespa: idc_BVSP_daily.csv, idx_BVSP_monthly.csv 4.2 Introdução TODO 4.3 Referências Análise das Atas do COPOM com text mining Mineração de textos do COPOM: criando um indicador de sentimentos "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
